{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0767b358",
   "metadata": {},
   "source": [
    "# FB2NEP — Data Analysis Notebook (Assessment)\n",
    "\n",
    "This notebook supports the **FB2NEP assessment**. Fill in the **Data mapping** cell before running the analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57c9a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Imports\n",
    "import os, sys, math, json, textwrap, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 200)\n",
    "\n",
    "print(\"Versions:\")\n",
    "print(\"pandas\", pd.__version__)\n",
    "print(\"statsmodels\", sm.__version__)\n",
    "print(\"numpy\", np.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d064ff",
   "metadata": {},
   "source": [
    "## Data mapping (REQUIRED)\n",
    "\n",
    "Assign the correct column names from `fb2nep.csv` to the variables below.  \n",
    "If unsure, run the next cell to list columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf61428",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List columns helper (run this first if needed)\n",
    "from pathlib import Path\n",
    "DATA_PATHS = [Path(\"./data/fb2nep.csv\"), Path(\"./fb2nep.csv\"), Path(\"../data/fb2nep.csv\")]\n",
    "for p in DATA_PATHS:\n",
    "    if p.exists():\n",
    "        df_head = pd.read_csv(p, nrows=5)\n",
    "        print(f\"Found: {p} — columns:\")\n",
    "        print(list(df_head.columns))\n",
    "        break\n",
    "else:\n",
    "    print(\"fb2nep.csv not found in ./data, ./, or ../data — please place it accordingly.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc9b8ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === EDIT THIS CELL ===\n",
    "# Map dataset columns here (strings). Use exact column names from fb2nep.csv.\n",
    "\n",
    "MAPPING = {\n",
    "    # Primary outcome (binary recommended: 0/1). Example: \"CVD_Incidence\"\n",
    "    \"outcome\": \"<OUTCOME_COL>\",\n",
    "\n",
    "    # Primary exposure: biomarker variable (continuous). Example: \"flavanol_biomarker\"\n",
    "    \"exposure_biomarker\": \"<BIOMARKER_COL>\",\n",
    "\n",
    "    # Secondary exposure: diet diary (DD) variable (continuous). Example: \"flavanol_dd\"\n",
    "    \"exposure_dd\": \"<DD_COL>\",\n",
    "\n",
    "    # Demographics/covariates (edit as applicable)\n",
    "    \"id\": \"<ID_COL>\",                 # e.g., \"ID\" or \"participant_id\" (optional but useful)\n",
    "    \"age\": \"<AGE_COL>\",               # e.g., \"age\"\n",
    "    \"sex\": \"<SEX_COL>\",               # e.g., \"sex\" coded as 0/1 or 'M'/'F'\n",
    "    \"bmi\": \"<BMI_COL>\",               # e.g., \"BMI\"\n",
    "    \"smoking\": \"<SMOKING_COL>\",       # e.g., 'never','former','current' (categorical)\n",
    "    \"ses\": \"<SES_COL>\",               # socioeconomic status (categorical or continuous)\n",
    "    # Add other candidate confounders as needed:\n",
    "    # \"physical_activity\": \"<PA_COL>\",\n",
    "    # \"energy_intake\": \"<ENERGY_COL>\",\n",
    "}\n",
    "\n",
    "# Candidate confounders used in change-in-estimate search (a list of keys from MAPPING)\n",
    "CANDIDATE_CONFOUNDERS = [\"age\", \"sex\", \"bmi\", \"smoking\", \"ses\"]\n",
    "\n",
    "# Optional: set outcome scale if continuous\n",
    "OUTCOME_IS_BINARY = True  # set to False if outcome is continuous\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd020d6",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed8abb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load fb2nep.csv\n",
    "def load_fb2nep():\n",
    "    for p in DATA_PATHS:\n",
    "        if p.exists():\n",
    "            return pd.read_csv(p)\n",
    "    raise FileNotFoundError(\"fb2nep.csv not found. Place it in ./data, ./, or ../data\")\n",
    "\n",
    "df = load_fb2nep()\n",
    "print(df.shape)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3cc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utilities to validate mapping and coerce variable types\n",
    "def validate_mapping(df, mapping):\n",
    "    missing = [k for k,v in mapping.items() if isinstance(v, str) and v.startswith(\"<\")]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Please fill in MAPPING for: {missing}\")\n",
    "    for k, v in mapping.items():\n",
    "        if v and v not in df.columns:\n",
    "            raise KeyError(f\"MAPPING[{k}] refers to '{v}', which is not a column in the dataset.\")\n",
    "    return True\n",
    "\n",
    "validate_mapping(df, MAPPING)\n",
    "\n",
    "# Coerce common types\n",
    "def coerce_types(d, mapping, outcome_is_binary=True):\n",
    "    d = d.copy()\n",
    "    if mapping.get(\"sex\") in d:\n",
    "        # Attempt to standardise sex to 0/1 if string-coded\n",
    "        if d[mapping[\"sex\"]].dtype == object:\n",
    "            d[mapping[\"sex\"]] = d[mapping[\"sex\"]].astype(str).str.strip().str[0].str.upper().map({\"M\":0, \"F\":1})\n",
    "    if outcome_is_binary and mapping.get(\"outcome\") in d:\n",
    "        # Try to coerce to 0/1 if appears boolean or categorical\n",
    "        if d[mapping[\"outcome\"]].dtype == object:\n",
    "            d[mapping[\"outcome\"]] = d[mapping[\"outcome\"]].astype(str).str.strip().str.lower().map({\"no\":0, \"yes\":1})\n",
    "        d[mapping[\"outcome\"]] = pd.to_numeric(d[mapping[\"outcome\"]], errors=\"coerce\")\n",
    "    return d\n",
    "\n",
    "df = coerce_types(df, MAPPING, OUTCOME_IS_BINARY)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce03b97",
   "metadata": {},
   "source": [
    "## Table 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c3bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def summarise_series(s):\n",
    "    if pd.api.types.is_numeric_dtype(s):\n",
    "        return pd.Series({\n",
    "            \"n\": s.notna().sum(),\n",
    "            \"mean\": s.mean(),\n",
    "            \"sd\": s.std(),\n",
    "            \"median\": s.median(),\n",
    "            \"iqr\": s.quantile(0.75) - s.quantile(0.25)\n",
    "        })\n",
    "    else:\n",
    "        vc = s.value_counts(dropna=False)\n",
    "        total = len(s)\n",
    "        return pd.Series({f\"{k} (n,%)\": f\"{v} ({v/total*100:.1f}%)\" for k,v in vc.items()})\n",
    "\n",
    "def table1(df, cols, by=None):\n",
    "    out = {}\n",
    "    if by is None:\n",
    "        for c in cols:\n",
    "            out[c] = summarise_series(df[c])\n",
    "        res = pd.concat(out, axis=1)\n",
    "    else:\n",
    "        groups = df.groupby(by, dropna=False)\n",
    "        parts = []\n",
    "        for lvl, dsub in groups:\n",
    "            part = pd.concat({c: summarise_series(dsub[c]) for c in cols}, axis=1)\n",
    "            part.columns = pd.MultiIndex.from_product([[f\"{by}={lvl}\"], part.columns])\n",
    "            parts.append(part)\n",
    "        res = pd.concat(parts, axis=1)\n",
    "    return res\n",
    "\n",
    "cols = [c for c in [MAPPING.get(\"age\"), MAPPING.get(\"sex\"), MAPPING.get(\"bmi\"), \n",
    "                    MAPPING.get(\"smoking\"), MAPPING.get(\"ses\"), \n",
    "                    MAPPING.get(\"exposure_biomarker\"), MAPPING.get(\"exposure_dd\")]\n",
    "        if c and c in df.columns]\n",
    "\n",
    "tbl1_overall = table1(df, cols)\n",
    "tbl1_overall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509f4682",
   "metadata": {},
   "source": [
    "## Missingness audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae084983",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def missingness_summary(d):\n",
    "    miss = d.isna().mean().sort_values(ascending=False)\n",
    "    out = pd.DataFrame({\"missing_prop\": miss, \"missing_%\": (miss*100).round(1)})\n",
    "    out[\"n_missing\"] = d.isna().sum()\n",
    "    out[\"n\"] = len(d)\n",
    "    return out\n",
    "\n",
    "miss = missingness_summary(df)\n",
    "miss.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31151829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualise missingness matrix (simple)\n",
    "def plot_missingness_matrix(d, max_cols=30):\n",
    "    d = d.copy()\n",
    "    if d.shape[1] > max_cols:\n",
    "        d = d.iloc[:, :max_cols]\n",
    "        print(f\"(Showing first {max_cols} columns)\")\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.imshow(d.isna(), aspect='auto', interpolation='nearest')\n",
    "    plt.xlabel(\"Columns (subset if large)\")\n",
    "    plt.ylabel(\"Rows\")\n",
    "    plt.title(\"Missingness matrix\")\n",
    "    plt.show()\n",
    "\n",
    "plot_missingness_matrix(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456661d1",
   "metadata": {},
   "source": [
    "## Biomarker vs Diet Diary (DD) comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565ae29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "biom = MAPPING[\"exposure_biomarker\"]\n",
    "dd   = MAPPING[\"exposure_dd\"]\n",
    "\n",
    "biom_valid = df[biom].astype(float)\n",
    "dd_valid   = df[dd].astype(float)\n",
    "\n",
    "# Scatter & correlation\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(dd_valid, biom_valid, alpha=0.6)\n",
    "plt.xlabel(f\"{dd}\")\n",
    "plt.ylabel(f\"{biom}\")\n",
    "plt.title(\"Biomarker vs DD: scatter\")\n",
    "plt.show()\n",
    "\n",
    "valid = df[[biom, dd]].dropna()\n",
    "r = valid[biom].corr(valid[dd])\n",
    "print(f\"Pearson r = {r:.3f} (n={len(valid)})\")\n",
    "\n",
    "# Bland–Altman (biomarker vs DD)\n",
    "def bland_altman(a, b):\n",
    "    a, b = np.asarray(a), np.asarray(b)\n",
    "    diff = a - b\n",
    "    mean = (a + b) / 2\n",
    "    mdiff = np.mean(diff)\n",
    "    sd = np.std(diff, ddof=1)\n",
    "    loa = (mdiff - 1.96*sd, mdiff + 1.96*sd)\n",
    "    return mean, diff, mdiff, loa\n",
    "\n",
    "mean_ab, diff_ab, mdiff, loa = bland_altman(valid[biom], valid[dd])\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.scatter(mean_ab, diff_ab, alpha=0.6)\n",
    "plt.axhline(mdiff)\n",
    "plt.axhline(loa[0])\n",
    "plt.axhline(loa[1])\n",
    "plt.xlabel(\"Mean of biomarker and DD\")\n",
    "plt.ylabel(\"Difference (biomarker − DD)\")\n",
    "plt.title(\"Bland–Altman plot\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Mean difference: {mdiff:.3f}; 95% LoA: [{loa[0]:.3f}, {loa[1]:.3f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f78ec9",
   "metadata": {},
   "source": [
    "## Primary association & confounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5665f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Build formulas\n",
    "outcome = MAPPING[\"outcome\"]\n",
    "exposure = MAPPING[\"exposure_biomarker\"]\n",
    "\n",
    "def make_formula(outcome, exposure, covars=None, binary=True):\n",
    "    rhs = exposure if covars is None or len(covars)==0 else exposure + \" + \" + \" + \".join(covars)\n",
    "    return f\"{outcome} ~ {rhs}\"\n",
    "\n",
    "def fit_model(df, formula, binary=True):\n",
    "    d = df.dropna()\n",
    "    if binary:\n",
    "        model = smf.logit(formula, data=d).fit(disp=False)\n",
    "    else:\n",
    "        model = smf.ols(formula, data=d).fit()\n",
    "    return model\n",
    "\n",
    "# Minimal model\n",
    "min_formula = make_formula(outcome, exposure, covars=[], binary=OUTCOME_IS_BINARY)\n",
    "min_model = fit_model(df, min_formula, OUTCOME_IS_BINARY)\n",
    "print(min_model.summary())\n",
    "\n",
    "# Confounder-adjusted model (pre-specified set)\n",
    "prespec_covars = [MAPPING[k] for k in CANDIDATE_CONFOUNDERS if MAPPING.get(k) in df.columns]\n",
    "adj_formula = make_formula(outcome, exposure, covars=prespec_covars, binary=OUTCOME_IS_BINARY)\n",
    "adj_model = fit_model(df, adj_formula, OUTCOME_IS_BINARY)\n",
    "print(adj_model.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f31f201",
   "metadata": {},
   "source": [
    "### Change‑in‑estimate (≥10%) procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc47ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_effect(model, exposure, binary=True):\n",
    "    # For logit, coefficient is log-odds; return OR. For OLS, return beta.\n",
    "    if binary:\n",
    "        b = model.params[exposure]\n",
    "        se = model.bse[exposure]\n",
    "        OR = np.exp(b)\n",
    "        lo = np.exp(b - 1.96*se)\n",
    "        hi = np.exp(b + 1.96*se)\n",
    "        return {\"effect\": OR, \"lo\": lo, \"hi\": hi, \"scale\": \"OR\"}\n",
    "    else:\n",
    "        b = model.params[exposure]\n",
    "        se = model.bse[exposure]\n",
    "        lo = b - 1.96*se\n",
    "        hi = b + 1.96*se\n",
    "        return {\"effect\": b, \"lo\": lo, \"hi\": hi, \"scale\": \"beta\"}\n",
    "\n",
    "base_eff = get_effect(min_model, exposure, OUTCOME_IS_BINARY)[\"effect\"]\n",
    "\n",
    "results = []\n",
    "for k in CANDIDATE_CONFOUNDERS:\n",
    "    cov = MAPPING.get(k)\n",
    "    if not cov or cov not in df.columns: \n",
    "        continue\n",
    "    f = make_formula(outcome, exposure, covars=[cov], binary=OUTCOME_IS_BINARY)\n",
    "    m = fit_model(df, f, OUTCOME_IS_BINARY)\n",
    "    eff = get_effect(m, exposure, OUTCOME_IS_BINARY)[\"effect\"]\n",
    "    change = 100 * (eff - base_eff) / base_eff if base_eff != 0 else np.nan\n",
    "    results.append({\"added\": cov, \"effect\": eff, \"% change vs minimal\": change})\n",
    "\n",
    "cei = pd.DataFrame(results).sort_values(\"% change vs minimal\", key=lambda s: s.abs(), ascending=False)\n",
    "cei\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a39deeeb",
   "metadata": {},
   "source": [
    "## Diagnostics (brief)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e42553b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Outlier check (standardised residuals)\n",
    "def standardised_residuals(model):\n",
    "    if OUTCOME_IS_BINARY:\n",
    "        # Use Pearson residuals as a simple check\n",
    "        resid = model.resid_pearson\n",
    "    else:\n",
    "        resid = model.get_influence().resid_studentized_internal\n",
    "    return pd.Series(resid, name=\"std_resid\")\n",
    "\n",
    "resid = standardised_residuals(adj_model)\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(resid.values, marker='o', linestyle='none', alpha=0.6)\n",
    "plt.axhline(3); plt.axhline(-3)\n",
    "plt.title(\"Standardised residuals (approx.)\")\n",
    "plt.xlabel(\"Observation (index in complete-case sample)\")\n",
    "plt.ylabel(\"Std residual\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c400b4c9",
   "metadata": {},
   "source": [
    "## Model exploration helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8402e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_model_with_covars(covars):\n",
    "    f = make_formula(outcome, exposure, covars=covars, binary=OUTCOME_IS_BINARY)\n",
    "    m = fit_model(df, f, OUTCOME_IS_BINARY)\n",
    "    print(f\"Formula: {f}\")\n",
    "    print(m.summary())\n",
    "\n",
    "# Example: try a different exposure (DD)\n",
    "def run_with_dd_as_exposure(covars):\n",
    "    f = make_formula(outcome, MAPPING[\"exposure_dd\"], covars=covars, binary=OUTCOME_IS_BINARY)\n",
    "    m = fit_model(df, f, OUTCOME_IS_BINARY)\n",
    "    print(f\"Formula: {f}\")\n",
    "    print(m.summary())\n",
    "\n",
    "print(\"Use run_model_with_covars([...]) to experiment; try prespec_covars or subsets.\")\n",
    "print(\"Use run_with_dd_as_exposure([...]) to compare biomarker vs DD.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702e36e7",
   "metadata": {},
   "source": [
    "## Save mapping snapshot (for reproducibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0bf35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "snapshot = {\n",
    "    \"mapping\": MAPPING,\n",
    "    \"candidates\": CANDIDATE_CONFOUNDERS,\n",
    "    \"binary_outcome\": OUTCOME_IS_BINARY,\n",
    "}\n",
    "\n",
    "Path(\"./artifacts\").mkdir(exist_ok=True, parents=True)\n",
    "with open(\"./artifacts/data_mapping_snapshot.json\", \"w\") as f:\n",
    "    json.dump(snapshot, f, indent=2)\n",
    "print(\"Saved ./artifacts/data_mapping_snapshot.json\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
