[
  {
    "objectID": "assessment/assessment_1b.html",
    "href": "assessment/assessment_1b.html",
    "title": "FB2NEP Assignment Notebook – Personal Dataset and Analysis",
    "section": "",
    "text": "This notebook supports Part B of the FB2NEP coursework assignment:\nYou do not need to understand Python code to complete this notebook. In most cases, you only need to:"
  },
  {
    "objectID": "assessment/assessment_1b.html#reminder-assignment-structure",
    "href": "assessment/assessment_1b.html#reminder-assignment-structure",
    "title": "FB2NEP Assignment Notebook – Personal Dataset and Analysis",
    "section": "0.1 Reminder – Assignment structure",
    "text": "0.1 Reminder – Assignment structure\nThe full coursework consists of:\n\nPart A (in Word only):\n\nShort knowledge questions.\nDrawing and explaining a DAG.\nInterpreting published results.\n\nPart B (this notebook + Word):\n\nB1: Table 1 and commentary.\nB2: Distributions and transformations.\nB3: Regression model and interpretation.\nB4: DAG-informed adjustment strategy.\nOptional bonus.\n\n\nThis notebook is designed to support Part B. You will need to copy results and figures from here into your Word document and write your interpretations there."
  },
  {
    "objectID": "assessment/assessment_1b.html#variable-definitions-and-coding",
    "href": "assessment/assessment_1b.html#variable-definitions-and-coding",
    "title": "FB2NEP Assignment Notebook – Personal Dataset and Analysis",
    "section": "0.2 Variable definitions and coding",
    "text": "0.2 Variable definitions and coding\nThe dataset used in this assignment is synthetic, but its structure, distributions, and relationships are designed to resemble those commonly encountered in observational nutritional epidemiology.\nYou should treat the variables below as if they originated from a real population study and interpret them accordingly.\n\n\n\n\n\n\n\n\nVariable\nDescription\nCoding / Units\n\n\n\n\nid\nUnique participant identifier\nInteger\n\n\nage\nAge at baseline\nYears (continuous)\n\n\nsex\nBiological sex\n\"Female\", \"Male\"\n\n\nses\nSocioeconomic status\n\"low\", \"middle\", \"high\"\n\n\nsmoking\nSmoking status\n\"never\", \"former\", \"current\"\n\n\npa\nPhysical activity level\n\"low\", \"moderate\", \"high\"\n\n\nssb\nSugar-sweetened beverage intake\nServings per day (continuous)\n\n\nbmi\nBody mass index\nkg/m² (continuous)\n\n\nobese\nObesity indicator\n0 = BMI &lt; 30 kg/m², 1 = BMI ≥ 30 kg/m²\n\n\ncvd_risk\nComposite cardiovascular disease risk score\nContinuous (higher = higher risk)\n\n\n\nImportant notes\n\nssb represents habitual intake and should be interpreted as an average daily exposure.\ncvd_risk is a continuous score derived from multiple risk factors; it is not a probability or a clinical diagnosis.\nRelationships between variables reflect plausible causal structures but do not represent real individuals."
  },
  {
    "objectID": "assessment/assessment_1b.html#step-1-set-up-python-libraries",
    "href": "assessment/assessment_1b.html#step-1-set-up-python-libraries",
    "title": "FB2NEP Assignment Notebook – Personal Dataset and Analysis",
    "section": "0.3 Step 1 – Set up Python libraries",
    "text": "0.3 Step 1 – Set up Python libraries\nRun the cell below once. It loads the Python libraries that this notebook uses.\n\n# ============================================================\n# Import required Python libraries\n#\n# You do not need to change anything in this cell.\n# Simply run it once. If you see warnings, that is usually fine.\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\n# Make plots appear inside the notebook\n%matplotlib inline\n\n# For nicer table display (if supported)\npd.set_option(\"display.max_columns\", 50)\npd.set_option(\"display.precision\", 3)\n\nprint(\"Libraries imported successfully.\")\n\nLibraries imported successfully."
  },
  {
    "objectID": "assessment/assessment_1b.html#step-2-submission-date-set-by-the-module-convenor",
    "href": "assessment/assessment_1b.html#step-2-submission-date-set-by-the-module-convenor",
    "title": "FB2NEP Assignment Notebook – Personal Dataset and Analysis",
    "section": "0.4 Step 2 – Submission date (set by the module convenor)",
    "text": "0.4 Step 2 – Submission date (set by the module convenor)\nThis notebook releases the final dataset only within the last two weeks before the submission deadline.\n\nBefore that point, the notebook runs in preview mode and intentionally introduces artefacts so that results are not reliable.\nFrom the release date onwards, the notebook generates a final, stable dataset.\n\nYou do not need to change anything in this section unless you are the module convenor.\n\n# ============================================================\n# Submission date (set by the module convenor)\n#\n# INSTRUCTION (convenor only):\n# - Set submission_date as YYYY-MM-DD.\n# - The final dataset will be available from (submission_date - 14 days).\n# ============================================================\n\nfrom datetime import date, datetime, timedelta\n\nsubmission_date = \"2026-03-01\"  # &lt;-- CONVENOR: EDIT THIS DATE (YYYY-MM-DD)\n\n# Parse and compute release date (14 days before submission)\n_submission_dt = datetime.strptime(submission_date, \"%Y-%m-%d\").date()\nrelease_date = _submission_dt - timedelta(days=14)\n\ntoday = date.today()\ndays_until_submission = (_submission_dt - today).days\ndays_until_release = (release_date - today).days\n\nprint(f\"Submission date:  {_submission_dt.isoformat()}\")\nprint(f\"Release date:     {release_date.isoformat()} (final dataset available from this date)\")\nprint(f\"Today:            {today.isoformat()}\")\n\nif today &lt; release_date:\n    print(\"\\nDATA STATUS: PREVIEW MODE (results intentionally perturbed; do not rely on them).\")\nelse:\n    print(\"\\nDATA STATUS: FINAL MODE (dataset is stable and can be used for your submission).\")\n\nSubmission date:  2026-03-01\nRelease date:     2026-02-15 (final dataset available from this date)\nToday:            2026-01-30\n\nDATA STATUS: PREVIEW MODE (results intentionally perturbed; do not rely on them)."
  },
  {
    "objectID": "assessment/assessment_1b.html#step-3-your-population-name-write-this-down",
    "href": "assessment/assessment_1b.html#step-3-your-population-name-write-this-down",
    "title": "FB2NEP Assignment Notebook – Personal Dataset and Analysis",
    "section": "0.5 Step 3 – Your population name (write this down)",
    "text": "0.5 Step 3 – Your population name (write this down)\nTo ensure that each student receives a different dataset without requiring any manual input, the notebook generates a short population name (a random word pair).\n\nThis population name will be shown in Table 1 as Population name.\nYou must write it down and include it in your submitted Word document.\nThe population name uniquely determines your dataset.\n\nThe population name is designed to be easy to record and later verify.\n\n# ============================================================\n# Generate a population name and derive a dataset seed\n#\n# You do not need to change anything in this cell.\n# IMPORTANT: write down the population name shown below.\n# ============================================================\n\nimport os\nimport time\nimport secrets\nimport hashlib\n\n# A small word list to generate a readable \"population name\"\n_adjectives = [\n    \"amber\", \"brisk\", \"calm\", \"cobalt\", \"dapper\", \"frosty\", \"gentle\", \"honey\",\n    \"ivory\", \"jolly\", \"keen\", \"lunar\", \"mellow\", \"navy\", \"olive\", \"plucky\",\n    \"quiet\", \"rustic\", \"silver\", \"tidy\", \"urban\", \"vivid\", \"witty\", \"zesty\"\n]\n\n_nouns = [\n    \"badger\", \"beacon\", \"cedar\", \"comet\", \"dolphin\", \"ember\", \"falcon\", \"garden\",\n    \"harbour\", \"island\", \"jigsaw\", \"lantern\", \"meadow\", \"otter\", \"piano\", \"quartz\",\n    \"river\", \"saffron\", \"thistle\", \"tulip\", \"valley\", \"walnut\", \"yonder\", \"zephyr\"\n]\n\n# Build an entropy string from multiple sources (time, process, OS randomness)\n_entropy = f\"{time.time_ns()}|{os.getpid()}|{secrets.token_hex(16)}\"\n_digest = hashlib.sha256(_entropy.encode(\"utf-8\")).digest()\n\nadj = _adjectives[_digest[0] % len(_adjectives)]\nnoun = _nouns[_digest[1] % len(_nouns)]\n\npopulation_name = f\"{adj}-{noun}\"\n\n# Derive a stable integer seed from the population name.\n# This allows the convenor to reproduce the dataset later by re-entering the population name.\n_salt = \"FB2NEP_assessment1_v1\"\n_seed_bytes = hashlib.sha256((population_name + \"|\" + _salt).encode(\"utf-8\")).digest()\nseed_int = int.from_bytes(_seed_bytes[:8], \"big\")\n\nprint(\"POPULATION NAME (record this):\", population_name)\nprint(\"Derived seed (internal):      \", seed_int)\n\n# Determine whether we should provide the final dataset or a perturbed preview dataset.\n# - Before release_date: preview mode (intentionally perturbed; changes between runs).\n# - From release_date onwards: final mode (stable for a given population name).\nif today &lt; release_date:\n    dataset_mode = \"PREVIEW\"\n    # Make the preview dataset change between runs by injecting additional time-based entropy.\n    _preview_bytes = hashlib.sha256((population_name + str(time.time_ns())).encode(\"utf-8\")).digest()\n    effective_seed = int.from_bytes(_preview_bytes[:8], \"big\")\nelse:\n    dataset_mode = \"FINAL\"\n    effective_seed = seed_int\n\nprint(\"Dataset mode:                 \", dataset_mode)\n\nPOPULATION NAME (record this): cobalt-walnut\nDerived seed (internal):       2759777107657430458\nDataset mode:                  PREVIEW"
  },
  {
    "objectID": "assessment/assessment_1b.html#step-4-generate-your-dataset",
    "href": "assessment/assessment_1b.html#step-4-generate-your-dataset",
    "title": "FB2NEP Assignment Notebook – Personal Dataset and Analysis",
    "section": "0.6 Step 4 – Generate your dataset",
    "text": "0.6 Step 4 – Generate your dataset\nThe cell below will generate your dataset and store it in a pandas DataFrame called df. It will also save a CSV file named:\nmy_fb2nep_assignment_data.csv\nImportant:\n\nThe dataset is determined by your Population name.\nIn the final two weeks before the submission deadline, the notebook runs in FINAL mode and the dataset is stable for a given population name.\nBefore that point, the notebook runs in PREVIEW mode and intentionally introduces artefacts, so results are not reliable and may change between runs.\n\nYou must include your Population name in your submitted Word document.\n\n# ============================================================\n# Generate the dataset for this assessment\n# ============================================================\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display\n\nrng = np.random.default_rng(effective_seed)\n\n# Number of participants in the synthetic cohort\nn = 2000\n\n# Study ID (1, 2, 3, ...)\nstudy_id = np.arange(1, n + 1)\n\n# Age: roughly 30–80 years, with a mean around 55\nage = rng.normal(loc=55, scale=12, size=n)\nage = np.clip(age, 30, 85)\n\n# Sex: approximately 50 % female, 50 % male\nsex = rng.choice([\"Female\", \"Male\"], size=n, p=[0.52, 0.48])\n\n# Socioeconomic status (ses): low, middle, high\nses = rng.choice([\"low\", \"middle\", \"high\"], size=n, p=[0.30, 0.45, 0.25])\n\n# Smoking status: never, former, current\nsmoking = rng.choice([\"never\", \"former\", \"current\"], size=n, p=[0.50, 0.30, 0.20])\n\n# Physical activity (pa): low, moderate, high\npa = rng.choice([\"low\", \"moderate\", \"high\"], size=n, p=[0.35, 0.45, 0.20])\n\n# Base SSB intake (servings per day): positively skewed\nssb_base = rng.gamma(shape=1.5, scale=0.7, size=n)\n\n# SES effect: higher intake in low SES, lower in high SES\nses_effect = np.where(ses == \"low\", 0.6, np.where(ses == \"middle\", 0.2, -0.3))\n\n# Age effect: slightly lower SSB with increasing age\nage_effect = -(age - 55) * 0.01\n\n# Sex effect: assume slightly higher SSB in males\nsex_effect = np.where(sex == \"Male\", 0.2, 0.0)\n\nssb = ssb_base + ses_effect + age_effect + sex_effect\nssb = np.clip(ssb, 0, None)  # SSB cannot be negative\n\n# BMI: mean around 27, increased by higher SSB and lower physical activity\nbmi_base = rng.normal(loc=27, scale=4.5, size=n)\n\n# Effect of SSB on BMI (small positive)\nbmi_ssb_effect = ssb * 0.6\n\n# Effect of physical activity on BMI\npa_effect = np.where(pa == \"low\", 1.5, np.where(pa == \"moderate\", 0.0, -1.0))\n\nbmi = bmi_base + bmi_ssb_effect + pa_effect\nbmi = np.clip(bmi, 16, 55)\n\n# Obesity indicator (binary): BMI &gt;= 30\nobese = (bmi &gt;= 30).astype(int)\n\n# CVD risk score: linear predictor + noise, then mapped to 0–100 range\nlp = (\n    -5.0\n    + 0.08 * (age - 55)                      # age effect\n    + 0.06 * (bmi - 27)                      # BMI effect\n    + 0.10 * ssb                              # SSB effect\n    + np.where(sex == \"Male\", 0.8, 0.0)      # male sex\n    + np.where(smoking == \"former\", 0.7, 0.0)\n    + np.where(smoking == \"current\", 1.5, 0.0)\n    + np.where(ses == \"low\", 0.7, 0.0)\n    + np.where(ses == \"high\", -0.4, 0.0)\n)\n\nlp_noisy = lp + rng.normal(loc=0.0, scale=0.8, size=n)\ncvd_risk = 100 / (1 + np.exp(-lp_noisy))\n\n# ------------------------------------------------------------\n# Preview mode: intentionally introduce artefacts\n#\n# This is used BEFORE the final dataset release date.\n# Results are not reliable and may change between runs.\n# ------------------------------------------------------------\nif dataset_mode == \"PREVIEW\":\n    # Flip sex labels (systematic misclassification)\n    sex = np.where(sex == \"Male\", \"Female\", \"Male\")\n\n    # Re-label smoking categories (artefactual distortion)\n    smoking = rng.choice([\"never\", \"former\", \"current\"], size=n, p=[0.40, 0.35, 0.25])\n\n    # Add additional noise to the outcome\n    cvd_risk = np.clip(cvd_risk + rng.normal(loc=0.0, scale=7.0, size=n), 0, 100)\n\n# Assemble the DataFrame\ndf = pd.DataFrame({\n    \"id\": study_id,\n    \"age\": age,\n    \"sex\": sex,\n    \"ses\": ses,\n    \"smoking\": smoking,\n    \"pa\": pa,\n    \"ssb\": ssb,\n    \"bmi\": bmi,\n    \"obese\": obese,\n    \"cvd_risk\": cvd_risk,\n})\n\n# Store identifiers (useful for your write-up and for verification)\ndf[\"population_name\"] = population_name\ndf[\"dataset_mode\"] = dataset_mode\n\nprint(\"First 5 rows of your dataset:\")\ndisplay(df.head())\n\n# Save to CSV\ncsv_filename = \"my_fb2nep_assignment_data.csv\"\ndf.to_csv(csv_filename, index=False)\n\nprint(f\"\\nDataset saved to: {csv_filename}\")\n\nif dataset_mode == \"PREVIEW\":\n    print(\"\\nNOTE: You are in PREVIEW mode. Results are intentionally perturbed and may change between runs.\")\n    print(f\"The FINAL dataset will be available from: {release_date.isoformat()}\")\nelse:\n    print(\"\\nYou are in FINAL mode. This dataset is stable for your population name and can be used for your submission.\")\n\nFirst 5 rows of your dataset:\n\n\n\n\n\n\n\n\n\nid\nage\nsex\nses\nsmoking\npa\nssb\nbmi\nobese\ncvd_risk\npopulation_name\ndataset_mode\n\n\n\n\n0\n1\n49.583\nMale\nmiddle\nnever\nlow\n2.902\n30.647\n1\n8.473\ncobalt-walnut\nPREVIEW\n\n\n1\n2\n32.694\nMale\nlow\ncurrent\nmoderate\n2.929\n28.754\n0\n6.682\ncobalt-walnut\nPREVIEW\n\n\n2\n3\n61.995\nMale\nhigh\nnever\nmoderate\n0.385\n24.780\n0\n7.998\ncobalt-walnut\nPREVIEW\n\n\n3\n4\n69.033\nFemale\nlow\nnever\nmoderate\n2.381\n24.050\n0\n28.155\ncobalt-walnut\nPREVIEW\n\n\n4\n5\n67.318\nMale\nhigh\nnever\nlow\n0.295\n34.808\n1\n4.607\ncobalt-walnut\nPREVIEW\n\n\n\n\n\n\n\n\nDataset saved to: my_fb2nep_assignment_data.csv\n\nNOTE: You are in PREVIEW mode. Results are intentionally perturbed and may change between runs.\nThe FINAL dataset will be available from: 2026-02-15"
  },
  {
    "objectID": "assessment/assessment_1b.html#bonus-option-a-stratified-analysis",
    "href": "assessment/assessment_1b.html#bonus-option-a-stratified-analysis",
    "title": "FB2NEP Assignment Notebook – Personal Dataset and Analysis",
    "section": "5.1 Bonus option A – Stratified analysis",
    "text": "5.1 Bonus option A – Stratified analysis\nRepeat your analysis of the association between SSB intake and CVD risk separately within strata of a key variable (for example obesity status or sex).\nFor example, you might: - Fit separate models for obese and non-obese participants, or - Compare mean CVD risk across levels of SSB within strata.\n\n5.1.1 Bonus question\n\nDescribe how the association between SSB intake and CVD risk compares across strata.\nComment on whether stratification changes your interpretation of the main findings.\nExplain briefly why stratification can be useful in epidemiological analysis.\n\nMaximum length: 150 words.\n\n# ============================================================\n# Example: Stratified analysis\n#\n# Stratification means analysing the association of interest\n# separately within subgroups (strata) of the population.\n#\n# In this example, we stratify by obesity status.\n# You could just as easily stratify by sex by changing ONE line\n# (see comment below).\n# ============================================================\n\n# Loop over the strata we want to analyse\n# Here:\n#   status = 0  -&gt; not obese\n#   status = 1  -&gt; obese\nfor status, label in [(0, \"Not obese\"), (1, \"Obese\")]:\n\n    # Create a subset of the data for this stratum\n    # To stratify by sex instead, you would replace this line with:\n    #   df_sub = df[df[\"sex\"] == \"Female\"]\n    # or:\n    #   df_sub = df[df[\"sex\"] == \"Male\"]\n    df_sub = df[df[\"obese\"] == status]\n\n    # Fit the regression model within this stratum\n    model_strat = smf.ols(\n        \"cvd_risk ~ ssb + age + C(sex) + C(ses) + C(smoking)\",\n        data=df_sub\n    ).fit()\n\n    # Print and display the model summary\n    print(f\"\\nStratified model: {label}\")\n    display(model_strat.summary())\n\n\nStratified model: Not obese\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ncvd_risk\nR-squared:\n0.168\n\n\nModel:\nOLS\nAdj. R-squared:\n0.164\n\n\nMethod:\nLeast Squares\nF-statistic:\n37.47\n\n\nDate:\nFri, 30 Jan 2026\nProb (F-statistic):\n5.08e-48\n\n\nTime:\n21:38:16\nLog-Likelihood:\n-4527.0\n\n\nNo. Observations:\n1307\nAIC:\n9070.\n\n\nDf Residuals:\n1299\nBIC:\n9111.\n\n\nDf Model:\n7\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-7.8056\n1.210\n-6.450\n0.000\n-10.180\n-5.432\n\n\nC(sex)[T.Male]\n-2.5110\n0.432\n-5.809\n0.000\n-3.359\n-1.663\n\n\nC(ses)[T.low]\n3.7458\n0.643\n5.822\n0.000\n2.484\n5.008\n\n\nC(ses)[T.middle]\n1.2568\n0.545\n2.307\n0.021\n0.188\n2.326\n\n\nC(smoking)[T.former]\n0.5416\n0.563\n0.962\n0.336\n-0.563\n1.646\n\n\nC(smoking)[T.never]\n0.6254\n0.549\n1.138\n0.255\n-0.452\n1.703\n\n\nssb\n-0.1207\n0.277\n-0.436\n0.663\n-0.664\n0.423\n\n\nage\n0.2452\n0.018\n13.333\n0.000\n0.209\n0.281\n\n\n\n\n\n\n\n\nOmnibus:\n571.852\nDurbin-Watson:\n2.068\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n3553.625\n\n\nSkew:\n1.936\nProb(JB):\n0.00\n\n\nKurtosis:\n10.090\nCond. No.\n325.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n\n\n\nStratified model: Obese\n\n\n\nOLS Regression Results\n\n\nDep. Variable:\ncvd_risk\nR-squared:\n0.254\n\n\nModel:\nOLS\nAdj. R-squared:\n0.246\n\n\nMethod:\nLeast Squares\nF-statistic:\n33.28\n\n\nDate:\nFri, 30 Jan 2026\nProb (F-statistic):\n6.32e-40\n\n\nTime:\n21:38:16\nLog-Likelihood:\n-2570.5\n\n\nNo. Observations:\n693\nAIC:\n5157.\n\n\nDf Residuals:\n685\nBIC:\n5193.\n\n\nDf Model:\n7\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-13.5429\n2.145\n-6.314\n0.000\n-17.754\n-9.332\n\n\nC(sex)[T.Male]\n-4.3177\n0.762\n-5.669\n0.000\n-5.813\n-2.822\n\n\nC(ses)[T.low]\n5.4913\n1.117\n4.916\n0.000\n3.298\n7.685\n\n\nC(ses)[T.middle]\n0.5848\n1.027\n0.570\n0.569\n-1.431\n2.601\n\n\nC(smoking)[T.former]\n-0.4153\n1.001\n-0.415\n0.678\n-2.380\n1.550\n\n\nC(smoking)[T.never]\n-0.3565\n0.970\n-0.368\n0.713\n-2.261\n1.548\n\n\nssb\n0.5130\n0.400\n1.281\n0.201\n-0.273\n1.299\n\n\nage\n0.3927\n0.032\n12.194\n0.000\n0.329\n0.456\n\n\n\n\n\n\n\n\nOmnibus:\n220.680\nDurbin-Watson:\n2.010\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n710.138\n\n\nSkew:\n1.524\nProb(JB):\n6.24e-155\n\n\nKurtosis:\n6.911\nCond. No.\n332.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "assessment/assessment_1b.html#bonus-option-b-variable-transformation",
    "href": "assessment/assessment_1b.html#bonus-option-b-variable-transformation",
    "title": "FB2NEP Assignment Notebook – Personal Dataset and Analysis",
    "section": "5.2 Bonus option B – Variable transformation",
    "text": "5.2 Bonus option B – Variable transformation\nApply a simple transformation to one key variable (for example SSB intake) and repeat the analysis.\nFor example, you might: - Log-transform SSB intake, or - Categorise SSB intake into low / medium / high consumption groups.\n\n5.2.1 Bonus question\n\nExplain why you chose this transformation.\nDescribe how the results compare with the main analysis.\nComment on whether the transformation improves interpretability or model fit.\n\nMaximum length: 150 words.\n\n# Example: log-transform SSB (adding a small constant)\ndf[\"log_ssb\"] = np.log(df[\"ssb\"] + 0.1)\n\nmodel_log = smf.ols(\n    \"cvd_risk ~ log_ssb + obese + age + C(sex) + C(ses) + C(smoking)\",\n    data=df\n).fit()\n\ndisplay(model_log.summary())\n\n\nOLS Regression Results\n\n\nDep. Variable:\ncvd_risk\nR-squared:\n0.206\n\n\nModel:\nOLS\nAdj. R-squared:\n0.203\n\n\nMethod:\nLeast Squares\nF-statistic:\n64.70\n\n\nDate:\nFri, 30 Jan 2026\nProb (F-statistic):\n1.79e-94\n\n\nTime:\n21:38:16\nLog-Likelihood:\n-7143.2\n\n\nNo. Observations:\n2000\nAIC:\n1.430e+04\n\n\nDf Residuals:\n1991\nBIC:\n1.435e+04\n\n\nDf Model:\n8\n\n\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nt\nP&gt;|t|\n[0.025\n0.975]\n\n\nIntercept\n-10.3437\n1.037\n-9.977\n0.000\n-12.377\n-8.310\n\n\nC(sex)[T.Male]\n-3.1386\n0.390\n-8.047\n0.000\n-3.903\n-2.374\n\n\nC(ses)[T.low]\n4.4529\n0.608\n7.318\n0.000\n3.260\n5.646\n\n\nC(ses)[T.middle]\n1.0725\n0.531\n2.020\n0.043\n0.031\n2.114\n\n\nC(smoking)[T.former]\n0.1463\n0.508\n0.288\n0.773\n-0.849\n1.142\n\n\nC(smoking)[T.never]\n0.2372\n0.494\n0.480\n0.631\n-0.731\n1.206\n\n\nlog_ssb\n0.0035\n0.293\n0.012\n0.990\n-0.571\n0.578\n\n\nobese\n1.8956\n0.408\n4.642\n0.000\n1.095\n2.696\n\n\nage\n0.2978\n0.017\n17.915\n0.000\n0.265\n0.330\n\n\n\n\n\n\n\n\nOmnibus:\n810.053\nDurbin-Watson:\n2.057\n\n\nProb(Omnibus):\n0.000\nJarque-Bera (JB):\n4326.150\n\n\nSkew:\n1.849\nProb(JB):\n0.00\n\n\nKurtosis:\n9.183\nCond. No.\n316.\n\n\n\nNotes:[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
  },
  {
    "objectID": "assessment/assessment_2.html",
    "href": "assessment/assessment_2.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "assessment/assessment_2.html#objective",
    "href": "assessment/assessment_2.html#objective",
    "title": "",
    "section": "1.1 Objective",
    "text": "1.1 Objective\n\nThis assessment evaluates your ability to translate epidemiological evidence into a coherent and plausible public health intervention. You will propose an intervention aimed at reducing harm related to sugar-sweetened beverage (SSB) intake and communicate your proposal clearly to a non-technical audience.\n\n\n1.1.1 Background\nYou are working as a junior researcher analysing observational data from a population-based study investigating associations between sugar-sweetened beverage (SSB) intake, obesity, and cardiovascular disease (CVD) risk.\nBefore any statistical analysis is conducted, the research team must make a number of substantive epidemiological decisions: how SSB intake should be estimated, what types of bias and censoring might arise, which variables plausibly confound the relationship of interest, and how causal relationships should be conceptualised.\nSection A of this assessment reflects this early analytical phase. You should answer the questions as if you were planning or reviewing a real-world epidemiological analysis, drawing on standard principles from nutritional epidemiology rather than on the specifics of the teaching dataset.\nSection B then represents the analytical phase, in which these ideas are explored using a realistic synthetic dataset provided for teaching purposes. The dataset is designed to mimic the structure and challenges of real observational nutrition data, while allowing each student to work with an individual dataset.\nAcross both sections, the emphasis is on epidemiological reasoning and interpretation, not on finding a single “correct” numerical answer."
  },
  {
    "objectID": "assessment/assessment_2.html#use-of-artificial-intelligence",
    "href": "assessment/assessment_2.html#use-of-artificial-intelligence",
    "title": "",
    "section": "1.2 Use of artificial intelligence",
    "text": "1.2 Use of artificial intelligence\nThe use of artificial intelligence tools (including generative AI for scripting, visuals, narration, or editing) is strictly prohibited in this assessment. All content must be your own work."
  },
  {
    "objectID": "assessment/assessment_2.html#what-you-must-submit",
    "href": "assessment/assessment_2.html#what-you-must-submit",
    "title": "",
    "section": "1.3 What you must submit",
    "text": "1.3 What you must submit\n\nA recorded presentation of approximately 7 minutes.\nA single file or link uploaded via the submission system.\n\nThere is no prescribed format. Any presentation style is acceptable (for example slides with narration, talking head, animation, performance-based formats), provided it communicates a clear public health intervention proposal."
  },
  {
    "objectID": "assessment/assessment_2.html#task",
    "href": "assessment/assessment_2.html#task",
    "title": "",
    "section": "1.4 Task",
    "text": "1.4 Task\nDesign and present a pitch for a public health intervention that addresses SSB intake and cardiometabolic risk.\nYour presentation should clearly address:\n\nThe public health problem and its relevance\nThe proposed intervention and its rationale\nThe target population and setting\nFeasibility and implementation considerations\nExpected benefits and possible unintended consequences"
  },
  {
    "objectID": "assessment/assessment_2.html#marking-rubric-and-assessment-criteria",
    "href": "assessment/assessment_2.html#marking-rubric-and-assessment-criteria",
    "title": "",
    "section": "1.5 Marking rubric and assessment criteria",
    "text": "1.5 Marking rubric and assessment criteria\nMarks are awarded for content, reasoning, and clarity, not production quality or format choice.\n\nProblem definition: Clear articulation of the public health issue, grounded in evidence\nIntervention rationale: Logical and evidence-informed justification\nTarget population: Clear and appropriate definition\nFeasibility: Consideration of practicality, barriers, and implementation\nPublic health impact: Discussion of expected benefits and potential harms\nCommunication quality: Clear, coherent, and engaging communication, regardless of format\n\n\n1.5.1 Mark descriptors\n\n70–100 (Excellent): Compelling, well-reasoned intervention grounded in evidence and clearly communicated.\n60–69 (Good): Coherent proposal with reasonable justification and clarity.\n50–59 (Adequate): Basic proposal with limited depth or critical engagement.\n&lt;50 (Poor): Unclear, unrealistic, or poorly justified proposal."
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html",
    "href": "notebooks/2.04_salt_reduction.html",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "",
    "text": "Learning Objectives: - Understand the UK salt reduction programme as a model public health intervention - Appreciate the value of objective biomarkers (urinary sodium) in evaluation - Work through health impact modelling: from intake change to CVD outcomes - Explore sensitivity analysis and uncertainty in policy evaluation - Critically assess the evidence for population-level dietary interventions"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#background-the-uk-salt-reduction-programme",
    "href": "notebooks/2.04_salt_reduction.html#background-the-uk-salt-reduction-programme",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "1 1. Background: The UK Salt Reduction Programme",
    "text": "1 1. Background: The UK Salt Reduction Programme\nThe UK’s salt reduction programme is widely regarded as one of the most successful population-level dietary interventions globally.\n\n1.1 Timeline\n\n2003: Food Standards Agency (FSA) launches voluntary salt reduction programme\n2006: Voluntary reformulation targets set for food industry\n2008-2014: Successive rounds of increasingly stringent targets\n2011: Responsibility transferred to Department of Health\n2014: Public Health England takes over\n\n\n\n1.2 Key Components\n\nVoluntary reformulation: Industry targets for salt content in processed foods\nLabelling: Traffic light front-of-pack labelling\nPublic awareness: Campaigns on salt and health\nMonitoring: Regular surveys of food composition and population intake\n\n\n\n1.3 Why Salt Matters\n\nHigh sodium intake raises blood pressure\nElevated blood pressure is a major risk factor for cardiovascular disease (CVD)\nMost dietary sodium comes from processed foods, not table salt\nPopulation-level reformulation can reduce intake without requiring individual behaviour change"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#setup",
    "href": "notebooks/2.04_salt_reduction.html#setup",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "2 2. Setup",
    "text": "2 2. Setup\n\n# ============================================================\n# Bootstrap cell\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\n\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\ncwd = pathlib.Path.cwd()\n\nif (cwd / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd\nelif (cwd.parent / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd.parent\nelse:\n    repo_root = cwd / REPO_DIR\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nscripts_dir = repo_root / \"scripts\"\nif str(scripts_dir) not in sys.path:\n    sys.path.insert(0, str(scripts_dir))\n\nprint(f\"Repository root: {repo_root}\")\nprint(\"Bootstrap completed successfully.\")\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom ipywidgets import interact, FloatSlider, IntSlider, VBox, HBox, Output\nimport ipywidgets as widgets\nfrom IPython.display import display\n\nfrom epi_utils import (\n    estimate_bp_change_from_sodium,\n    estimate_cvd_risk_change,\n    calculate_deaths_averted\n)\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = [10, 6]\nnp.random.seed(42)\n\nprint(\"Libraries loaded successfully.\")"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#measuring-salt-intake-the-gold-standard",
    "href": "notebooks/2.04_salt_reduction.html#measuring-salt-intake-the-gold-standard",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "3 3. Measuring Salt Intake: The Gold Standard",
    "text": "3 3. Measuring Salt Intake: The Gold Standard\n\n3.1 The Problem with Self-Report\nDietary assessment methods (FFQs, 24-hour recalls) have significant limitations for sodium: - Salt added during cooking/at table is hard to quantify - Sodium content varies within food categories - People underreport intake\n\n\n3.2 24-Hour Urinary Sodium\nThe gold standard for measuring sodium intake is 24-hour urine collection: - ~90% of ingested sodium is excreted in urine - Objective, not reliant on recall - Can detect true population changes\n\n\n3.3 Conversion\n\\[\\text{Salt (g)} = \\text{Sodium (mmol)} \\times 0.0584\\]\nOr approximately: Salt (g) ≈ Sodium (g) × 2.5\n\n# UK population salt intake data from 24-hour urinary sodium surveys\n# Source: National Diet and Nutrition Survey (NDNS) and dedicated salt surveys\n\nuk_salt_data = pd.DataFrame({\n    'year': [2000, 2005, 2008, 2011, 2014, 2019],\n    'salt_g_day': [9.5, 9.0, 8.6, 8.1, 8.0, 8.4],  # Illustrative values\n    'lower_ci': [9.1, 8.6, 8.2, 7.7, 7.6, 8.0],\n    'upper_ci': [9.9, 9.4, 9.0, 8.5, 8.4, 8.8],\n    'n': [1500, 1800, 1200, 1000, 800, 600]\n})\n\n# Government target\ntarget = 6.0\n\nprint(\"UK Population Salt Intake (24-hour urinary sodium)\")\nprint(\"=\" * 60)\ndisplay(uk_salt_data)\n\n\n# Visualise the trend\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.errorbar(uk_salt_data['year'], uk_salt_data['salt_g_day'],\n            yerr=[uk_salt_data['salt_g_day'] - uk_salt_data['lower_ci'],\n                  uk_salt_data['upper_ci'] - uk_salt_data['salt_g_day']],\n            fmt='o-', capsize=5, capthick=2, linewidth=2, markersize=10,\n            color='steelblue', label='Measured intake')\n\nax.axhline(y=target, color='green', linestyle='--', linewidth=2, label=f'Government target ({target}g)')\nax.axhline(y=5.0, color='red', linestyle=':', linewidth=2, label='WHO recommendation (5g)')\n\n# Shade the intervention period\nax.axvspan(2003, 2014, alpha=0.2, color='yellow', label='Active intervention period')\n\nax.set_xlabel('Year', fontsize=12)\nax.set_ylabel('Salt intake (g/day)', fontsize=12)\nax.set_title('UK Population Salt Intake: Measured by 24-Hour Urinary Sodium', fontsize=14)\nax.legend(loc='upper right')\nax.set_ylim(4, 11)\nax.set_xlim(1998, 2021)\n\nplt.tight_layout()\nplt.show()\n\n# Calculate reduction\nreduction = uk_salt_data['salt_g_day'].iloc[0] - uk_salt_data['salt_g_day'].iloc[-2]\npct_reduction = reduction / uk_salt_data['salt_g_day'].iloc[0] * 100\n\nprint(f\"\\nSalt intake reduction (2000-2014): {reduction:.1f} g/day ({pct_reduction:.0f}%)\")\nprint(f\"Note: Intake increased slightly between 2014-2019 after programme was deprioritised\")"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#the-causal-pathway-salt-blood-pressure-cvd",
    "href": "notebooks/2.04_salt_reduction.html#the-causal-pathway-salt-blood-pressure-cvd",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "4 4. The Causal Pathway: Salt → Blood Pressure → CVD",
    "text": "4 4. The Causal Pathway: Salt → Blood Pressure → CVD\n\n4.1 Evidence Base\nThe causal chain from salt to cardiovascular disease is well established:\n\nSalt → Blood Pressure: Meta-analyses of RCTs show ~1 mmHg reduction in SBP per 1g/day salt reduction\nBlood Pressure → CVD: Observational studies and trials show clear dose-response\n\n\n\n4.2 Key Parameters from Literature\n\n\n\n\n\n\n\n\nParameter\nValue\nSource\n\n\n\n\nSBP change per 1g salt reduction\n-1.0 to -1.5 mmHg\nHe & MacGregor meta-analysis\n\n\nStroke risk per 2 mmHg SBP\n-10%\nBPLTTC\n\n\nCHD risk per 2 mmHg SBP\n-7%\nBPLTTC\n\n\n\n\ndef estimate_bp_change(salt_reduction_g, effect_per_g=1.2):\n    \"\"\"\n    Estimate systolic blood pressure change from salt reduction.\n    \n    Parameters\n    ----------\n    salt_reduction_g : float\n        Reduction in salt intake (g/day)\n    effect_per_g : float\n        mmHg SBP reduction per g salt reduction (default 1.2)\n    \n    Returns\n    -------\n    float\n        Estimated SBP reduction (mmHg)\n    \"\"\"\n    return salt_reduction_g * effect_per_g\n\n\ndef estimate_cvd_risk_reduction(sbp_reduction_mmhg, outcome='stroke'):\n    \"\"\"\n    Estimate relative risk reduction for CVD outcomes from BP reduction.\n    \n    Parameters\n    ----------\n    sbp_reduction_mmhg : float\n        Reduction in systolic blood pressure (mmHg)\n    outcome : str\n        'stroke' or 'chd'\n    \n    Returns\n    -------\n    float\n        Relative risk reduction (as proportion, e.g., 0.10 = 10% reduction)\n    \"\"\"\n    # Risk reduction per 2 mmHg SBP reduction\n    rr_per_2mmhg = {'stroke': 0.10, 'chd': 0.07}\n    \n    # Calculate for actual BP change\n    risk_reduction = (sbp_reduction_mmhg / 2) * rr_per_2mmhg[outcome]\n    \n    return risk_reduction\n\n\n# Apply to UK data\nsalt_reduction = 1.5  # g/day achieved 2000-2014\n\nsbp_reduction = estimate_bp_change(salt_reduction)\nstroke_risk_reduction = estimate_cvd_risk_reduction(sbp_reduction, 'stroke')\nchd_risk_reduction = estimate_cvd_risk_reduction(sbp_reduction, 'chd')\n\nprint(\"Estimated Health Impact of UK Salt Reduction\")\nprint(\"=\" * 50)\nprint(f\"\\nSalt intake reduction: {salt_reduction:.1f} g/day\")\nprint(f\"Estimated SBP reduction: {sbp_reduction:.1f} mmHg\")\nprint(f\"\\nEstimated relative risk reduction:\")\nprint(f\"  Stroke: {stroke_risk_reduction:.1%}\")\nprint(f\"  CHD: {chd_risk_reduction:.1%}\")"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#health-impact-modelling",
    "href": "notebooks/2.04_salt_reduction.html#health-impact-modelling",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "5 5. Health Impact Modelling",
    "text": "5 5. Health Impact Modelling\nTo estimate deaths or DALYs averted, we need to combine:\n\nRelative risk reduction (from above)\nBaseline event rates (from vital statistics)\nPopulation size (from census)\n\n\\[\\text{Deaths averted} = \\text{Population} \\times \\text{Baseline rate} \\times \\text{RR reduction}\\]\n\n# UK baseline data (illustrative, approximate)\nuk_population_adult = 52_000_000  # Adults 18+\n\n# Annual CVD mortality rates (per 100,000)\nbaseline_rates = {\n    'stroke_mortality': 45,  # per 100,000 per year\n    'chd_mortality': 95,     # per 100,000 per year\n}\n\ndef calculate_deaths_averted_manual(population, baseline_rate_per_100k, \n                                    risk_reduction, years=1):\n    \"\"\"\n    Calculate deaths averted from a risk reduction.\n    \n    Parameters\n    ----------\n    population : int\n        Population size\n    baseline_rate_per_100k : float\n        Baseline mortality rate per 100,000 per year\n    risk_reduction : float\n        Relative risk reduction (as proportion)\n    years : int\n        Number of years\n    \n    Returns\n    -------\n    float\n        Estimated deaths averted\n    \"\"\"\n    baseline_deaths = (population / 100_000) * baseline_rate_per_100k * years\n    deaths_averted = baseline_deaths * risk_reduction\n    return deaths_averted\n\n\n# Calculate for UK\nstroke_deaths_averted = calculate_deaths_averted_manual(\n    uk_population_adult,\n    baseline_rates['stroke_mortality'],\n    stroke_risk_reduction,\n    years=10  # Over 10 years\n)\n\nchd_deaths_averted = calculate_deaths_averted_manual(\n    uk_population_adult,\n    baseline_rates['chd_mortality'],\n    chd_risk_reduction,\n    years=10\n)\n\ntotal_cvd_deaths_averted = stroke_deaths_averted + chd_deaths_averted\n\nprint(\"Estimated Deaths Averted (10-year period)\")\nprint(\"=\" * 50)\nprint(f\"\\nSalt reduction: {salt_reduction:.1f} g/day\")\nprint(f\"Population: {uk_population_adult:,}\")\nprint(f\"\\nStroke deaths averted: {stroke_deaths_averted:,.0f}\")\nprint(f\"CHD deaths averted: {chd_deaths_averted:,.0f}\")\nprint(f\"\\nTotal CVD deaths averted: {total_cvd_deaths_averted:,.0f}\")\nprint(f\"Per year: {total_cvd_deaths_averted/10:,.0f}\")"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#sensitivity-analysis",
    "href": "notebooks/2.04_salt_reduction.html#sensitivity-analysis",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "6 6. Sensitivity Analysis",
    "text": "6 6. Sensitivity Analysis\nOur estimates depend on several uncertain parameters. Sensitivity analysis explores how results change when we vary these assumptions.\n\n# One-way sensitivity analysis: vary the BP effect per gram of salt\n\neffect_range = np.linspace(0.5, 2.0, 20)  # mmHg per g salt\ndeaths_by_effect = []\n\nfor effect in effect_range:\n    sbp = estimate_bp_change(salt_reduction, effect_per_g=effect)\n    stroke_rr = estimate_cvd_risk_reduction(sbp, 'stroke')\n    chd_rr = estimate_cvd_risk_reduction(sbp, 'chd')\n    \n    stroke_deaths = calculate_deaths_averted_manual(\n        uk_population_adult, baseline_rates['stroke_mortality'], stroke_rr, years=10)\n    chd_deaths = calculate_deaths_averted_manual(\n        uk_population_adult, baseline_rates['chd_mortality'], chd_rr, years=10)\n    \n    deaths_by_effect.append(stroke_deaths + chd_deaths)\n\n# Plot\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.plot(effect_range, np.array(deaths_by_effect)/1000, 'b-', linewidth=2)\nax.axvline(x=1.2, color='red', linestyle='--', label='Base case (1.2 mmHg/g)')\nax.fill_between(effect_range, 0, np.array(deaths_by_effect)/1000, alpha=0.3)\n\nax.set_xlabel('Blood pressure effect (mmHg per g salt reduction)', fontsize=12)\nax.set_ylabel('CVD deaths averted (thousands, 10 years)', fontsize=12)\nax.set_title('Sensitivity Analysis: Effect of BP-Salt Relationship', fontsize=14)\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"Deaths averted range: {min(deaths_by_effect):,.0f} to {max(deaths_by_effect):,.0f}\")\n\n\n# Probabilistic sensitivity analysis (Monte Carlo)\n\nn_simulations = 10000\n\n# Parameter distributions\nsalt_reduction_samples = np.random.normal(1.5, 0.3, n_simulations)  # g/day\neffect_per_g_samples = np.random.normal(1.2, 0.3, n_simulations)    # mmHg/g\nstroke_rr_per_2mmhg_samples = np.random.normal(0.10, 0.02, n_simulations)\nchd_rr_per_2mmhg_samples = np.random.normal(0.07, 0.015, n_simulations)\n\ndeaths_averted_samples = []\n\nfor i in range(n_simulations):\n    sbp = salt_reduction_samples[i] * effect_per_g_samples[i]\n    \n    stroke_rr = (sbp / 2) * stroke_rr_per_2mmhg_samples[i]\n    chd_rr = (sbp / 2) * chd_rr_per_2mmhg_samples[i]\n    \n    stroke_deaths = (uk_population_adult / 100_000) * baseline_rates['stroke_mortality'] * stroke_rr * 10\n    chd_deaths = (uk_population_adult / 100_000) * baseline_rates['chd_mortality'] * chd_rr * 10\n    \n    deaths_averted_samples.append(stroke_deaths + chd_deaths)\n\ndeaths_averted_samples = np.array(deaths_averted_samples)\n\n# Results\nmean_deaths = np.mean(deaths_averted_samples)\nci_lower = np.percentile(deaths_averted_samples, 2.5)\nci_upper = np.percentile(deaths_averted_samples, 97.5)\n\nprint(\"Probabilistic Sensitivity Analysis Results\")\nprint(\"=\" * 50)\nprint(f\"\\nMean deaths averted (10 years): {mean_deaths:,.0f}\")\nprint(f\"95% credible interval: [{ci_lower:,.0f} - {ci_upper:,.0f}]\")\n\n\n# Visualise distribution\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.hist(deaths_averted_samples/1000, bins=50, density=True, alpha=0.7, color='steelblue')\nax.axvline(x=mean_deaths/1000, color='red', linestyle='-', linewidth=2, label=f'Mean: {mean_deaths/1000:.0f}k')\nax.axvline(x=ci_lower/1000, color='orange', linestyle='--', linewidth=2, label=f'95% CI: [{ci_lower/1000:.0f}k - {ci_upper/1000:.0f}k]')\nax.axvline(x=ci_upper/1000, color='orange', linestyle='--', linewidth=2)\n\nax.set_xlabel('CVD deaths averted (thousands, 10 years)', fontsize=12)\nax.set_ylabel('Density', fontsize=12)\nax.set_title('Probabilistic Sensitivity Analysis: Deaths Averted', fontsize=14)\nax.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#interactive-health-impact-model",
    "href": "notebooks/2.04_salt_reduction.html#interactive-health-impact-model",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "7 7. Interactive Health Impact Model",
    "text": "7 7. Interactive Health Impact Model\nExplore how different assumptions affect the estimated health impact.\n\n# Interactive model\nsalt_slider = FloatSlider(value=1.5, min=0.5, max=3.0, step=0.1, description='Salt reduction (g):')\nbp_effect_slider = FloatSlider(value=1.2, min=0.5, max=2.0, step=0.1, description='BP effect (mmHg/g):')\nyears_slider = IntSlider(value=10, min=1, max=20, step=1, description='Time horizon (years):')\n\noutput = Output()\n\ndef update_model(change=None):\n    salt_red = salt_slider.value\n    bp_effect = bp_effect_slider.value\n    years = years_slider.value\n    \n    sbp = salt_red * bp_effect\n    stroke_rr = (sbp / 2) * 0.10\n    chd_rr = (sbp / 2) * 0.07\n    \n    stroke_deaths = calculate_deaths_averted_manual(\n        uk_population_adult, baseline_rates['stroke_mortality'], stroke_rr, years)\n    chd_deaths = calculate_deaths_averted_manual(\n        uk_population_adult, baseline_rates['chd_mortality'], chd_rr, years)\n    total = stroke_deaths + chd_deaths\n    \n    with output:\n        output.clear_output(wait=True)\n        \n        print(f\"\\n{'='*50}\")\n        print(f\"HEALTH IMPACT ESTIMATE\")\n        print(f\"{'='*50}\")\n        print(f\"\\nInputs:\")\n        print(f\"  Salt reduction: {salt_red:.1f} g/day\")\n        print(f\"  BP effect: {bp_effect:.1f} mmHg per g salt\")\n        print(f\"  Time horizon: {years} years\")\n        print(f\"\\nIntermediate calculations:\")\n        print(f\"  SBP reduction: {sbp:.1f} mmHg\")\n        print(f\"  Stroke risk reduction: {stroke_rr:.1%}\")\n        print(f\"  CHD risk reduction: {chd_rr:.1%}\")\n        print(f\"\\nEstimated deaths averted:\")\n        print(f\"  Stroke: {stroke_deaths:,.0f}\")\n        print(f\"  CHD: {chd_deaths:,.0f}\")\n        print(f\"  TOTAL: {total:,.0f}\")\n        print(f\"  Per year: {total/years:,.0f}\")\n\nfor slider in [salt_slider, bp_effect_slider, years_slider]:\n    slider.observe(update_model, names='value')\n\nprint(\"Adjust the parameters to see how the health impact estimate changes:\")\ndisplay(VBox([salt_slider, bp_effect_slider, years_slider, output]))\nupdate_model()"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#strengths-of-the-uk-programme",
    "href": "notebooks/2.04_salt_reduction.html#strengths-of-the-uk-programme",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "8 8. Strengths of the UK Programme",
    "text": "8 8. Strengths of the UK Programme\n\n8.1 Why Is This a Model Intervention?\n\nObjective outcome measurement: 24-hour urinary sodium provides unbiased evidence of population intake change\nClear causal pathway: Salt → BP → CVD is well established from multiple study designs\nPopulation-level approach: Reformulation reduces intake without requiring individual behaviour change\nRegular monitoring: Repeated surveys allow tracking of progress\nIndustry engagement: Voluntary targets achieved significant reformulation\n\n\n\n8.2 Limitations\n\nNo control group: Cannot prove causality at population level\nOther secular trends: Diet and CVD risk changed for other reasons too\nSurvey representativeness: 24-hour urine collection has low response rates\nLag times: Full health impact takes years to materialise"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#discussion-questions",
    "href": "notebooks/2.04_salt_reduction.html#discussion-questions",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "9 9. Discussion Questions",
    "text": "9 9. Discussion Questions\n\nThe measurement advantage: Why is urinary sodium considered more reliable than dietary assessment? What are its limitations?\nVoluntary vs mandatory: The UK used voluntary industry targets. What are the trade-offs compared to mandatory salt limits (as used in some countries)?\nEquity implications: Is reformulation equitable? Who benefits most from processed food reformulation?\nTransferability: Would this approach work for sugar reduction? Why might sugar be harder than salt?\nThe 2019 uptick: Salt intake appeared to increase between 2014-2019 after the programme was deprioritised. What does this suggest about the sustainability of voluntary approaches?"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#exercises",
    "href": "notebooks/2.04_salt_reduction.html#exercises",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "10 10. Exercises",
    "text": "10 10. Exercises\n\n10.1 Exercise 1: Calculate Impact of Achieving the 6g Target\nIf the UK achieved the 6g/day target (from current ~8g), estimate the additional deaths averted per year.\n\n# YOUR CODE HERE\n\n\n\n\n10.2 Exercise 2: Sensitivity to Baseline Rate\nRepeat the analysis for a country with higher CVD mortality (e.g., Eastern Europe with ~200 per 100,000 for CHD). How does this affect the absolute number of deaths averted?\n\n# YOUR CODE HERE"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#summary",
    "href": "notebooks/2.04_salt_reduction.html#summary",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "11 Summary",
    "text": "11 Summary\n\nThe UK salt reduction programme achieved a measurable ~15% reduction in population salt intake\n24-hour urinary sodium provides objective evidence of population intake change\nHealth impact modelling combines intake changes, dose-response relationships, and baseline rates\nSensitivity analysis reveals the uncertainty in estimates (wide credible intervals)\nThe programme demonstrates the value of objective biomarkers in evaluating dietary interventions"
  },
  {
    "objectID": "notebooks/2.04_salt_reduction.html#references",
    "href": "notebooks/2.04_salt_reduction.html#references",
    "title": "2.04 – Case Study: Salt Reduction (With Objective Measures)",
    "section": "12 References",
    "text": "12 References\n\nHe FJ, Pombo-Rodrigues S, MacGregor GA. (2014). Salt reduction in England from 2003 to 2011. BMJ Open.\nHe FJ, Li J, MacGregor GA. (2013). Effect of longer term modest salt reduction on blood pressure. Cochrane Database.\nLewington S et al. (2002). Age-specific relevance of usual blood pressure to vascular mortality. Lancet.\nPublic Health England. (2020). Salt targets 2024."
  },
  {
    "objectID": "notebooks/1.07_data_transformation.html",
    "href": "notebooks/1.07_data_transformation.html",
    "title": "1.07 – Data Transformation and Preparation for Modelling",
    "section": "",
    "text": "Version 0.0.7\nIn this workbook we explore how to prepare variables for statistical modelling.\nWe consider:\nThe focus is on understanding why transformations are used in epidemiological analyses, and how they affect model assumptions and interpretation.\nWe begin with some background theory, using small simulated examples to illustrate:\nWe then work with the FB2NEP synthetic cohort to apply these ideas in practice.\nRun the first code cell to set up the repository and load the data.\n# ============================================================\n# FB2NEP bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the fb2nep-epi repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads and runs scripts/bootstrap.py.\n# - Makes the main dataset available as the variable `df`.\n#\n# Important:\n# - You may see messages printed below (for example from pip\n#   or from the bootstrap script). This is expected.\n# - You may also see WARNINGS (often in yellow). In most cases\n#   these are harmless and can be ignored for this module.\n# - The main thing to watch for is a red error traceback\n#   (for example FileNotFoundError, ModuleNotFoundError).\n#   If that happens, please re-run this cell first. If the\n#   error persists, ask for help.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\nimport importlib.util\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\n# REPO_URL: address of the GitHub repository.\n# REPO_DIR: folder name that will be created when cloning.\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the fb2nep-epi repository\n# ------------------------------------------------------------\n# In local Jupyter, you may already be inside the repository,\n# for example in fb2nep-epi/notebooks.\n#\n# In Colab, the default working directory is /content, so\n# we need to clone the repository into /content/fb2nep-epi\n# and then change into that folder.\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/bootstrap.py exists here)\nif (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n    repo_root = cwd\n\n# Case B: we are outside the repository (for example in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if it is not present yet\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change the working directory to the repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nprint(f\"Repository root set to: {repo_root}\")\n\n# ------------------------------------------------------------\n# 2. Load scripts/bootstrap.py as a module and call init()\n# ------------------------------------------------------------\n# The shared bootstrap script contains all logic to:\n# - Ensure that required Python packages are installed.\n# - Ensure that the synthetic dataset exists (and generate it\n#   if needed).\n# - Load the dataset into a pandas DataFrame.\n#\n# We load the script as a normal Python module (fb2nep_bootstrap)\n# and then call its init() function.\nbootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n\nif not bootstrap_path.is_file():\n    raise FileNotFoundError(\n        f\"Could not find {bootstrap_path}. \"\n        \"Please check that the fb2nep-epi repository structure is intact.\"\n    )\n\n# Create a module specification from the file\nspec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\nbootstrap = importlib.util.module_from_spec(spec)\nsys.modules[\"fb2nep_bootstrap\"] = bootstrap\n\n# Execute the bootstrap script in the context of this module\nspec.loader.exec_module(bootstrap)\n\n# The init() function is defined in scripts/bootstrap.py.\n# It returns:\n# - df   : the main synthetic cohort as a pandas DataFrame.\n# - CTX  : a small context object with paths, flags and settings.\ndf, CTX = bootstrap.init()\n\n# Optionally expose a few additional useful variables from the\n# bootstrap module (if they exist). These are not essential for\n# most analyses, but can be helpful for advanced use.\nfor name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n    if hasattr(bootstrap, name):\n        globals()[name] = getattr(bootstrap, name)\n\nprint(\"Bootstrap completed successfully.\")\nprint(\"The main dataset is available as the variable `df`.\")\nprint(\"The context object is available as `CTX`.\")"
  },
  {
    "objectID": "notebooks/1.07_data_transformation.html#background-theory-with-simple-examples",
    "href": "notebooks/1.07_data_transformation.html#background-theory-with-simple-examples",
    "title": "1.07 – Data Transformation and Preparation for Modelling",
    "section": "1 Background theory with simple examples",
    "text": "1 Background theory with simple examples\nBefore using the FB2NEP dataset, we illustrate a few key ideas with small simulated examples:\n\nWhat a normal distribution looks like.\nWhat residuals are and why we care about their distribution.\nWhat heteroskedasticity looks like in a residual plot.\nHow one might decide whether a transformation is needed.\n\nThese examples are deliberately simple and are not based on real nutritional data.\nWe first import the Python packages required for this workbook:\n\nnumpy and pandas for data handling.\nmatplotlib for visualisation.\nscipy.stats for skewness and Box–Cox transformation.\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\n\n%matplotlib inline\n\n# For reproducible simulated examples\nnp.random.seed(11088)\n\n\n1.1 Background 1 – What is a normal distribution?\nMany statistical methods assume that some quantity is approximately normally distributed. The normal distribution is the familiar “bell-shaped” curve:\n\nMost values lie near the mean.\nValues become less frequent as we move further away from the mean.\nThe distribution is symmetric around the mean.\n\nWe now simulate 10,000 values from a normal distribution with mean 0 and standard deviation 1 and plot a histogram.\n\n# Simulate data from a standard normal distribution\n# mean = 0, standard deviation = 1\n\nx_norm = np.random.normal(loc=0.0, scale=1.0, size=10_000)\n\nplt.figure(figsize=(6, 4))\nplt.hist(x_norm, bins=30)\nplt.xlabel(\"Value\")\nplt.ylabel(\"Frequency\")\nplt.title(\"Simulated standard normal distribution\")\nplt.tight_layout()\nplt.show()\n\nprint(\"Mean:\", x_norm.mean())\nprint(\"Standard deviation:\", x_norm.std())\nprint(\"Skewness:\", stats.skew(x_norm))\n\n\n\n1.2 Background 2 – What are residuals?\nWe start by looking at how well a simple linear model reproduces the data:\n\nThe fitted value is the model’s prediction for each observation.\nPlotting observed vs fitted shows whether the model captures the average pattern.\n\nWe then look at residuals (observed − fitted):\n\nResiduals show what is left after the model has explained as much variation as it can.\nTheir distribution (for example, symmetry, spread) is what many methods make assumptions about.\n\n\n# Example 1: residuals approximately normal\n# ----------------------------------------\n# We simulate x from a normal distribution and y from a linear relation\n# plus normal (Gaussian) error.\n\nn = 500\nx1 = np.random.normal(loc=0.0, scale=1.0, size=n)\nepsilon1 = np.random.normal(loc=0.0, scale=1.0, size=n)  # normal residuals\ny1 = 2.0 + 1.5 * x1 + epsilon1\n\n# Fit a straight line y1 ~ x1 using numpy.polyfit\nslope1, intercept1 = np.polyfit(x1, y1, 1)\nfitted1 = intercept1 + slope1 * x1\nresiduals1 = y1 - fitted1\n\n# Example 2: residuals clearly non-normal\n# ---------------------------------------\n# We use the same x, but generate skewed error terms (for example,\n# from an exponential distribution).\n\nepsilon2 = np.random.exponential(scale=1.0, size=n)  # skewed residuals\ny2 = 2.0 + 1.5 * x1 + epsilon2\n\nslope2, intercept2 = np.polyfit(x1, y2, 1)\nfitted2 = intercept2 + slope2 * x1\nresiduals2 = y2 - fitted2\n\n# Plot observed vs fitted values for the two examples\n# ---------------------------------------------------\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\ndef add_identity_line(ax, x, y):\n    \"\"\"Add a y = x reference line over the range of fitted/observed values.\"\"\"\n    lo = min(x.min(), y.min())\n    hi = max(x.max(), y.max())\n    ax.plot([lo, hi], [lo, hi], color=\"black\", linewidth=1)\n\n# Example 1: approximately normal residuals\naxes[0].scatter(fitted1, y1, alpha=0.5)\nadd_identity_line(axes[0], fitted1, y1)\naxes[0].set_title(\"Observed vs fitted – normal errors\")\naxes[0].set_xlabel(\"Fitted value\")\naxes[0].set_ylabel(\"Observed value\")\n\n# Example 2: skewed residuals\naxes[1].scatter(fitted2, y2, alpha=0.5)\nadd_identity_line(axes[1], fitted2, y2)\naxes[1].set_title(\"Observed vs fitted – skewed errors\")\naxes[1].set_xlabel(\"Fitted value\")\naxes[1].set_ylabel(\"Observed value\")\n\nplt.tight_layout()\nplt.show()\n\nIn both panels, the points lie roughly along the 45° line, so the average relation is captured well by the straight line model. The difference between the two examples will become clearer when we examine the residuals.\n\n# Histograms of residuals for both cases\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\naxes[0].hist(residuals1, bins=30)\naxes[0].set_title(\"Residuals – approximately normal\")\naxes[0].set_xlabel(\"Residual\")\naxes[0].set_ylabel(\"Frequency\")\n\naxes[1].hist(residuals2, bins=30)\naxes[1].set_title(\"Residuals – clearly skewed\")\naxes[1].set_xlabel(\"Residual\")\naxes[1].set_ylabel(\"Frequency\")\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Example 1 residual skewness:\", stats.skew(residuals1))\nprint(\"Example 2 residual skewness:\", stats.skew(residuals2))\n\n# Residuals vs fitted values\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\n\naxes[0].scatter(fitted1, residuals1, alpha=0.5)\naxes[0].axhline(0.0, color=\"black\", linewidth=1)\naxes[0].set_title(\"Residuals vs fitted – normal errors\")\naxes[0].set_xlabel(\"Fitted value\")\naxes[0].set_ylabel(\"Residual\")\n\naxes[1].scatter(fitted2, residuals2, alpha=0.5)\naxes[1].axhline(0.0, color=\"black\", linewidth=1)\naxes[1].set_title(\"Residuals vs fitted – skewed errors\")\naxes[1].set_xlabel(\"Fitted value\")\naxes[1].set_ylabel(\"Residual\")\n\nplt.tight_layout()\nplt.show()\n\nExercise – residuals and assumptions\n\nWhich example looks closer to the ideal of symmetric, roughly constant residuals?\nIn which example would you be more confident about using methods that assume normally distributed residuals with constant variance?\n\n\n\n1.3 Background 3 – Heteroskedasticity (changing variance)\nIn many real data sets, the spread of residuals is not constant across the range of fitted values. If the residuals fan out as the predicted value increases, this is called heteroskedasticity.\nWe now simulate a simple example where the variance of the error term increases with the level of x and show a residual plot.\n\n# Simulate heteroskedastic data\n# -----------------------------\n# We let the standard deviation of the error term increase with |x|.\n\nx_h = np.linspace(-3, 3, n)\nsigma_h = 0.5 + 0.5 * np.abs(x_h)  # larger variance for large |x|\nepsilon_h = np.random.normal(loc=0.0, scale=sigma_h)\ny_h = 1.0 + 2.0 * x_h + epsilon_h\n\nslope_h, intercept_h = np.polyfit(x_h, y_h, 1)\nfitted_h = intercept_h + slope_h * x_h\nresiduals_h = y_h - fitted_h\n\nplt.figure(figsize=(6, 4))\nplt.scatter(fitted_h, residuals_h, alpha=0.5)\nplt.axhline(0.0, color=\"black\", linewidth=1)\nplt.xlabel(\"Fitted value\")\nplt.ylabel(\"Residual\")\nplt.title(\"Heteroskedasticity: residuals fan out with fitted value\")\nplt.tight_layout()\nplt.show()\n\n\n\n1.4 Background 4 – How to decide whether to transform\nIn practice there is no automatic rule for when to transform a variable. Analysts usually combine:\n\nExploratory plots of the variable itself (histograms, density plots) to see skewness.\nResidual plots from a model fitted on the original scale.\nSubject-matter knowledge, for example whether multiplicative (percentage) effects are plausible.\n\nSome practical guidelines:\n\nIf residuals are approximately symmetric and the variance is roughly constant across fitted values, a transformation is often unnecessary.\nIf the predictor is strongly right-skewed and residuals are clearly non-normal or heteroskedastic, a log or Box–Cox transformation can help.\nIf predictors are on very different scales and you mainly care about comparing effect sizes, z-scoring can be useful.\nTransformations should also respect interpretability: a modest gain in “niceness” of residual plots may not justify a scale that is hard to explain."
  },
  {
    "objectID": "notebooks/1.07_data_transformation.html#initial-inspection-of-the-fb2nep-data",
    "href": "notebooks/1.07_data_transformation.html#initial-inspection-of-the-fb2nep-data",
    "title": "1.07 – Data Transformation and Preparation for Modelling",
    "section": "2 1. Initial inspection of the FB2NEP data",
    "text": "2 1. Initial inspection of the FB2NEP data\nWe now return to the FB2NEP synthetic cohort. Before transforming variables, we:\n\nCheck that the variables of interest exist.\nObtain a first impression of their ranges and types.\n\nIn this workbook we will mainly use:\n\nred_meat_g_d (or energy_kcal as fallback) as an example of a skewed exposure.\nBMI (kg/m²) and SBP (systolic blood pressure, mmHg) as common continuous variables.\n\n\nfrom IPython.display import display\n\n# Show the first rows (participants) of the dataset\ndisplay(df.head())\n\n# Show data types of a subset of variables that we will use later.\ncols_of_interest = [\n    c for c in [\"red_meat_g_d\", \"energy_kcal\", \"BMI\", \"SBP\"] if c in df.columns\n]\nprint(\"\\nColumns of interest:\", cols_of_interest)\ndisplay(df[cols_of_interest].dtypes)"
  },
  {
    "objectID": "notebooks/1.07_data_transformation.html#log-transformation-nutrition-data-and-log-normality",
    "href": "notebooks/1.07_data_transformation.html#log-transformation-nutrition-data-and-log-normality",
    "title": "1.07 – Data Transformation and Preparation for Modelling",
    "section": "3 2. Log transformation – nutrition data and log-normality",
    "text": "3 2. Log transformation – nutrition data and log-normality\nMany dietary and biomarker variables are right-skewed: most participants have low or moderate values, with a long tail of high values. In nutrition, it is often more plausible to think in terms of proportional changes (for example, doubling intake) than in terms of fixed absolute changes.\nIf a variable is approximately log-normal (that is, its logarithm is roughly normal), then:\n\nThe variable itself is right-skewed.\nThe log-transformed variable is closer to a normal distribution.\nEffects can often be interpreted in terms of percentage or fold-changes.\n\nWe now inspect the distribution of a skewed dietary variable and apply a log transformation.\n\n# Choose a variable that is likely to be right-skewed.\nvar = \"red_meat_g_d\" if \"red_meat_g_d\" in df.columns else \"energy_kcal\"\nprint(f\"Using variable: {var}\")\n\n# Drop missing values for plotting and computing skewness.\nx = df[var].dropna()\n\nplt.figure(figsize=(6, 4))\nx.hist(bins=30)\nplt.xlabel(var)\nplt.ylabel(\"Number of participants\")\nplt.title(f\"Distribution of {var}\")\nplt.tight_layout()\nplt.show()\n\nprint(\"Number of non-missing values:\", x.shape[0])\nprint(\"Mean:\", x.mean())\nprint(\"Median:\", x.median())\nprint(\"Skewness:\", stats.skew(x))\n\nLike most dietary data, meat intake is skewed - other variables are often even more skewed.\nWhat happens if we log-transform it?\n\nfrom scripts.helpers_tables import log_transform, plot_hist_pair\n\n# Use a small constant to avoid problems if there are zeros.\nlog_const = 0.1\nlog_var_name = \"log_\" + var\ndf[log_var_name] = log_transform(df[var], constant=log_const)\n\nprint(f\"Created column: {log_var_name} (log-transformed {var})\")\n\n# Compare skewness before and after log transformation.\nx_log = df[log_var_name].dropna()\nprint(\"Skewness (original):\", stats.skew(x))\nprint(\"Skewness (log-transformed):\", stats.skew(x_log))\n\nplot_hist_pair(\n    original=df[var],\n    transformed=df[log_var_name],\n    original_label=var,\n    transformed_label=f\"log({var} + {log_const})\",\n)\n\n\n3.1 Exercise – log transformation\n\nCompare the histograms and skewness values before and after log transformation.\nDoes the log-transformed variable look more symmetric?\nIf you were to regress SBP on this exposure, would you consider using the log scale rather than the original scale? Why or why not?"
  },
  {
    "objectID": "notebooks/1.07_data_transformation.html#standardisation-z-scores-making-variables-comparable",
    "href": "notebooks/1.07_data_transformation.html#standardisation-z-scores-making-variables-comparable",
    "title": "1.07 – Data Transformation and Preparation for Modelling",
    "section": "4 3. Standardisation (z-scores) – making variables comparable",
    "text": "4 3. Standardisation (z-scores) – making variables comparable\nVariables such as BMI and SBP are measured on different scales and in different units. When several such variables are included in a model, it can be useful to standardise them so that:\n\nThe mean is 0.\nThe standard deviation is 1.\n\nThese standardised values are called z-scores. They are defined as:\n\\[ z_i = \\frac{x_i - \\bar{x}}{s_x} \\]\nwhere \\(\\bar{x}\\) is the sample mean and \\(s_x\\) is the sample standard deviation.\nIn psychology and some areas of epidemiology it is common to report effects per 1 standard deviation increase, because this makes effect sizes comparable across different variables.\n\nfrom scripts.helpers_tables import z_score\n\nfor v in [\"BMI\", \"SBP\"]:\n    if v in df.columns:\n        z_name = \"z_\" + v\n        df[z_name] = z_score(df[v])\n        print(f\"\\nSummary of z-scored {v} (stored in {z_name}):\")\n        print(df[z_name].describe())\n\n\n# Visualisation: compare the distributions of BMI and z-scored BMI.\nif \"BMI\" in df.columns:\n    plt.figure(figsize=(10, 4))\n\n    plt.subplot(1, 2, 1)\n    df[\"BMI\"].dropna().hist(bins=30)\n    plt.xlabel(\"BMI (kg/m²)\")\n    plt.ylabel(\"Number of participants\")\n    plt.title(\"BMI (original scale)\")\n\n    plt.subplot(1, 2, 2)\n    df[\"z_BMI\"].dropna().hist(bins=30)\n    plt.xlabel(\"z-BMI\")\n    plt.ylabel(\"Number of participants\")\n    plt.title(\"BMI (z-score)\")\n\n    plt.tight_layout()\n    plt.show()\n\n\n4.1 Exercise – interpretation of z-scores\n\nCheck the summary output for z_BMI and z_SBP.\n\nAre the means close to 0 and the standard deviations close to 1?\n\nImagine a regression model where the coefficient for z_BMI is 2.5 mmHg.\n\nHow would you describe this effect in words (per 1 standard deviation increase in BMI)?\nHow does this compare to a model where BMI is used in kg/m² without standardisation?"
  },
  {
    "objectID": "notebooks/1.07_data_transformation.html#boxcox-transformations-a-flexible-family",
    "href": "notebooks/1.07_data_transformation.html#boxcox-transformations-a-flexible-family",
    "title": "1.07 – Data Transformation and Preparation for Modelling",
    "section": "5 4. Box–Cox transformations – a flexible family",
    "text": "5 4. Box–Cox transformations – a flexible family\nThe log transformation is a special case of a more general family of power transformations called Box–Cox transformations. Box–Cox chooses an exponent (lambda, \\(\\lambda\\)) that aims to make the distribution more symmetric.\nFor a variable \\(x\\) that is strictly positive, the Box–Cox transformation is defined as\n\\[\nx^{(\\lambda)} =\n\\begin{cases}\n\\dfrac{x^{\\lambda} - 1}{\\lambda}, & \\text{if } \\lambda \\neq 0, \\\\[6pt]\n\\log(x), & \\text{if } \\lambda = 0.\n\\end{cases}\n\\]\nKey points:\n\nFor \\(\\lambda = 1\\), the Box–Cox transform is essentially the original scale.\nFor \\(\\lambda = 0\\), the transform corresponds to a log transform.\nThe method requires \\(x &gt; 0\\). In practice, one often adds a small constant (for example, \\(x^{*} = x + c\\)) if some observations are zero or very small.\n\nWe now apply a Box–Cox transformation to the same skewed dietary variable and compare its distribution with the original and log-transformed versions.\n\n# Box–Cox transformation requires strictly positive values.\n# We therefore shift the data if necessary.\n\nx_bc = df[var].dropna()\nmin_x = x_bc.min()\n\nshift = 0.0\nif min_x &lt;= 0:\n    # Add a small positive amount so that the minimum becomes slightly &gt; 0.\n    shift = -min_x + 1e-6\n\nx_bc_shifted = x_bc + shift\n\n# Apply Box–Cox. stats.boxcox returns transformed data and the estimated lambda.\nx_bc_transformed, lambda_bc = stats.boxcox(x_bc_shifted)\n\n# Store Box–Cox transformed values in the DataFrame, aligned with the original index.\nboxcox_var_name = \"boxcox_\" + var\ndf[boxcox_var_name] = np.nan\ndf.loc[x_bc.index, boxcox_var_name] = x_bc_transformed\n\nprint(f\"Created column: {boxcox_var_name} (Box–Cox transformed {var})\")\nprint(f\"Box–Cox lambda (λ): {lambda_bc:.3f}\")\nprint(f\"Shift applied before Box–Cox: {shift}\")\n\n# Compare skewness for original, log and Box–Cox transformed data.\nx_boxcox = df[boxcox_var_name].dropna()\nprint(\"Skewness (original):\", stats.skew(x))\nprint(\"Skewness (log):\", stats.skew(x_log))\nprint(\"Skewness (Box–Cox):\", stats.skew(x_boxcox))\n\nplot_hist_pair(\n    original=df[var],\n    transformed=df[boxcox_var_name],\n    original_label=var,\n    transformed_label=f\"Box–Cox({var})\",\n)\n\n\n5.1 Exercise – log versus Box–Cox\n\nCompare the histograms and skewness values for the original, log-transformed and Box–Cox transformed variables.\nWhich transformation appears to give the most symmetric distribution?\nWhich transformation would you prefer to use in a model, and why? Consider both distributional properties and interpretability."
  },
  {
    "objectID": "notebooks/1.07_data_transformation.html#categorisation-and-quantiles-useful-but-risky",
    "href": "notebooks/1.07_data_transformation.html#categorisation-and-quantiles-useful-but-risky",
    "title": "1.07 – Data Transformation and Preparation for Modelling",
    "section": "6 5. Categorisation and quantiles – useful but risky",
    "text": "6 5. Categorisation and quantiles – useful but risky\nEpidemiological papers often categorise continuous variables, for example:\n\nClinical categories (for example, normal weight, overweight, obesity based on BMI).\nQuantiles such as tertiles, quartiles or quintiles of intake.\n\nWhile categorisation can simplify presentation, it comes with several problems:\n\nInformation loss: individuals with very different values may end up in the same category.\nArbitrary cut-points: results can change if cut-points are moved slightly.\nReduced power: categorisation usually reduces statistical power.\n\nQuantiles are particularly important in nutrition:\n\nThey are often used to describe the distribution (for example, median and interquartile range).\nThey are also used to form exposure groups (for example, quintiles of intake).\n\nHowever, the extreme quantiles (lowest and highest) can cover quite wide ranges of the underlying variable, especially when the distribution is skewed. We explore this using BMI and BMI quantiles.\n\nif {\"BMI\", \"SBP\"}.issubset(df.columns):\n    # Create BMI quintiles using pandas.qcut.\n    # qcut creates categories with (approximately) equal numbers of observations.\n    df[\"BMI_quintile\"] = pd.qcut(df[\"BMI\"], 5, labels=[\"Q1\", \"Q2\", \"Q3\", \"Q4\", \"Q5\"])\n\n    print(\"BMI quintiles (cut-points and ranges):\")\n    print(\n        df.groupby(\"BMI_quintile\")[\"BMI\"].describe()[[\"min\", \"25%\", \"50%\", \"75%\", \"max\"]]\n    )\n\n    # Scatter plot of SBP vs continuous BMI.\n    plt.figure(figsize=(6, 4))\n    plt.scatter(df[\"BMI\"], df[\"SBP\"], alpha=0.3)\n    plt.xlabel(\"BMI (kg/m²)\")\n    plt.ylabel(\"SBP (mmHg)\")\n    plt.title(\"SBP vs continuous BMI\")\n    plt.tight_layout()\n    plt.show()\n\n    # Boxplot of SBP by BMI quintile.\n    plt.figure(figsize=(6, 4))\n    df.boxplot(column=\"SBP\", by=\"BMI_quintile\")\n    plt.xlabel(\"BMI quintile\")\n    plt.ylabel(\"SBP (mmHg)\")\n    plt.title(\"SBP by BMI quintile\")\n    plt.suptitle(\"\")  # Remove the default super-title.\n    plt.tight_layout()\n    plt.show()\n\n    # Compare mean SBP between quintiles.\n    print(\"\\nMean SBP by BMI quintile:\")\n    print(df.groupby(\"BMI_quintile\")[\"SBP\"].mean())\n\n\n6.1 Exercise – categories and extreme quantiles\n\nCompare the scatter plot of SBP vs continuous BMI with the boxplot of SBP by BMI quintiles.\n\nWhat information is visible in the scatter plot but lost when using quintiles?\nHow much variation in SBP exists within each quintile?\n\nLook at the ranges of BMI within each quintile.\n\nAre the lowest and highest values in Q1 and Q5 quite far apart?\nHow might this affect interpretation of “lowest” vs “highest” quintile?\n\nIn which situations might categorisation still be useful or necessary (for example, clinical decision thresholds)?"
  },
  {
    "objectID": "notebooks/1.07_data_transformation.html#special-case-truncation-of-analytical-values",
    "href": "notebooks/1.07_data_transformation.html#special-case-truncation-of-analytical-values",
    "title": "1.07 – Data Transformation and Preparation for Modelling",
    "section": "7 6. Special case – truncation of analytical values",
    "text": "7 6. Special case – truncation of analytical values\nLaboratory measurements often have a limit of detection (LOD), a lower limit of quantification (LLOQ) and sometimes an upper limit of quantification (ULOQ). Values below or above these limits are truncated by the measurement process.\nCommon strategies for handling values below LOD or LLOQ include:\n\nTreating them as missing and analysing only values above the limit.\nReplacing them with a constant (for example, LOD/2 or LOD/√2).\nUsing the measured values as reported (if the instrument provides them), but recognising that they are less reliable.\n\nWe now simulate a small biomarker with a LOD and show how different approaches affect the distribution, particularly when applying a log transformation.\n\n# Simulate a right-skewed biomarker (for example, a log-normal-like distribution)\n\nbio_raw = np.random.lognormal(mean=1.0, sigma=0.7, size=2000)\n\n# Assume a limit of detection (LOD)\nLOD = 1.0\nprint(\"Assumed LOD:\", LOD)\n\n# Create three versions:\n# 1) Values below LOD treated as missing (NaN).\n# 2) Values below LOD replaced by LOD/2.\n# 3) Original measured values (no truncation), for comparison.\n\nbio_measured = bio_raw.copy()\nbio_na = bio_raw.copy()\nbio_const = bio_raw.copy()\n\nbio_na[bio_na &lt; LOD] = np.nan\nbio_const[bio_const &lt; LOD] = LOD / 2.0\n\nprint(\"Proportion of values below LOD:\", np.mean(bio_raw &lt; LOD))\n\n# Plot histograms for the three approaches on the original scale\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n\naxes[0].hist(bio_measured, bins=30)\naxes[0].axvline(LOD, color=\"red\", linestyle=\"--\")\naxes[0].set_title(\"Measured values (no truncation)\")\naxes[0].set_xlabel(\"Biomarker\")\naxes[0].set_ylabel(\"Frequency\")\n\n    \naxes[1].hist(bio_na[~np.isnan(bio_na)], bins=30)\naxes[1].axvline(LOD, color=\"red\", linestyle=\"--\")\naxes[1].set_title(\"Values &lt; LOD as missing\")\naxes[1].set_xlabel(\"Biomarker\")\n\naxes[2].hist(bio_const, bins=30)\naxes[2].axvline(LOD, color=\"red\", linestyle=\"--\")\naxes[2].set_title(\"Values &lt; LOD replaced by LOD/2\")\naxes[2].set_xlabel(\"Biomarker\")\n\nplt.tight_layout()\nplt.show()\n\n# Now apply a log transform (using a small constant to allow zeros if they appear)\neps = 1e-6\nlog_measured = np.log(bio_measured + eps)\nlog_na = np.log(bio_na + eps)\nlog_const = np.log(bio_const + eps)\n\nfig, axes = plt.subplots(1, 3, figsize=(15, 4), sharey=True)\n\naxes[0].hist(log_measured, bins=30)\naxes[0].set_title(\"log(measured)\")\naxes[0].set_xlabel(\"log(biomarker)\")\naxes[0].set_ylabel(\"Frequency\")\n\naxes[1].hist(log_na[~np.isnan(log_na)], bins=30)\naxes[1].set_title(\"log(values ≥ LOD)\")\naxes[1].set_xlabel(\"log(biomarker)\")\n\naxes[2].hist(log_const, bins=30)\naxes[2].set_title(\"log(values with LOD/2)\")\naxes[2].set_xlabel(\"log(biomarker)\")\n\nplt.tight_layout()\nplt.show()\n\n\n7.1 Exercise – thinking about truncation\n\nCompare the three original-scale histograms.\n\nHow does treating values below LOD as missing change the shape of the distribution?\nHow does replacing values below LOD by a constant (LOD/2) change it?\n\nCompare the three log-scale histograms.\n\nDoes constant substitution create an artificial spike in the distribution?\nHow might this affect model fitting and interpretation?\n\nIn a real analysis, which approach would you choose, and what sensitivity analyses might you consider?"
  },
  {
    "objectID": "notebooks/1.07_data_transformation.html#summary-interpretation-and-practical-rules",
    "href": "notebooks/1.07_data_transformation.html#summary-interpretation-and-practical-rules",
    "title": "1.07 – Data Transformation and Preparation for Modelling",
    "section": "8 7. Summary, interpretation and practical rules",
    "text": "8 7. Summary, interpretation and practical rules\nWhen transformed variables are used in models, regression coefficients change their meaning:\n\nLog-transformed exposure (for example, log(intake))\n\nA one-unit increase in log(intake) corresponds to a multiplicative change in the original intake.\nEffects are often interpreted per k-fold increase (for example, per doubling), which can be obtained by rescaling the log variable.\n\nBox–Cox transformed exposure\n\nCoefficients are expressed in a transformed scale that is not directly intuitive.\nInterpretation often focuses on comparing predicted outcomes at specific, back-transformed values rather than on a generic “one-unit change”.\n\nz-scored variables (standardisation)\n\nEffects are per 1 standard deviation increase in the original variable.\nThis facilitates comparison of effect sizes between variables but may be less intuitive for clinical audiences.\n\nCategorised variables (for example, quantiles)\n\nCoefficients compare categories (for example, highest vs lowest quintile) and are easy to describe.\nHowever, they conceal variability within categories and depend on arbitrary cut-points.\n\n\nFor reporting, it is often helpful to:\n\nPresent results both on the transformed scale (for modelling) and back-transformed or re-expressed in the original units (for interpretation).\nClearly state which transformation was used, and, where relevant, which constants or parameters (for example, () in Box–Cox, constants for LOD imputation) were applied.\n\n\n8.1 Practical rules (summary)\n\nUse plots of the variable and residuals to guide decisions; do not rely solely on formal normality tests.\nConsider a log transform for strongly right-skewed positive variables, especially where proportional changes are meaningful.\nUse z-scores when comparing effect sizes across variables on different scales.\nUse Box–Cox when a simple log transform is clearly inadequate and interpretability can be handled via back-transformation.\nUse categorisation sparingly; when it is required (for example, clinical thresholds), be explicit about cut-points and acknowledge information loss.\nFor truncated analytical values, state clearly how values below LOD / LLOQ were handled and consider sensitivity analyses with alternative approaches.\n\n\n\n8.2 Summary table – transformations and their properties\n\n\n\n\n\n\n\n\n\n\n\n\nApproach\nType\nReversible?\nTypical purpose\nAdvantages\nLimitations / cautions\nInterpretation of 1-unit change\n\n\n\n\nLog(x + c)\nMonotonic transform\nYes\nReduce right-skewness; stabilise variance\nSimple; widely understood; often matches biology\nRequires x + c &gt; 0; choice of c affects tail\nMultiplicative change in original x\n\n\nBox–Cox(x; λ)\nPower transform\nYes\nReduce skewness; approximate normality\nFlexible; λ estimated from data\nLess intuitive; sensitive to outliers and truncation\nModel-based; often via back-transforms\n\n\nz-score = (x − mean)/sd\nLinear transform\nYes\nStandardise scale; compare effect sizes\nMean 0, sd 1; easy comparison across variables\nDepends on sample distribution; less intuitive clinically\nChange per 1 sd in original x\n\n\nCategorisation (quantiles)\nNon-linear, discrete\nNo\nSimplify presentation; threshold-based decisions\nEasy to present and explain; suits decision rules\nInformation loss; arbitrary cut-points; reduced power\nCategory contrasts (e.g. highest vs lowest)\n\n\nTruncation + constant imput.\nAd hoc modification\nPartly\nHandle values &lt; LOD / LLOQ\nSimple; preserves sample size\nArtificial spikes; may bias low range\nDepends on imputation rule\n\n\nUse measured values (LOD-flag)\nMeasurement choice\nYes\nRetain numeric range from instrument\nAvoids artificial truncation\nLow-range uncertainty; method-specific behaviour\nAs for raw scale; note increased uncertainty"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html",
    "href": "notebooks/2.05_sugar_reduction.html",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "",
    "text": "Learning Objectives: - Understand that a biomarker for sugar intake EXISTS but has not been widely deployed - Compare evaluation approaches for the Soft Drinks Industry Levy (SDIL) - Critically assess the evidence hierarchy: purchase data, dietary surveys, modelled estimates - Appreciate the role of uncertainty in policy evaluation - Contrast the sugar and salt reduction programmes"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#introduction-the-sugar-challenge",
    "href": "notebooks/2.05_sugar_reduction.html#introduction-the-sugar-challenge",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "1 1. Introduction: The Sugar Challenge",
    "text": "1 1. Introduction: The Sugar Challenge\n\n1.1 A Common Misconception\nIt is often stated that sugar intake cannot be measured with a biomarker. This is incorrect.\nA validated biomarker for total sugars intake exists: - 24-hour urinary sucrose + fructose (Tasevska et al., 2005) - Validated in controlled feeding studies - Successfully used in EPIC-Norfolk and Health Survey for England\n\n\n1.2 So Why Hasn’t It Been Used?\nUnlike the UK salt reduction programme, which incorporated regular biomarker monitoring, sugar reduction policies have not routinely used the urinary sugars biomarker. This is largely due to:\n\nLogistical challenges: 24-hour urine collection is burdensome\nCost: Laboratory analysis at population scale is expensive\nHistorical timing: The biomarker was validated (2005) but sugar policy only became a priority later (2015+)\nPolicy design: SDIL focused on reformulation, not population intake monitoring\n\nThis represents a missed opportunity for objective evaluation."
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#setup",
    "href": "notebooks/2.05_sugar_reduction.html#setup",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "2 2. Setup",
    "text": "2 2. Setup\n\n# ============================================================\n# Bootstrap cell\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\n\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\ncwd = pathlib.Path.cwd()\n\nif (cwd / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd\nelif (cwd.parent / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd.parent\nelse:\n    repo_root = cwd / REPO_DIR\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nscripts_dir = repo_root / \"scripts\"\nif str(scripts_dir) not in sys.path:\n    sys.path.insert(0, str(scripts_dir))\n\nprint(f\"Repository root: {repo_root}\")\nprint(\"Bootstrap completed successfully.\")\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = [10, 6]\nnp.random.seed(42)\n\nprint(\"Libraries loaded successfully.\")"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#the-urinary-sugars-biomarker",
    "href": "notebooks/2.05_sugar_reduction.html#the-urinary-sugars-biomarker",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "3 3. The Urinary Sugars Biomarker",
    "text": "3 3. The Urinary Sugars Biomarker\n\n3.1 How It Works\n\nSmall amounts of dietary sucrose and fructose are excreted intact in urine\nThe sum of 24-hour urinary sucrose and fructose correlates with total sugars intake\nA calibration equation converts biomarker values to estimated intake\n\n\n\n3.2 Key Research\n\n\n\n\n\n\n\nStudy\nFinding\n\n\n\n\nTasevska et al. (2005)\nValidated biomarker in controlled feeding study\n\n\nJoosen et al. (2008)\nConfirmed validity in obese individuals\n\n\nKuhnle et al. (2015)\nAssociation with overweight/obesity in EPIC-Norfolk\n\n\nCampbell et al. (2017)\nAssociation with obesity in nationally-representative HSE sample\n\n\n\n\n# Data from Campbell et al. (2017) - HSE 2005\n# Source: Campbell R et al. (2017). PLoS ONE 12(7): e0179508\n\nhse_biomarker_data = pd.DataFrame({\n    'Measure': ['Estimated sugar intake (self-report, NDNS)', \n                'Estimated sugar intake (biomarker, HSE)'],\n    'Women (g/day)': [78, 117],\n    'Men (g/day)': [107, 162]\n})\n\nprint(\"Sugar Intake: Self-Report vs Biomarker Estimates\")\nprint(\"=\" * 60)\ndisplay(hse_biomarker_data)\nprint(\"\\nBiomarker estimates are ~50% higher than self-reported intakes.\")\nprint(\"This suggests substantial underreporting of sugar intake.\")\nprint(\"\\nSource: Campbell et al. (2017). PLoS ONE.\")\n\n\n# Association between biomarker-estimated sugar intake and obesity\n# Source: Campbell et al. (2017) Table 2\n\nobesity_or = pd.DataFrame({\n    'Obesity measure': ['BMI &gt;= 30', 'Waist circumference', 'Waist-to-hip ratio'],\n    'OR per 10g sugar': [1.02, 1.03, 1.04],\n    '95% CI lower': [1.00, 1.01, 1.02],\n    '95% CI upper': [1.04, 1.05, 1.06],\n    'p-value': ['&lt;0.05', '&lt;0.01', '&lt;0.001']\n})\n\nprint(\"Association: Biomarker Sugar Intake and Obesity Risk\")\nprint(\"=\" * 60)\ndisplay(obesity_or)\nprint(\"\\nInterpretation: Each 10g/day increase in sugar intake is associated\")\nprint(\"with 2-4% higher odds of obesity.\")\nprint(\"\\nSource: Campbell et al. (2017). PLoS ONE.\")"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#the-soft-drinks-industry-levy-sdil",
    "href": "notebooks/2.05_sugar_reduction.html#the-soft-drinks-industry-levy-sdil",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "4 4. The Soft Drinks Industry Levy (SDIL)",
    "text": "4 4. The Soft Drinks Industry Levy (SDIL)\nThe SDIL uses a tiered structure:\n\n\n\nSugar content\nLevy rate\n\n\n\n\n&lt; 5g per 100ml\n£0.00\n\n\n5-8g per 100ml\n£0.18 per litre\n\n\n&gt; 8g per 100ml\n£0.24 per litre\n\n\n\nSource: HMRC (2018). Soft Drinks Industry Levy.\n\n# Product reformulation data\n# Source: Public Health England (2020). Sugar reduction: progress report 2015-2019\n\nreformulation_data = pd.DataFrame({\n    'Period': ['Pre-announcement (2015)', 'Post-announcement (2017)', 'Post-implementation (2019)'],\n    'Mean sugar (g/100ml)': [4.4, 3.5, 2.9],\n    'Products in high levy band (%)': [52, 38, 15],\n    'Products in zero levy band (%)': [39, 49, 73]\n})\n\nprint(\"Soft Drink Reformulation\")\nprint(\"=\" * 60)\ndisplay(reformulation_data)\nprint(\"\\nSource: PHE (2020). Sugar reduction progress report.\")\n\n\n# Visualise reformulation\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\nax = axes[0]\ncolors = ['#e74c3c', '#f39c12', '#27ae60']\nbars = ax.bar(range(3), reformulation_data['Mean sugar (g/100ml)'], color=colors)\nax.set_xticks(range(3))\nax.set_xticklabels(['2015', '2017', '2019'])\nax.set_ylabel('Mean sugar (g/100ml)')\nax.set_title('Average Sugar Content in Soft Drinks')\nax.set_ylim(0, 5)\nfor i, v in enumerate(reformulation_data['Mean sugar (g/100ml)']):\n    ax.text(i, v + 0.1, f'{v}g', ha='center', fontweight='bold')\n\nax = axes[1]\nwidth = 0.35\nx = np.arange(3)\nax.bar(x - width/2, reformulation_data['Products in high levy band (%)'], width, \n       label='High levy (&gt;8g)', color='#e74c3c')\nax.bar(x + width/2, reformulation_data['Products in zero levy band (%)'], width, \n       label='Zero levy (&lt;5g)', color='#27ae60')\nax.set_xticks(x)\nax.set_xticklabels(['2015', '2017', '2019'])\nax.set_ylabel('Percentage of products')\nax.set_title('Distribution Across Levy Bands')\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key finding: Substantial reformulation occurred, mostly before implementation.\")"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#comparing-salt-and-sugar-programme-evaluation",
    "href": "notebooks/2.05_sugar_reduction.html#comparing-salt-and-sugar-programme-evaluation",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "5 5. Comparing Salt and Sugar Programme Evaluation",
    "text": "5 5. Comparing Salt and Sugar Programme Evaluation\n\n5.1 Why Was Salt Easier to Evaluate?\n\n\n\n\n\n\n\n\nFactor\nSalt Programme\nSugar Programme\n\n\n\n\nBiomarker used?\nYes (24-h urinary Na)\nNo (despite availability)\n\n\nRegular monitoring\nFSA/PHE surveys 2000-2019\nNo systematic biomarker surveys\n\n\nIntake data quality\nHigh (objective)\nLow (self-report, purchase data)\n\n\nPolicy focus\nPopulation intake\nProduct reformulation\n\n\n\n\n# Evidence confidence by outcome level\nevidence_layers = [\n    ('Product reformulation', 0.9, 'Direct measurement of products'),\n    ('Household purchases', 0.7, 'Kantar panel data'),\n    ('Individual intake (self-report)', 0.4, 'NDNS dietary surveys'),\n    ('Individual intake (biomarker)', 0.85, 'AVAILABLE but not deployed'),\n    ('Weight/BMI change', 0.2, 'Confounding, secular trends'),\n    ('Disease outcomes', 0.1, 'Decades away')\n]\n\nfig, ax = plt.subplots(figsize=(10, 5))\n\nlabels = [e[0] for e in evidence_layers]\nconfidence = [e[1] for e in evidence_layers]\nnotes = [e[2] for e in evidence_layers]\n\ncolors = plt.cm.RdYlGn(np.array(confidence))\nbars = ax.barh(labels, confidence, color=colors)\n\n# Highlight the biomarker bar\nbars[3].set_edgecolor('blue')\nbars[3].set_linewidth(3)\n\nax.set_xlim(0, 1)\nax.set_xlabel('Confidence in evidence')\nax.set_title('SDIL Evaluation: Evidence Confidence by Outcome Level')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Note: The urinary sugars biomarker (blue outline) could provide high-confidence\")\nprint(\"evidence of intake change, but has not been deployed for routine monitoring.\")"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#the-counterfactual-problem",
    "href": "notebooks/2.05_sugar_reduction.html#the-counterfactual-problem",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "6 6. The Counterfactual Problem",
    "text": "6 6. The Counterfactual Problem\nWithout biomarker data, we must infer intake from indirect sources. This introduces the counterfactual problem: What would have happened without the SDIL?\n\n# Different counterfactual scenarios\n# Based on: Pell D et al. (2021). BMJ 372:n254\n\nyears = np.arange(2014, 2022)\nobserved = np.array([155, 150, 140, 125, 105, 100, 98, 95])\n\ncounterfactuals = {\n    'Flat trend': np.array([155, 155, 155, 155, 155, 155, 155, 155]),\n    'Slow decline (-2%/yr)': 155 * (0.98 ** np.arange(8)),\n    'Moderate decline (-3%/yr)': 155 * (0.97 ** np.arange(8)),\n}\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.plot(years, observed, 'ko-', linewidth=3, markersize=10, label='Observed')\n\ncolors = ['#3498db', '#e74c3c', '#2ecc71']\nfor i, (name, cf) in enumerate(counterfactuals.items()):\n    ax.plot(years, cf, '--', linewidth=2, color=colors[i], label=f'Counterfactual: {name}')\n\nax.axvline(x=2018.25, color='red', linestyle=':', linewidth=2)\nax.text(2018.4, 160, 'SDIL', fontsize=10, color='red')\nax.set_xlabel('Year', fontsize=12)\nax.set_ylabel('Sugar from soft drinks (g/hh/week)', fontsize=12)\nax.set_title('The Counterfactual Problem', fontsize=13)\nax.legend(loc='upper right')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Estimated effect in 2021 depends on counterfactual assumption:\")\nfor name, cf in counterfactuals.items():\n    effect = cf[-1] - observed[-1]\n    print(f\"  {name}: {effect:.0f} g/hh/week reduction\")"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#discussion-questions",
    "href": "notebooks/2.05_sugar_reduction.html#discussion-questions",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "7 7. Discussion Questions",
    "text": "7 7. Discussion Questions\n\nMissed opportunity: Why do you think the urinary sugars biomarker was not incorporated into SDIL evaluation? What would it have taken to do so?\nReformulation paradox: If manufacturers reformulated before implementation, the levy raised less revenue. Is this success or failure?\nFuture policy: If you were designing a new sugar reduction policy, would you incorporate biomarker monitoring? What would be the costs and benefits?\nEvidence standards: Should we require biomarker evidence before concluding that a dietary policy has worked?"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#exercises",
    "href": "notebooks/2.05_sugar_reduction.html#exercises",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "8 8. Exercises",
    "text": "8 8. Exercises\n\n8.1 Exercise 1: Design a Monitoring Programme\nDesign a biomarker-based monitoring programme for sugar intake: - What sample size would you need? - How often would you survey? - What would be the approximate cost? - What are the barriers to implementation?\n\n# YOUR DESIGN HERE"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#summary",
    "href": "notebooks/2.05_sugar_reduction.html#summary",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "9 Summary",
    "text": "9 Summary\n\nA validated biomarker for sugar intake exists (urinary sucrose + fructose)\nThis biomarker has not been routinely used for policy evaluation\nSDIL evaluation relies on product data and purchase data, not intake biomarkers\nThe counterfactual problem introduces substantial uncertainty\nThis represents a missed opportunity for objective policy evaluation"
  },
  {
    "objectID": "notebooks/2.05_sugar_reduction.html#references",
    "href": "notebooks/2.05_sugar_reduction.html#references",
    "title": "2.05 – Case Study: Sugar Reduction (Biomarker Underutilised)",
    "section": "10 References",
    "text": "10 References\n\nTasevska N et al. (2005). Urinary sucrose and fructose as biomarkers for sugar consumption. Cancer Epidemiol Biomarkers Prev.\nJoosen AMCP et al. (2008). Urinary sucrose and fructose as biomarkers: comparison of normal weight and obese volunteers. Int J Obes.\nKuhnle GGC et al. (2015). Association between sucrose intake and risk of overweight and obesity in EPIC-Norfolk. Public Health Nutr.\nCampbell R et al. (2017). Association between urinary biomarkers of total sugars intake and measures of obesity. PLoS ONE.\nPell D et al. (2021). Changes in soft drinks purchased by British households associated with the SDIL. BMJ.\nPHE (2020). Sugar reduction: progress report 2015-2019."
  },
  {
    "objectID": "notebooks/1.05_representative.html",
    "href": "notebooks/1.05_representative.html",
    "title": "1.05 – Populations, Samples, and Representativeness",
    "section": "",
    "text": "Version 0.1.0\nThis workbook introduces core ideas in sampling for nutritional epidemiology:\nWe use two datasets:\n# ============================================================\n# FB2NEP bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the fb2nep-epi repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads and runs scripts/bootstrap.py.\n# - Makes the main dataset available as the variable `df`.\n#\n# Important:\n# - You may see messages printed below (for example from pip\n#   or from the bootstrap script). This is expected.\n# - You may also see WARNINGS (often in yellow). In most cases\n#   these are harmless and can be ignored for this module.\n# - The main thing to watch for is a red error traceback\n#   (for example FileNotFoundError, ModuleNotFoundError).\n#   If that happens, please re-run this cell first. If the\n#   error persists, ask for help.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\nimport importlib.util\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\n# REPO_URL: address of the GitHub repository.\n# REPO_DIR: folder name that will be created when cloning.\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the fb2nep-epi repository\n# ------------------------------------------------------------\n# In local Jupyter, you may already be inside the repository,\n# for example in fb2nep-epi/notebooks.\n#\n# In Colab, the default working directory is /content, so\n# we need to clone the repository into /content/fb2nep-epi\n# and then change into that folder.\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/bootstrap.py exists here)\nif (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n    repo_root = cwd\n\n# Case B: we are outside the repository (for example in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if it is not present yet\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change the working directory to the repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nprint(f\"Repository root set to: {repo_root}\")\n\n# ------------------------------------------------------------\n# 2. Load scripts/bootstrap.py as a module and call init()\n# ------------------------------------------------------------\n# The shared bootstrap script contains all logic to:\n# - Ensure that required Python packages are installed.\n# - Ensure that the synthetic dataset exists (and generate it\n#   if needed).\n# - Load the dataset into a pandas DataFrame.\n#\n# We load the script as a normal Python module (fb2nep_bootstrap)\n# and then call its init() function.\nbootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n\nif not bootstrap_path.is_file():\n    raise FileNotFoundError(\n        f\"Could not find {bootstrap_path}. \"\n        \"Please check that the fb2nep-epi repository structure is intact.\"\n    )\n\n# Create a module specification from the file\nspec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\nbootstrap = importlib.util.module_from_spec(spec)\nsys.modules[\"fb2nep_bootstrap\"] = bootstrap\n\n# Execute the bootstrap script in the context of this module\nspec.loader.exec_module(bootstrap)\n\n# The init() function is defined in scripts/bootstrap.py.\n# It returns:\n# - df   : the main synthetic cohort as a pandas DataFrame.\n# - CTX  : a small context object with paths, flags and settings.\ndf, CTX = bootstrap.init()\n\n# Optionally expose a few additional useful variables from the\n# bootstrap module (if they exist). These are not essential for\n# most analyses, but can be helpful for advanced use.\nfor name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n    if hasattr(bootstrap, name):\n        globals()[name] = getattr(bootstrap, name)\n\nprint(\"Bootstrap completed successfully.\")\nprint(\"The main dataset is available as the variable `df`.\")\nprint(\"The context object is available as `CTX`.\")\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scripts.fetch_nhanes_demo import load_nhanes_demo\n\n# ---------------------------------------------------------------------\n# Fixed random seed for all simulations in this workbook.\n# ---------------------------------------------------------------------\nRANDOM_SEED = 11088\nrng = np.random.default_rng(RANDOM_SEED)\n\n# ---------------------------------------------------------------------\n# Load processed NHANES subset (downloaded and tidied by helper script).\n# ---------------------------------------------------------------------\nnhanes = load_nhanes_demo(cache=True)\n\n# ---------------------------------------------------------------------\n# Load / reuse the FB2NEP synthetic cohort\n# ---------------------------------------------------------------------\n\n# (If you prefer to be explicit about the path, you could do:)\nfb2nep_path = repo_root / CSV_REL\nprint(\"Loading FB2NEP synthetic cohort from:\", fb2nep_path)\ndf_fb2nep = pd.read_csv(fb2nep_path)\n\nprint(\"NHANES demo shape:\", nhanes.shape)\nprint(\"FB2NEP synthetic cohort shape:\", df_fb2nep.shape)\n\nprint(\"NHANES\")\nnhanes.head()\nprint(\"FB2NEP\")\ndf_fb2nep.head()"
  },
  {
    "objectID": "notebooks/1.05_representative.html#concepts-and-definitions",
    "href": "notebooks/1.05_representative.html#concepts-and-definitions",
    "title": "1.05 – Populations, Samples, and Representativeness",
    "section": "1 1. Concepts and definitions",
    "text": "1 1. Concepts and definitions\n\n1.1 1.1 General population, target population, and sample\n\nGeneral population: The entire group about which we ultimately wish to draw conclusions. Examples: “all adults living in the United States”, or “all adults aged 40 years and older in the United Kingdom”.\nTarget population (study population): The subset of the general population that a particular study intends to represent. Examples: all adults aged 40–79 years registered with a general practitioner in England, or all post-menopausal women without prior cardiovascular disease at baseline. This definition is conceptual and exists before sampling.\nSampling frame: The operational list or mechanism used to select individuals. Examples: general practice registers, electoral rolls, health insurance lists, employee registers. Individuals who are not in the sampling frame cannot be selected, even if they belong to the target population.\nSample: The actual set of individuals who are recruited and provide data. The sample may differ from the target population because of non-response, exclusion criteria, and practical constraints.\n\nKey questions for any study:\n\nWho did we intend to study? (target population)\nWho did we actually study? (realised sample)\nHow different are these groups with respect to variables that matter for our research question?\n\n\n\n1.2 1.2 Representativeness, sampling error, and bias\nA sample is representative of a population when the distribution of key characteristics in the sample matches that of the population of interest. Common characteristics:\n\nSex.\nAge distribution.\nSocioeconomic position (for example, education, deprivation indices).\nEthnicity (if available).\n\nRepresentativeness is mainly about external validity: how far we can generalise study findings beyond those who took part.\nIt is useful to distinguish two sources of difference between sample and population:\n\nSampling error: Random variation in estimates because we observe only a finite sample rather than the whole population. Sampling error becomes smaller as the sample size increases.\nSystematic bias: Systematic differences between sample and population (for example, non-participation of people with poor health or low income) that do not disappear when the sample size increases.\n\nIn this workbook we will:\n\nUse NHANES as a reference survey for the general population.\nTreat the FB2NEP synthetic cohort as a “study sample” and compare it to NHANES.\nUse repeated sampling from NHANES to illustrate sampling error.\n\n\n\n1.3 1.3 NHANES and the FB2NEP synthetic cohort\n\nNHANES 🇺🇸 uses complex probability sampling strategies to obtain an approximately nationally representative sample of the civilian, non-institutionalised United States population. For this workbook we focus on adults and use a limited set of variables: age, sex, race/ethnicity, education, and BMI.\nThe FB2NEP synthetic cohort is a simulated cohort used for teaching. It mimics a longitudinal study of adults aged 40 years and older with follow-up for cardiovascular disease and cancer. It is not designed to be representative of any real country, but we can still compare its structure to NHANES.\n\n\n# Quick look at the FB2NEP synthetic cohort.\n\n    \nfb_cols = [\n    \"id\", \"age\", \"sex\", \"IMD_quintile\", \"SES_class\", \"BMI\", \"SBP\",\n    \"fruit_veg_g_d\", \"red_meat_g_d\", \"CVD_incident\", \"Cancer_incident\"\n]\n\n# Use list comprehension to keep only columns that are actually present.\nfb_cols = [c for c in fb_cols if c in df_fb2nep.columns]\n\ndf_fb2nep[fb_cols].head()"
  },
  {
    "objectID": "notebooks/1.05_representative.html#basic-distributions-in-nhanes",
    "href": "notebooks/1.05_representative.html#basic-distributions-in-nhanes",
    "title": "1.05 – Populations, Samples, and Representativeness",
    "section": "2 2. Basic distributions in NHANES",
    "text": "2 2. Basic distributions in NHANES\nWe first inspect the distribution of sex, age, and race/ethnicity in the NHANES subset. We create simple age groups for adults and compute proportion tables.\n\n# Create age groups for NHANES adults.\n#\n# We restrict to adults aged 20 years and older (already done in the helper script),\n# and group into: 20–39, 40–59, 60+ years.\n\nage_bins_nhanes = [20, 40, 60, np.inf]\nage_labels_nhanes = [\"20–39\", \"40–59\", \"60+\"]\n\nnhanes = nhanes.copy()\nnhanes[\"age_group\"] = pd.cut(\n    nhanes[\"age_years\"],\n    bins=age_bins_nhanes,\n    labels=age_labels_nhanes,\n    right=False,\n)\n\nnhanes[[\"age_years\", \"age_group\", \"sex\", \"race_eth\", \"education\", \"bmi\"]].head()\n\n\nfrom scripts.helpers_tables import proportion_table\n\nprint(\"NHANES sex distribution:\")\ndisplay(proportion_table(nhanes, \"sex\", dropna=False))\n\nprint(\"\\nNHANES age group distribution:\")\ndisplay(proportion_table(nhanes, \"age_group\", dropna=False))\n\nprint(\"\\nNHANES race/ethnicity distribution:\")\ndisplay(proportion_table(nhanes, \"race_eth\", dropna=False))"
  },
  {
    "objectID": "notebooks/1.05_representative.html#representation-relative-to-the-united-states-census",
    "href": "notebooks/1.05_representative.html#representation-relative-to-the-united-states-census",
    "title": "1.05 – Populations, Samples, and Representativeness",
    "section": "3 3. Representation relative to the United States Census",
    "text": "3 3. Representation relative to the United States Census\nNHANES is designed to be approximately representative of the United States population. To illustrate this, we compare NHANES proportions to approximate adult distributions from the United States Census (values here are simplified for teaching).\nWe compute a representation ratio for each category:\n\\[\\begin{equation}\n\\text{representation ratio} = \\frac{\\text{NHANES proportion}}{\\text{Census proportion}}.\n\\end{equation}\\]\n\nA value close to 1 means that NHANES has a similar proportion to the Census.\nValues greater than 1 indicate over-representation in NHANES.\nValues less than 1 indicate under-representation.\n\n\n# Approximate United States Census distributions for adults.\n# These values are illustrative and are not official statistics.\n\ncensus_sex = pd.DataFrame({\n    \"sex\": [\"Female\", \"Male\"],\n    \"census_prop\": [0.509, 0.491],\n})\n\ncensus_age = pd.DataFrame({\n    \"age_group\": [\"20–39\", \"40–59\", \"60+\"],\n    \"census_prop\": [0.35, 0.33, 0.32],\n})\n\ncensus_race = pd.DataFrame({\n    \"race_eth\": [\"White\", \"Black\", \"Hispanic\", \"Asian\", \"Other\"],\n    \"census_prop\": [0.58, 0.12, 0.19, 0.06, 0.05],\n})\n\ncensus_sex\n\n\nfrom scripts.helpers_tables import representation_table\n\n\n# Compute NHANES proportion tables.\nnhanes_sex = proportion_table(nhanes, \"sex\").reset_index().rename(columns={\"index\": \"sex\"})\nnhanes_age = proportion_table(nhanes, \"age_group\").reset_index().rename(columns={\"index\": \"age_group\"})\nnhanes_race = proportion_table(nhanes, \"race_eth\").reset_index().rename(columns={\"index\": \"race_eth\"})\n\n# Compute representation ratios.\nrepr_sex = representation_table(nhanes_sex, census_sex, \"sex\")\nrepr_age = representation_table(nhanes_age, census_age, \"age_group\")\nrepr_race = representation_table(nhanes_race, census_race, \"race_eth\")\n\nrepr_sex\n\n\nfrom scripts.helpers_tables import plot_representation\n\nplot_representation(repr_sex, \"sex\", \"NHANES vs Census: sex\")\nplot_representation(repr_age, \"age_group\", \"NHANES vs Census: age group\")\nplot_representation(repr_race, \"race_eth\", \"NHANES vs Census: race/ethnicity\")\n\n\n3.1 Interpretation\n\nA representation ratio close to 1.0 indicates that NHANES has a similar proportion to the Census for that category.\nRatios above 1.0 indicate that the category is over-represented in NHANES; ratios below 1.0 indicate under-representation.\nNHANES is designed to be reasonably close to the United States population, but it is not perfect. Some groups will be slightly over- or under-represented even after weighting."
  },
  {
    "objectID": "notebooks/1.05_representative.html#sampling-variability-and-sample-size-nhanes-bmi-example",
    "href": "notebooks/1.05_representative.html#sampling-variability-and-sample-size-nhanes-bmi-example",
    "title": "1.05 – Populations, Samples, and Representativeness",
    "section": "4 4. Sampling variability and sample size (NHANES BMI example)",
    "text": "4 4. Sampling variability and sample size (NHANES BMI example)\nWe now treat the NHANES subset as our population and examine how estimates of mean BMI vary when we repeatedly draw samples of different sizes.\nFor a numeric variable such as BMI we compute:\n\nThe population mean and standard deviation using all NHANES adults.\nThe distribution of sample means from many repeated samples of size n.\n\nWe expect that:\n\nThe mean of the sample means will be close to the true population mean.\nThe variability of the sample means will decrease as the sample size increases.\n\n\n# Population statistics for BMI in NHANES.\n\nif \"bmi\" not in nhanes.columns:\n    raise KeyError(\"The NHANES dataset does not contain a 'bmi' column.\")\n\npop_mean_bmi = nhanes[\"bmi\"].mean()\npop_sd_bmi = nhanes[\"bmi\"].std()\n\nprint(\"NHANES BMI (adults) – population statistics:\")\nprint(f\"  Mean: {pop_mean_bmi:5.2f}\")\nprint(f\"  SD:   {pop_sd_bmi:5.2f}\")\n\n\n4.1 4.1 Setting up the simulation\nWe now simulate what would happen if we repeatedly carried out the same study many times on different random samples from the same population.\nThe goal is to understand the sampling distribution of the mean BMI for different sample sizes.\nThe key ideas are:\n\nWe treat the NHANES dataset (nhanes) as if it were the true population.\nFor each chosen sample size n (for example, 10, 50, 100, 500, 2 000), we:\n\nDraw a simple random sample of size n from NHANES (without replacement).\nCalculate the mean BMI in that sample.\nRepeat this process n_sim times (for example, 300 times).\n\nThe resulting collection of mean BMI values shows how much the estimate of the mean BMI varies from study to study, purely due to random sampling.\n\n\nfrom scripts.helpers_tables import draw_sample_mean_bmi, simulate_sampling_distribution\n\n# Compute the \"true\" population mean BMI from the full NHANES dataset.\n# In this workbook we treat NHANES as the population.\ntrue_mean_bmi = float(nhanes[\"bmi\"].mean().round(2))\n\nprint(f\"True (population) mean BMI from NHANES: {true_mean_bmi:.2f}\")\n\n# Choose sample sizes for comparison and number of simulations.\nsample_sizes = [10, 50, 100, 500, 2000]\nn_sim = 300   # number of repeated samples for each n\n\n# Dictionary to store the sampling distributions for each n\nsampling_results = {}\nfor n in sample_sizes:\n    sampling_results[n] = simulate_sampling_distribution(nhanes, n, n_sim, rng)\n\n# Inspect the first few simulated means for n = 10\nprint(\"\\nFirst five simulated sample means (n = 10):\")\nprint(np.round(sampling_results[10][:5], 2))\n\n# Compare the average of simulated means to the true mean\nsimulated_mean_10 = float(np.mean(sampling_results[10]).round(2))\nprint(f\"\\nAverage of simulated means for n = 10: {simulated_mean_10:.2f}\")\nprint(f\"Difference from true mean: {simulated_mean_10 - true_mean_bmi:+.2f}\")\n\n\n# Inspect the first few simulated means for n = 500\nprint(\"\\nFirst five simulated sample means (n = 500):\")\nprint(np.round(sampling_results[500][:5], 2))\n\n# Compare the average of simulated means to the true mean\nsimulated_mean_500 = float(np.mean(sampling_results[500]).round(2))\nprint(f\"\\nAverage of simulated means for n = 500: {simulated_mean_500:.2f}\")\nprint(f\"Difference from true mean: {simulated_mean_500 - true_mean_bmi:+.2f}\")\n\n\n# Plot histograms of sample means for each sample size.\n\nfor n in sample_sizes:\n    means = sampling_results[n]\n\n    plt.figure(figsize=(6, 4))\n    plt.hist(means, bins=20)\n    plt.axvline(pop_mean_bmi, linestyle=\"--\")\n    plt.xlabel(\"Sample mean BMI\")\n    plt.ylabel(\"Frequency across simulations\")\n    plt.title(f\"Sampling distribution of mean BMI (n = {n})\")\n    plt.tight_layout()\n    plt.show()\n\n    print(f\"n = {n}\")\n    print(f\"  Mean of sample means: {means.mean():6.3f}\")\n    print(f\"  SD of sample means:   {means.std():6.3f}\")\n    print(f\"  Population mean BMI:  {pop_mean_bmi:6.3f}\\n\")\n\n\n\n4.2 Interpretation\n\nThe centre of each sampling distribution (mean of the sample means) is close to the true NHANES mean BMI.\nThe spread of the sampling distribution (standard deviation of the sample means) decreases as the sample size increases.\nThe reduction in spread is approximately proportional to 1/√n, which is the basis for many ideas in statistical inference (for example, standard errors and confidence intervals)."
  },
  {
    "objectID": "notebooks/1.05_representative.html#comparing-the-fb2nep-synthetic-cohort-to-nhanes",
    "href": "notebooks/1.05_representative.html#comparing-the-fb2nep-synthetic-cohort-to-nhanes",
    "title": "1.05 – Populations, Samples, and Representativeness",
    "section": "5 5. Comparing the FB2NEP synthetic cohort to NHANES",
    "text": "5 5. Comparing the FB2NEP synthetic cohort to NHANES\nWe now compare the structure of the FB2NEP synthetic cohort to the NHANES reference survey. The aim is not to make the synthetic cohort “representative of the United States”, but to illustrate how one might compare a study sample to a reference population.\nBecause the FB2NEP cohort includes only adults aged 40 years and older, we first create a matching subset of NHANES adults aged 40 years and older.\n\n# Restrict NHANES to adults aged 40+ years and define age groups.\n\nnhanes_40plus = nhanes[nhanes[\"age_years\"] &gt;= 40].copy()\n\nage_bins_40 = [40, 55, 70, np.inf]\nage_labels_40 = [\"40–54\", \"55–69\", \"70+\"]\n\nnhanes_40plus[\"age_group_40\"] = pd.cut(\n    nhanes_40plus[\"age_years\"],\n    bins=age_bins_40,\n    labels=age_labels_40,\n    right=False,\n)\n\n# Define equivalent age groups in the FB2NEP cohort.\ndf_fb2nep = df_fb2nep.copy()\ndf_fb2nep[\"age_group_40\"] = pd.cut(\n    df_fb2nep[\"age\"],\n    bins=age_bins_40,\n    labels=age_labels_40,\n    right=False,\n)\n\nnhanes_40plus[[\"age_years\", \"age_group_40\", \"sex\"]].head()\n\n\nfrom scripts.helpers_tables import compare_two_sources\n\n\n\nprint(\"Sex distribution: NHANES 40+ vs FB2NEP synthetic cohort\")\ndisplay(compare_two_sources(nhanes_40plus, df_fb2nep, \"sex\", \"NHANES40\", \"FB2NEP\"))\n\nprint(\"\\nAge group (40+): NHANES vs FB2NEP synthetic cohort\")\ndisplay(compare_two_sources(nhanes_40plus, df_fb2nep, \"age_group_40\", \"NHANES40\", \"FB2NEP\"))\n\n\n5.1 7.1 Coding differences: sex in NHANES vs FB2NEP\n\nWhat is going on here? NHANES and FB2NEP code sex differently!\nNHANES uses \"Female\" / \"Male\", whereas FB2NEP uses \"F\" / \"M\".\n\nThis is a simple but important example of a coding problem:\n\nThe two datasets refer to the same underlying concept (biological sex at baseline),\nbut they use different labels for the categories.\n\nIf we tabulate or merge without harmonising the codes, we obtain misleading tables:\n\nFour rows (F, M, Female, Male) instead of two,\nmissing values in some of the columns because categories do not align.\n\nThis kind of problem occurs frequently when combining:\n\ndifferent surveys,\nregistry data and cohort data,\ndifferent waves of the same study.\n\nWe now create a harmonised version of sex in both datasets, with the labels \"Female\" and \"Male\", and then repeat the comparison.\n\n# -------------------------------------------\n# Harmonise the coding of 'sex' in both sets\n# -------------------------------------------\n\ndef harmonise_sex(series: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Map different encodings of sex to common labels \"Female\" / \"Male\".\n\n    Parameters\n    ----------\n    series : pandas.Series\n        Original sex variable (for example 'F'/'M' or 'Female'/'Male').\n\n    Returns\n    -------\n    pandas.Series\n        New series with values \"Female\" or \"Male\" (or NaN if unknown).\n\n    Notes\n    -----\n    - Any unexpected categories are printed so that they can be\n      checked manually (for example 'Other', 'Prefer not to say').\n    \"\"\"\n    # Define how original codes are to be translated.\n    code_map = {\n        \"F\": \"Female\",\n        \"M\": \"Male\",\n        \"Female\": \"Female\",\n        \"Male\": \"Male\",\n    }\n\n    # Apply the mapping; entries not in code_map become NaN.\n    mapped = series.map(code_map)\n\n    # Identify any values that were not in the mapping.\n    unknown = series[~series.isna() & mapped.isna()].unique()\n    if len(unknown) &gt; 0:\n        print(\"Warning: unexpected categories in 'sex':\", unknown)\n\n    return mapped\n\n\n# Apply the harmonisation to both datasets.\nnhanes_40plus[\"sex_harmonised\"] = harmonise_sex(nhanes_40plus[\"sex\"])\ndf_fb2nep[\"sex_harmonised\"] = harmonise_sex(df_fb2nep[\"sex\"])\n\n# Quick check: show the distribution in each dataset.\nprint(\"NHANES 40 Plus (harmonised):\")\nprint(nhanes_40plus[\"sex_harmonised\"].value_counts(dropna=False))\nprint(\"\\nFB2NEP (harmonised):\")\nprint(df_fb2nep[\"sex_harmonised\"].value_counts(dropna=False))\n\nLet’s repeat the code above:\n\n\nprint(\"Sex distribution: NHANES 40+ vs FB2NEP synthetic cohort\")\ndisplay(compare_two_sources(nhanes_40plus, df_fb2nep, \"sex_harmonised\", \"NHANES40\", \"FB2NEP\"))\n\nprint(\"\\nAge group (40+): NHANES vs FB2NEP synthetic cohort\")\ndisplay(compare_two_sources(nhanes_40plus, df_fb2nep, \"age_group_40\", \"NHANES40\", \"FB2NEP\"))\n\n\nHippo cameo: imagine that exactly one extremely diligent hippo lives in the catchment area and always volunteers for every nutrition study. The hippo will appear in many study samples, but this single observation tells us little about the many hippos who do not volunteer. This is an example of how selection can be systematic rather than random.\n\n\n\n5.2 Interpretation\n\nThe FB2NEP synthetic cohort is not intended to match NHANES exactly, but it is still useful to compare basic characteristics such as sex and age distribution.\nIf a real study sample differs strongly from a reference survey (such as NHANES or NDNS), then the study may have limited external validity for the general population.\nThe comparison does not by itself prove bias in associations, but it is an important step in describing who we are studying.\n\n\n\n5.3 7.2 Additional variables for representativeness: SES vs education\nWe may wish to compare other characteristics between NHANES and the FB2NEP synthetic cohort, for example:\n\neducation in NHANES (for example, “≤High school”, “Some college”, “Bachelor+”), and\n\nSES_class in FB2NEP (for example, “ABC1”, “C2DE”).\n\nBoth variables describe aspects of socioeconomic position, but they are not the same:\n\neducation is an individual-level measure of highest qualification;\nSES_class in FB2NEP is a social grade based on occupation (ABC1 vs C2DE).\n\nDirect comparison of the original categories is therefore not meaningful. Instead, we can:\n\nInspect the category labels and distributions in each dataset.\nConstruct a very crude harmonised variable that groups people into “lower” and “higher” socioeconomic position.\nCompare these simplified variables, while being explicit about the approximation.\n\nThis illustrates that harmonisation often involves judgement and information loss. In real analyses, this should be documented and justified.\n\nfrom scripts.helpers_tables import compare_two_sources\n\n# -------------------------------------------------\n# Step 1: inspect the original SES / education codes\n# -------------------------------------------------\n\nprint(\"NHANES 'education' categories:\")\nprint(nhanes[\"education\"].value_counts(dropna=False))\nprint(\"\\nFB2NEP 'SES_class' categories:\")\nprint(df_fb2nep[\"SES_class\"].value_counts(dropna=False))\n\n# -------------------------------------------------\n# Step 2: create a very crude harmonised SEP variable\n# -------------------------------------------------\n# For teaching purposes we define:\n#\n#   - In NHANES (education):\n#       \"≤High school\"  -&gt; \"lower\"\n#       \"Some college\"  -&gt; \"middle\"\n#       \"Bachelor+\"     -&gt; \"higher\"\n#\n#   - In FB2NEP (SES_class):\n#       \"C2DE\"          -&gt; \"lower\"\n#       \"ABC1\"          -&gt; \"higher\"\n#\n# We then collapse to a simple \"lower\" vs \"higher_or_middle\" binary\n# variable in both datasets, so that we can compare something roughly\n# analogous across the two sources.\n#\n# This is deliberately crude and should be interpreted with caution.\n\ndef make_sep_binary_from_education(education: pd.Series) -&gt; pd.Series:\n    \"\"\"\n    Convert NHANES education categories into a crude binary SEP measure.\n\n    Returns \"lower\" or \"higher_or_middle\". Explicit \"Unknown\" values\n    are treated as missing (NaN). Any truly unexpected categories are\n    reported.\n    \"\"\"\n    # First treat the NHANES label \"Unknown\" as missing.\n    education_clean = education.replace(\"Unknown\", pd.NA)\n\n    # Map the remaining education categories to a simplified SEP measure.\n    edu_map = {\n        \"≤High school\": \"lower\",\n        \"Some college\": \"higher_or_middle\",\n        \"College+\": \"higher_or_middle\",   # corrected label\n    }\n\n    sep = education_clean.map(edu_map)\n\n    # Identify any values that are not missing and were not mapped.\n    unknown = education_clean[~education_clean.isna() & sep.isna()].unique()\n    if len(unknown) &gt; 0:\n        print(\"Warning: unexpected education categories:\", unknown)\n\n    return sep\n\n\ndef make_sep_binary_from_ses_class(ses: pd.Series) -&gt; pd.Series:\n    \"\"\"Convert FB2NEP SES_class into a crude binary SEP measure.\n\n    Returns \"lower\" or \"higher_or_middle\".\n    \"\"\"\n    # Create dictionary mapping SES to category\n    \n\n    ses_map = {\n        \"C2DE\": \"lower\",\n        \"ABC1\": \"higher_or_middle\",\n    }\n\n    sep = ses.map(ses_map)\n\n    unknown = ses[~ses.isna() & sep.isna()].unique()\n    if len(unknown) &gt; 0:\n        print(\"Warning: unexpected SES_class categories:\", unknown)\n\n    return sep\n\n\n# Apply the mappings to create harmonised binary SEP variables.\nnhanes[\"SEP_binary\"] = make_sep_binary_from_education(nhanes[\"education\"])\ndf_fb2nep[\"SEP_binary\"] = make_sep_binary_from_ses_class(df_fb2nep[\"SES_class\"])\n\nprint(\"\\nNHANES SEP_binary:\")\nprint(nhanes[\"SEP_binary\"].value_counts(dropna=False))\nprint(\"\\nFB2NEP SEP_binary:\")\nprint(df_fb2nep[\"SEP_binary\"].value_counts(dropna=False))\n\n# -------------------------------------------------\n# Step 3: compare the crude SEP distributions\n# -------------------------------------------------\n\nsep_comparison = compare_two_sources(\n    ref=nhanes,\n    study=df_fb2nep,\n    column=\"SEP_binary\",\n    ref_label=\"NHANES40\",\n    study_label=\"FB2NEP\",\n)\n\nsep_comparison"
  },
  {
    "objectID": "notebooks/1.05_representative.html#exercises",
    "href": "notebooks/1.05_representative.html#exercises",
    "title": "1.05 – Populations, Samples, and Representativeness",
    "section": "6 8. Exercises",
    "text": "6 8. Exercises\n\nChange the sample size in the BMI simulation\nIn the sampling demonstration, add another sample size (for example, n = 50 or n = 5000) to the sample_sizes list and rerun the simulation. How does the spread of the sample mean BMI change? Relate your findings to the idea that standard errors shrink with 1/√n.\nAlternative age groupings\nDefine different age groupings (for example, two broad groups 40–64 and 65+) and repeat the comparison. How does the choice of grouping affect your impression of representativeness?\nSelection bias thought experiment\nSuppose that in a real cohort, people with very poor health are less likely to participate. Describe in a short paragraph how this could bias estimates of the association between physical activity and cardiovascular disease.\nCountry-specific reference surveys\nIn the United Kingdom, the National Diet and Nutrition Survey (NDNS) is often used as a reference for diet and some health indicators. Look up (outside this notebook) what NDNS is and which population it covers. How might you compare a United Kingdom cohort to NDNS in a similar way to what we have done here with NHANES?"
  },
  {
    "objectID": "notebooks/1.05_representative.html#summary",
    "href": "notebooks/1.05_representative.html#summary",
    "title": "1.05 – Populations, Samples, and Representativeness",
    "section": "7 9. Summary",
    "text": "7 9. Summary\n\nPopulation, target population, and sample are related but distinct concepts. It is essential to be explicit about each before analysing data.\nRepresentativeness concerns the similarity of the sample to the population of interest in terms of key characteristics and is closely linked to external validity.\nSampling error arises because we observe only a finite sample. It decreases as the sample size increases but never disappears entirely.\nUsing a reference survey such as NHANES allows us to compare the structure of a study sample (here: the FB2NEP synthetic cohort) to a broader population.\nRestricted or specialised cohorts can provide excellent information about associations within certain groups but are not automatically representative of all adults.\nComparing your own data to reference surveys is a routine and important step in nutritional epidemiology, both for describing study populations and for assessing the generalisability of findings."
  },
  {
    "objectID": "notebooks/2.02_qaly.html",
    "href": "notebooks/2.02_qaly.html",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "",
    "text": "Learning Objectives - Understand the components of Disability-Adjusted Life Years (DALYs) - Calculate Years of Life Lost (YLL) and Years Lived with Disability (YLD) - Explain how disability weights are elicited and why methods produce different values - Compare DALYs and Quality-Adjusted Life Years (QALYs) - Critically evaluate key assumptions embedded in these measures"
  },
  {
    "objectID": "notebooks/2.02_qaly.html#why-do-we-need-summary-measures-of-health",
    "href": "notebooks/2.02_qaly.html#why-do-we-need-summary-measures-of-health",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "1 1. Why Do We Need Summary Measures of Health?",
    "text": "1 1. Why Do We Need Summary Measures of Health?\nTraditional health statistics often focus on mortality (death rates, life expectancy, cause of death). This misses an essential part of population health: morbidity.\nConsider two conditions: - Ischaemic heart disease: substantial mortality, often with disability before death - Low back pain: very low mortality, but common and disabling\nIf we only count deaths, we systematically undervalue conditions that cause suffering without killing. Summary measures such as DALYs and QALYs attempt to capture both mortality and morbidity in a single metric.\n\n1.1 The policy problem\nA fixed budget forces comparisons across outcomes. For example: - Preventing deaths from heart disease - Reducing chronic pain in large numbers of people - Restoring vision through surgery\nTo compare these, we need a common currency for health."
  },
  {
    "objectID": "notebooks/2.02_qaly.html#setup",
    "href": "notebooks/2.02_qaly.html#setup",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "2 2. Setup",
    "text": "2 2. Setup\nFirst, we load the course utilities and reference data.\n\n# ============================================================\n# Bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the course repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads the course utility module (epi_utils.py).\n#\n# Important:\n# - You may see messages printed below (e.g. from pip or git).\n# - Warnings (often in yellow) are usually harmless.\n# - If you see a red error traceback, re-run this cell first.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the repository\n# ------------------------------------------------------------\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/epi_utils.py exists)\nif (cwd / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd\n# Case B: we are in a subdirectory of the repository\nelif (cwd.parent / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd.parent\n# Case C: we are outside the repository (e.g. in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if not present\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change working directory to repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\n# Add scripts directory to Python path\nscripts_dir = repo_root / \"scripts\"\nif str(scripts_dir) not in sys.path:\n    sys.path.insert(0, str(scripts_dir))\n\nprint(f\"Repository root: {repo_root}\")\nprint(\"Bootstrap completed successfully.\")\n\n\n# ------------------------------------------------------------\n# Import libraries and course utilities\n# ------------------------------------------------------------\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport ipywidgets as widgets\nfrom ipywidgets import interact, FloatSlider, VBox, HBox, Output\nfrom IPython.display import display\nfrom pandas.io.formats import style\n\n# Import course utilities from the repository\nfrom epi_utils import (\n    LIFE_TABLE, GBD_DISABILITY_WEIGHTS, EXERCISE_CONDITIONS,\n    get_life_expectancy, calculate_yll, calculate_yld,\n    calculate_dalys, calculate_qalys_gained\n)\n\n# A small helper for reliable disability-weight lookup\ndef get_dw(condition_name: str) -&gt; float:\n    \"\"\"Return the disability weight for a condition name, or raise a clear error.\"\"\"\n    row = GBD_DISABILITY_WEIGHTS.loc[GBD_DISABILITY_WEIGHTS[\"condition\"] == condition_name]\n    if row.empty:\n        raise KeyError(\n            f\"Condition not found in GBD_DISABILITY_WEIGHTS: {condition_name!r}. \"\n            \"Check spelling and capitalisation.\"\n        )\n    return float(row[\"disability_weight\"].iloc[0])\n\nprint(\"Libraries loaded successfully.\")\nprint(f\"Python: {sys.version.split()[0]} | pandas: {pd.__version__} | numpy: {np.__version__}\")\n\n\n# View the reference life table\nprint(\"GBD 2019 Reference Life Table (abridged)\")\nprint(\"=\" * 40)\ndisplay(LIFE_TABLE)\n\n\n# View disability weights for selected conditions\nprint(\"GBD 2019 Disability Weights (selected conditions)\")\nprint(\"=\" * 60)\ndisplay(GBD_DISABILITY_WEIGHTS.sort_values(\"disability_weight\", ascending=False))"
  },
  {
    "objectID": "notebooks/2.02_qaly.html#building-dalys-from-first-principles",
    "href": "notebooks/2.02_qaly.html#building-dalys-from-first-principles",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "3 3. Building DALYs from First Principles",
    "text": "3 3. Building DALYs from First Principles\nThe DALY combines two components:\n\\[\\text{DALY} = \\text{YLL} + \\text{YLD}\\]\nWhere: - YLL (Years of Life Lost) measures premature mortality - YLD (Years Lived with Disability) measures time spent in less than full health\nInterpretation: 1 DALY corresponds to 1 year of healthy life lost. Higher DALYs indicate greater burden.\n\n3.1 3.1 Years of Life Lost (YLL)\nYLL compares the age at death to a reference life expectancy:\n\\[\\text{YLL} = \\sum_{a} N_a \\times L_a\\]\nWhere: - \\(N_a\\) is the number of deaths at age \\(a\\) - \\(L_a\\) is the standard life expectancy remaining at age \\(a\\)\n\n# Example: Mortality from ischaemic heart disease\n\nihd_deaths = {\n    45: 50,\n    55: 200,\n    65: 500,\n    75: 800,\n    85: 400\n}\n\ntotal_yll, yll_breakdown = calculate_yll(ihd_deaths)\n\nprint(\"Ischaemic Heart Disease - YLL Calculation\")\nprint(\"=\" * 50)\ndisplay(yll_breakdown)\n\nn_deaths = sum(ihd_deaths.values())\nprint(f\"\\nTotal deaths: {n_deaths:,}\")\nprint(f\"Total YLL: {total_yll:,.0f} years\")\nprint(f\"Average YLL per death: {total_yll / n_deaths:.1f} years\")\n\n\n\n3.2 Exercise 3.1: Calculate YLL for a different condition\nRoad traffic accidents often affect younger age groups. Use the mortality data below to calculate YLL and compare with ischaemic heart disease.\nYour output should include: - Total deaths and total YLL for road traffic accidents - A short comparison statement (one or two sentences)\n\n# Road traffic accident deaths\nrta_deaths = {\n    15: 30,\n    25: 80,\n    35: 60,\n    45: 40,\n    55: 20,\n    65: 10\n}\n\ntotal_rta_yll, yll_rta_breakdown = calculate_yll(rta_deaths)\n\nprint(\"RTA - YLL Calculation\")\nprint(\"=\" * 50)\ndisplay(yll_rta_breakdown)\n\nn_rta_deaths = sum(rta_deaths.values())\nprint(f\"\\nTotal deaths: {n_rta_deaths:,}\")\nprint(f\"Total YLL: {total_rta_yll:,.0f} years\")\nprint(f\"Average YLL per death: {total_rta_yll / n_rta_deaths:.1f} years\")\n\n\n\n\nprint(\"\\n\\nCompare with IHD\")\nprint(f\"\\nTotal deaths: {n_deaths:,}\")\nprint(f\"Total YLL: {total_yll:,.0f} years\")\nprint(f\"Average YLL per death: {total_yll / n_deaths:.1f} years\")\n\n\n\n3.3 3.2 Years Lived with Disability (YLD)\nIn this notebook we use a prevalence-based formulation (as used in modern GBD work):\n\\[\\text{YLD} = \\text{Prevalence} \\times DW\\]\nWhere: - Prevalence is the number of people living with the condition (a stock, not a flow) - DW is the disability weight on a 0–1 scale (0 = full health, 1 = equivalent to death)\nWhen prevalence is measured at a point in time, the YLD can be interpreted as the number of healthy years lost per year in the population.\n\n# Example: Comparing YLD for different conditions (1-year interpretation)\n\nconditions = [\n    {\"name\": \"Type 2 diabetes (without complications)\", \"prevalence\": 200_000},\n    {\"name\": \"Depression (moderate)\", \"prevalence\": 100_000},\n    {\"name\": \"Back pain (severe)\", \"prevalence\": 75_000},\n    {\"name\": \"Iron deficiency\", \"prevalence\": 3_000},\n]\n\nprint(\"YLD Comparison (prevalence-based; interpret as annual healthy years lost)\")\nprint(\"YLD per Year in population of ~ 5 Million\")\nprint(\"=\" * 75)\n\nyld_results = []\nfor c in conditions:\n    dw = get_dw(c[\"name\"])\n    yld = calculate_yld(c[\"prevalence\"], dw)\n    yld_results.append(\n        {\n            \"Condition\": c[\"name\"],\n            \"Prevalence\": c[\"prevalence\"],\n            \"Disability weight (DW)\": dw,\n            \"YLD\": yld,\n        }\n    )\n\ndf_yld = pd.DataFrame(yld_results)\n\ndf_yld[\"Disability weight (DW)\"] = df_yld[\"Disability weight (DW)\"].round(3)\ndf_yld[\"YLD\"] = df_yld[\"YLD\"].round(0).astype(int)\n\n\ndisplay(df_yld)\n\n\n\n3.4 3.3 Putting it together: Total DALYs\n\n# Ischaemic heart disease: high mortality, moderate disability\nihd = calculate_dalys(\n    deaths_by_age={45: 50, 55: 200, 65: 500, 75: 800, 85: 400},\n    prevalence=50_000,\n    disability_weight=get_dw(\"Ischaemic Heart Disease\"),\n    condition_name=\"Ischaemic Heart Disease\"\n)\n\n\n# Severe low back pain: very low mortality, high disability burden\nlbp = calculate_dalys(\n    deaths_by_age={},  # assume zero deaths for this illustrative example\n    prevalence=200_000,\n    disability_weight=get_dw(\"Back pain (severe)\"),\n    condition_name=\"Back pain (severe)\"\n)"
  },
  {
    "objectID": "notebooks/2.02_qaly.html#comparison-individual-level-dalys",
    "href": "notebooks/2.02_qaly.html#comparison-individual-level-dalys",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "4 Comparison: Individual-Level DALYs",
    "text": "4 Comparison: Individual-Level DALYs\nSo far we have calculated population-level DALYs by summing across many deaths and prevalent cases. But DALYs can also be computed for individuals, which helps illustrate how the components (YLL and YLD) contribute to total burden.\nFor an individual:\n\nYLL = remaining life expectancy at age of death (from reference life table)\nYLD = years lived with condition × disability weight\n\nThe following examples show how different life histories produce different DALY profiles, even when the total burden may be similar.\n\n# Simulated individuals (illustrative)\npeople = [\n    {\n        \"id\": \"Person A\",\n        \"age_at_death\": 75,\n        \"years_with_condition\": 30,\n        \"dw\": get_dw(\"Depression (moderate)\"),\n    },\n    {\n        \"id\": \"Person B\",\n        \"age_at_death\": 75,\n        \"years_with_condition\": 20,\n        \"dw\": get_dw(\"Obesity (class III, BMI &gt;= 40)\"),\n    },\n    {\n        \"id\": \"Person C\",\n        \"age_at_death\": 30,\n        \"years_with_condition\": 1,\n        \"dw\": get_dw(\"Iron deficiency\"),\n    },\n    {\n        \"id\": \"Person D\",\n        \"age_at_death\": 85,\n        \"years_with_condition\": 15,\n        \"dw\": get_dw(\"Cancer (terminal, with severe pain)\"),\n    },\n]\n\nThese four individuals illustrate contrasting patterns:\n\nPerson A dies older (75) after 30 years of moderate depression. Their high YLD (from depression) will dominate.\nPerson B dies at 75 after 20 years of obesity. Their YLL is lower, but the long duration of disability contributes to YLD.\nPerson C dies very young (30) with only 1 year of mild iron deficiency. Almost all burden is from mortality.\nPerson D lives to 85 but endures 15 years of terminal cancer with severe pain. High disability weight means substantial YLD despite near-normal lifespan.\n\n\nresults = []\n\nfor p in people:\n    # YLL: remaining life expectancy at age of death\n    yll = get_life_expectancy(p[\"age_at_death\"])\n\n    # YLD: years lived with disability × DW\n    yld = p[\"years_with_condition\"] * p[\"dw\"]\n\n    results.append({\n        \"Person\": p[\"id\"],\n        \"YLL\": yll,\n        \"YLD\": yld,\n    })\n\ndf_individual = pd.DataFrame(results)\ndisplay(df_individual.round(2))\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.bar(\n    df_individual[\"Person\"],\n    df_individual[\"YLL\"],\n    label=\"Years of Life Lost (YLL)\"\n)\n\nax.bar(\n    df_individual[\"Person\"],\n    df_individual[\"YLD\"],\n    bottom=df_individual[\"YLL\"],\n    label=\"Years Lived with Disability (YLD)\"\n)\n\nax.set_ylabel(\"Years\")\nax.set_title(\"DALYs for Simulated Individuals\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n4.1 Life-course decomposition\nThe stacked bar chart above shows DALYs as the sum of YLL and YLD, but this can obscure the life actually lived. A more complete picture decomposes the entire life course into four segments:\n\nDisability-free life (DFL): years lived in full health before the condition\nHealthy-equivalent years during disability: even with a condition, some health remains (the fraction \\(1 - DW\\))\nYLD: the healthy years lost while living with disability (\\(\\text{duration} \\times DW\\))\nYLL: the healthy years lost due to premature death\n\nWhen stacked, these four components sum to the reference life expectancy (the GBD endpoint). This visualisation shows that DALYs represent the gap between actual health experience and the theoretical maximum.\n\n\n\n# Example people structure (adjust to your actual list)\npeople = [\n    {\"id\": \"Person A\", \"age_at_death\": 75, \"years_with_condition\": 30, \"dw\": get_dw(\"Depression (moderate)\")},\n    {\"id\": \"Person B\", \"age_at_death\": 75, \"years_with_condition\": 20, \"dw\": get_dw(\"Back pain (severe)\")},\n    {\"id\": \"Person C\", \"age_at_death\": 30, \"years_with_condition\":  1, \"dw\": get_dw(\"Iron deficiency\")},\n    {\"id\": \"Person D\", \"age_at_death\": 85, \"years_with_condition\": 15, \"dw\": get_dw(\"Type 2 diabetes (without complications)\")},\n]\n\n\nrows = []\nfor p in people:\n    age = p[\"age_at_death\"]\n    t = p[\"years_with_condition\"]\n    dw = p[\"dw\"]\n\n    # Simple assumption for teaching: condition starts t years before death\n    dfl_years = max(age - t, 0)                 # years lived disability-free\n    lived_with_disability_healthy = t * (1 - dw)\n    yld = t * dw\n\n    # YLL from the reference life table (remaining LE at age of death)\n    yll = get_life_expectancy(age)\n\n    reference_endpoint = age + yll\n\n    rows.append({\n        \"Person\": p[\"id\"],\n        \"DFL (years lived disability-free)\": dfl_years,\n        \"Healthy lived (during disability)\": lived_with_disability_healthy,\n        \"YLD (healthy years lost)\": yld,\n        \"YLL (years of life lost)\": yll,\n        \"Reference endpoint age\": reference_endpoint,\n    })\n\ndf = pd.DataFrame(rows)\n\n# ---- Plot (horizontal stacked bars read better) ----\nfig, ax = plt.subplots(figsize=(10, 6))\n\ny = np.arange(len(df))\n\nleft = np.zeros(len(df))\nax.barh(y, df[\"DFL (years lived disability-free)\"], left=left, label=\"DFL (lived, disability-free)\")\nleft += df[\"DFL (years lived disability-free)\"]\n\nax.barh(y, df[\"Healthy lived (during disability)\"], left=left, label=\"Healthy lived (during disability)\")\nleft += df[\"Healthy lived (during disability)\"]\n\nax.barh(y, df[\"YLD (healthy years lost)\"], left=left, label=\"YLD (healthy-equivalent loss)\")\nleft += df[\"YLD (healthy years lost)\"]\n\nax.barh(y, df[\"YLL (years of life lost)\"], left=left, label=\"YLL (loss due to premature death)\")\n\nax.set_yticks(y)\nax.set_yticklabels(df[\"Person\"])\nax.set_xlabel(\"Years (stacked to the GBD reference endpoint)\")\nax.set_title(\"Life course decomposition to reference endpoint: DFL → disability → YLD → YLL\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\ndisplay(df[[\n    \"Person\",\n    \"Reference endpoint age\",\n    \"DFL (years lived disability-free)\",\n    \"Healthy lived (during disability)\",\n    \"YLD (healthy years lost)\",\n    \"YLL (years of life lost)\",\n]].round(2))\n\n\n\n4.2 Interpreting the comparison\nNotice several patterns in these individual DALY profiles:\nPremature death dominates for young adults. Person C (dying at 30) has almost no YLD because the condition was brief and mild, but a very large YLL because decades of potential life were lost.\nHigh disability weights matter more with longer duration. Person D’s terminal cancer (DW ≈ 0.54) over 15 years generates substantial YLD, even though they lived to 85.\nCommon conditions with modest weights can accumulate burden. Obesity or diabetes with DW around 0.05–0.09 might seem mild, but over 20+ years the YLD accumulates.\nThe reference endpoint is aspirational, not typical. The GBD reference life table reflects a theoretical maximum (around 86–88 years for adults). Most populations do not achieve this, which is precisely the point: the gap between actual and aspirational is what DALYs measure."
  },
  {
    "objectID": "notebooks/2.02_qaly.html#disability-weights-what-they-are-and-how-they-are-elicited",
    "href": "notebooks/2.02_qaly.html#disability-weights-what-they-are-and-how-they-are-elicited",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "5 4. Disability weights: what they are, and how they are elicited",
    "text": "5 4. Disability weights: what they are, and how they are elicited\nA disability weight is not measured in a laboratory. It is derived from judgements about the severity of a health state.\n\n5.1 4.1 How GBD elicits disability weights\nThe Global Burden of Disease (GBD) study primarily uses paired comparisons from large surveys. Respondents are shown two short descriptions of health states and asked which person is healthier. These ordinal judgements are modelled statistically and then anchored to produce disability weights on a 0–1 scale.\n\n\n5.2 4.2 Other elicitation methods (and why they differ)\nDifferent elicitation methods often produce systematically different values for the same health state.\n\nVisual analogue scale (VAS): respondents place a health state on a line (for example 0 = death, 100 = full health). This is simple, but it is sensitive to anchoring and does not enforce an explicit trade-off.\nTime trade-off (TTO): respondents trade length of life for quality of life (for example, 10 years in a health state is equivalent to x years in full health). This enforces a quantity–quality trade, but results depend on the time horizon and attitudes to death.\nStandard gamble (SG): respondents choose between a certain impaired health state and a gamble between full health and immediate death. This incorporates risk and is influenced by risk aversion.\nPerson trade-off (PTO): respondents trade benefits across people (for example, curing 100 people of A versus curing y people of B). This resembles policy choices but mixes severity with moral judgements about distribution.\n\nUtilities and negative values: in some QALY valuation systems, a health state can be valued as worse than death, producing negative utilities. Disability weights used in DALYs are bounded between 0 and 1; they do not take negative values.\n\n\n5.3 4.3 Mini-exercise: VAS versus TTO for one health state\nIn this mini-exercise you will value one health state using two methods:\n\nVAS: you place the state on a 0–100 scale\nTTO: you indicate how many years in full health are equivalent to 10 years in the state\n\nWe convert to a disability weight using: \\[DW = 1 - u\\] where \\(u\\) is the implied utility.\n\n# Choose one condition for the method comparison\ncondition_name = \"Depression (moderate)\"\ngbd_dw = get_dw(condition_name)\ngbd_u = 1 - gbd_dw\n\nprint(f\"Condition: {condition_name}\")\nprint(f\"GBD disability weight (DW): {gbd_dw:.3f}  |  implied utility (1-DW): {gbd_u:.3f}\")\n\n\n# Interactive comparison: VAS and TTO\n\nvas = widgets.IntSlider(\n    value=70, min=0, max=100, step=1,\n    description=\"VAS (0–100):\",\n    continuous_update=False\n)\n\ntto = widgets.FloatSlider(\n    value=8.0, min=0.0, max=10.0, step=0.1,\n    description=\"TTO (years):\",\n    continuous_update=False\n)\n\nout = Output()\n\ndef update_display(change=None):\n    with out:\n        out.clear_output(wait=True)\n\n        # VAS utility and DW\n        u_vas = vas.value / 100\n        dw_vas = 1 - u_vas\n\n        # TTO utility and DW (10-year horizon)\n        u_tto = tto.value / 10\n        dw_tto = 1 - u_tto\n\n        df = pd.DataFrame(\n            {\n                \"Method\": [\"VAS\", \"TTO (10-year horizon)\", \"GBD (reference)\"],\n                \"Utility (u)\": [u_vas, u_tto, 1 - gbd_dw],\n                \"Disability weight (DW = 1-u)\": [dw_vas, dw_tto, gbd_dw],\n            }\n        )\n\n        display(df.style.format({\"Utility (u)\": \"{:.3f}\", \"Disability weight (DW = 1-u)\": \"{:.3f}\"}))\n\nfor w in [vas, tto]:\n    w.observe(update_display, names=\"value\")\n\nupdate_display()\ndisplay(VBox([widgets.HTML(\"&lt;b&gt;Set values and compare:&lt;/b&gt;\"), vas, tto, out]))\n\n\n\n5.4 4.4 Exercise: Set your own disability weights\nAssign your own disability weights to the conditions below, then compare them to the GBD values.\nInterpretation: - 0.00 means full health - 1.00 means equivalent to death\n\n# Display conditions for students to consider\nprint(\"DISABILITY WEIGHT EXERCISE\")\nprint(\"=\" * 80)\nprint(\"\\nAssign a disability weight (0–1) to each condition below.\")\nprint(\"0 = full health, 1 = equivalent to death\")\nprint(\"-\" * 80)\n\nfor i, c in enumerate(EXERCISE_CONDITIONS, 1):\n    print(f\"\\n{i}. {c['name'].upper()}\")\n    print(f\"   {c['description']}\")\n\n\n# Interactive widget for setting disability weights\n\nsliders = {}\nslider_widgets = []\n\nfor c in EXERCISE_CONDITIONS:\n    slider = FloatSlider(\n        value=0.10, min=0.0, max=1.0, step=0.01,\n        description=\"\",\n        continuous_update=False,\n        readout_format=\".2f\",\n        layout=widgets.Layout(width=\"320px\")\n    )\n    sliders[c[\"name\"]] = slider\n    label = widgets.HTML(value=f\"&lt;b&gt;{c['name']}&lt;/b&gt;\", layout=widgets.Layout(width=\"220px\"))\n    slider_widgets.append(HBox([label, slider]))\n\noutput = Output()\n\ndef compare_weights(change=None):\n    with output:\n        output.clear_output(wait=True)\n\n        comparison_data = []\n        for c in EXERCISE_CONDITIONS:\n            your_weight = sliders[c[\"name\"]].value\n            gbd_weight = c[\"gbd_weight\"]\n            comparison_data.append(\n                {\n                    \"Condition\": c[\"name\"],\n                    \"Your weight\": your_weight,\n                    \"GBD weight\": gbd_weight,\n                    \"Difference (you - GBD)\": your_weight - gbd_weight,\n                }\n            )\n\n        df = pd.DataFrame(comparison_data)\n\n        # Simple plot without hard-coded colours\n        fig, ax = plt.subplots(figsize=(12, 6))\n        x = np.arange(len(df))\n        width = 0.40\n\n        ax.bar(x - width/2, df[\"Your weight\"], width, label=\"Your weights\")\n        ax.bar(x + width/2, df[\"GBD weight\"], width, label=\"GBD weights\")\n\n        ax.set_ylabel(\"Disability weight\")\n        ax.set_title(\"Your disability weights versus GBD 2019\")\n        ax.set_xticks(x)\n        ax.set_xticklabels(df[\"Condition\"], rotation=45, ha=\"right\")\n        ax.set_ylim(0, 1)\n        ax.legend()\n        plt.tight_layout()\n        plt.show()\n\n        display(\n            df.style.format(\n                {\"Your weight\": \"{:.3f}\", \"GBD weight\": \"{:.3f}\", \"Difference (you - GBD)\": \"{:+.3f}\"}\n            )\n        )\n\nfor slider in sliders.values():\n    slider.observe(compare_weights, names=\"value\")\n\ncompare_button = widgets.Button(description=\"Compare to GBD\")\ncompare_button.on_click(compare_weights)\n\nprint(\"Adjust the sliders, then click 'Compare to GBD':\")\ndisplay(VBox(slider_widgets + [compare_button, output]))\n\n\n\n5.5 4.5 Discussion questions\n\nWhere did you differ most from GBD, and why?\nAdaptation: people living with a condition may rate it as less severe. Whose values should count?\nCultural variation: might valuations differ across settings?\nSome valuation systems allow negative utilities (worse than death). How should a policy system handle this conceptually?"
  },
  {
    "objectID": "notebooks/2.02_qaly.html#how-disability-weights-change-rankings",
    "href": "notebooks/2.02_qaly.html#how-disability-weights-change-rankings",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "6 5. How disability weights change rankings",
    "text": "6 5. How disability weights change rankings\nWe now use a hypothetical dataset to show how disability weights affect disease-burden rankings.\n\n# Hypothetical prevalence and mortality data (illustrative)\npopulation_data = {\n    \"Mild anaemia\": {\"prevalence\": 500_000, \"deaths_by_age\": {}},\n    \"Moderate hearing loss\": {\"prevalence\": 300_000, \"deaths_by_age\": {}},\n    \"Moderate depression\": {\"prevalence\": 150_000, \"deaths_by_age\": {45: 50, 55: 30, 65: 20}},\n    \"Severe low back pain\": {\"prevalence\": 100_000, \"deaths_by_age\": {}},\n    \"Complete blindness\": {\"prevalence\": 30_000, \"deaths_by_age\": {}},\n    \"Severe dementia\": {\"prevalence\": 80_000, \"deaths_by_age\": {75: 2_000, 85: 5_000}},\n    \"Type 2 diabetes (controlled)\": {\"prevalence\": 400_000, \"deaths_by_age\": {55: 100, 65: 500, 75: 1_000, 85: 500}},\n    \"Obesity (BMI ≥ 40)\": {\"prevalence\": 200_000, \"deaths_by_age\": {45: 50, 55: 200, 65: 300, 75: 200}},\n}\n\ndef calculate_burden_rankings(population_data: dict, disability_weights: dict, strict: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Return DALY rankings for a set of conditions.\n\n    Parameters\n    ----------\n    population_data:\n        Dictionary of condition -&gt; {prevalence, deaths_by_age}.\n    disability_weights:\n        Dictionary of condition -&gt; disability weight.\n    strict:\n        If True, raise an error when a disability weight is missing. If False, use NaN.\n    \"\"\"\n    results = []\n\n    for condition, data in population_data.items():\n        if condition not in disability_weights:\n            if strict:\n                raise KeyError(f\"Missing disability weight for condition: {condition!r}\")\n            dw = np.nan\n        else:\n            dw = disability_weights[condition]\n\n        yll = 0.0\n        if data[\"deaths_by_age\"]:\n            yll, _ = calculate_yll(data[\"deaths_by_age\"])\n\n        yld = calculate_yld(data[\"prevalence\"], dw) if pd.notna(dw) else np.nan\n        dalys = yll + yld if pd.notna(yld) else np.nan\n\n        results.append({\"Condition\": condition, \"DW\": dw, \"YLL\": yll, \"YLD\": yld, \"DALYs\": dalys})\n\n    df = pd.DataFrame(results).sort_values(\"DALYs\", ascending=False)\n    df[\"Rank\"] = range(1, len(df) + 1)\n    return df[[\"Rank\", \"Condition\", \"DW\", \"YLL\", \"YLD\", \"DALYs\"]]\n\n\n# Rankings using GBD weights (for these named conditions)\ngbd_weights = {c[\"name\"]: c[\"gbd_weight\"] for c in EXERCISE_CONDITIONS}\ngbd_rankings = calculate_burden_rankings(population_data, gbd_weights, strict=True)\n\nprint(\"Disease burden rankings using GBD disability weights\")\nprint(\"=\" * 80)\ndisplay(gbd_rankings.style.format({\"DW\": \"{:.3f}\", \"YLL\": \"{:,.0f}\", \"YLD\": \"{:,.0f}\", \"DALYs\": \"{:,.0f}\"}))\n\n\n# Rankings using your weights (from the sliders)\nyour_weights = {name: slider.value for name, slider in sliders.items()}\nyour_rankings = calculate_burden_rankings(population_data, your_weights, strict=True)\n\nprint(\"Disease burden rankings using your disability weights\")\nprint(\"=\" * 80)\ndisplay(your_rankings.style.format({\"DW\": \"{:.3f}\", \"YLL\": \"{:,.0f}\", \"YLD\": \"{:,.0f}\", \"DALYs\": \"{:,.0f}\"}))"
  },
  {
    "objectID": "notebooks/2.02_qaly.html#dalys-versus-qalys",
    "href": "notebooks/2.02_qaly.html#dalys-versus-qalys",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "7 6. DALYs versus QALYs",
    "text": "7 6. DALYs versus QALYs\n\n\n\n\n\n\n\n\nAspect\nDALY\nQALY\n\n\n\n\nDirection\nHigher = more burden\nHigher = more health gain\n\n\nTypical use\nPopulation burden estimation\nCost-effectiveness analysis\n\n\nCommon users\nGBD, WHO\nNICE and health technology assessment\n\n\n\nA useful identity is:\n\\[\\text{QALYs} = \\text{years lived} \\times u\\]\nwhere \\(u\\) is a health-related utility (often on a 0–1 scale).\n\n# Example: Cataract surgery programme (illustrative)\n\nqalys, utility_gain = calculate_qalys_gained(\n    intervention_effect=0.95,\n    population=10_000,\n    duration=15,\n    dw_before=get_dw(\"Blindness\"),\n    dw_after=0.003  # illustrative post-treatment weight\n)\n\nprint(\"Cataract surgery programme (illustrative)\")\nprint(\"=\" * 55)\nprint(\"Population: 10,000 people\")\nprint(\"Success rate: 95%\")\nprint(\"Duration of benefit: 15 years\")\nprint()\nprint(f\"Utility gain per person: {utility_gain:.3f}\")\nprint(f\"Total QALYs gained: {qalys:,.0f}\")\n\nprogramme_cost = 10_000_000\nprint()\nprint(f\"If programme cost is £10,000,000: £{programme_cost / qalys:,.0f} per QALY (simple ratio)\")"
  },
  {
    "objectID": "notebooks/2.02_qaly.html#limitations-and-critique",
    "href": "notebooks/2.02_qaly.html#limitations-and-critique",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "8 7. Limitations and critique",
    "text": "8 7. Limitations and critique\n\n8.1 7.1 The disability rights critique\nA common critique is that QALYs (and sometimes DALY-based reasoning) can disadvantage people with disability if the model treats their lives as systematically producing fewer quality-adjusted years.\n\n\n8.2 7.2 Illustration: the discrimination problem\n\nprint(\"The QALY discrimination illustration\")\nprint(\"=\" * 60)\nprint(\"Scenario: A life-saving treatment adds 10 years of life.\")\nprint()\n\nqalys_nondisabled = 10 * 1.0\nprint(f\"Patient A (utility 1.0): 10 years × 1.0 = {qalys_nondisabled:.1f} QALYs\")\n\ndw_paraplegia = 0.133  # illustrative\nu_paraplegia = 1 - dw_paraplegia\nqalys_disabled = 10 * u_paraplegia\nprint(f\"Patient B (paraplegia, utility {u_paraplegia:.3f}): 10 years × {u_paraplegia:.3f} = {qalys_disabled:.1f} QALYs\")\nprint()\nprint(f\"Under this framework, Patient A yields {qalys_nondisabled / qalys_disabled:.1%} of Patient B's QALYs for the same life extension.\")"
  },
  {
    "objectID": "notebooks/2.02_qaly.html#exercises",
    "href": "notebooks/2.02_qaly.html#exercises",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "9 8. Exercises",
    "text": "9 8. Exercises\n\n9.1 Exercise 8.1: Calculate DALYs for iron deficiency anaemia\nData (hypothetical population): - Mild anaemia: 800,000 (DW = 0.004) - Moderate anaemia: 150,000 (DW = 0.052) - Severe anaemia: 20,000 (DW = 0.149) - Deaths: 500 at age 75, and 1,000 at age 85\nYour output should include: - YLD (sum across severities) - YLL (from deaths) - Total DALYs\n\n# YOUR CODE HERE\n\n# Hint:\n# 1. Compute YLD for each severity using calculate_yld(prevalence, dw)\n# 2. Compute YLL using calculate_yll({75: 500, 85: 1000})\n# 3. Add them to obtain total DALYs\n\n\n\n9.2 Exercise 8.2: Reflection questions\n\nThe GBD weight for obesity (0.086) is lower than for depression (0.145). Do you agree?\nSome early DALY formulations included age-weighting (valuing some ages more than others). What are the arguments for and against?\nHow might you measure the burden of poor diet without relying only on disease categories?"
  },
  {
    "objectID": "notebooks/2.02_qaly.html#references",
    "href": "notebooks/2.02_qaly.html#references",
    "title": "2.02 - Quantitative Measures of Population Health — DALYs and QALYs",
    "section": "10 References",
    "text": "10 References\n\nGBD 2019 Diseases and Injuries Collaborators. (2020). The Lancet.\nSalomon JA et al. (2015). Disability weights for the Global Burden of Disease Study. Lancet Global Health.\nNord E. (2015). Limitations of the QALY. Cambridge Quarterly of Healthcare Ethics."
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html",
    "href": "notebooks/1.08_regression_modelling_01.html",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "",
    "text": "Version 0.0.5\nThis workbook introduces the foundations of regression modelling in nutritional epidemiology.\nWe will focus on:\nAll analyses use the synthetic FB2NEP cohort.\nRun the first code cell to configure the repository and load the dataset.\n# ============================================================\n# FB2NEP bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the fb2nep-epi repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads and runs scripts/bootstrap.py.\n# - Makes the main dataset available as the variable `df`.\n#\n# Important:\n# - You may see messages printed below (for example from pip\n#   or from the bootstrap script). This is expected.\n# - You may also see WARNINGS (often in yellow). In most cases\n#   these are harmless and can be ignored for this module.\n# - The main thing to watch for is a red error traceback\n#   (for example FileNotFoundError, ModuleNotFoundError).\n#   If that happens, please re-run this cell first. If the\n#   error persists, ask for help.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\nimport importlib.util\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\n# REPO_URL: address of the GitHub repository.\n# REPO_DIR: folder name that will be created when cloning.\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the fb2nep-epi repository\n# ------------------------------------------------------------\n# In local Jupyter, you may already be inside the repository,\n# for example in fb2nep-epi/notebooks.\n#\n# In Colab, the default working directory is /content, so\n# we need to clone the repository into /content/fb2nep-epi\n# and then change into that folder.\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/bootstrap.py exists here)\nif (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n    repo_root = cwd\n\n# Case B: we are outside the repository (for example in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if it is not present yet\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change the working directory to the repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nprint(f\"Repository root set to: {repo_root}\")\n\n# ------------------------------------------------------------\n# 2. Load scripts/bootstrap.py as a module and call init()\n# ------------------------------------------------------------\n# The shared bootstrap script contains all logic to:\n# - Ensure that required Python packages are installed.\n# - Ensure that the synthetic dataset exists (and generate it\n#   if needed).\n# - Load the dataset into a pandas DataFrame.\n#\n# We load the script as a normal Python module (fb2nep_bootstrap)\n# and then call its init() function.\nbootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n\nif not bootstrap_path.is_file():\n    raise FileNotFoundError(\n        f\"Could not find {bootstrap_path}. \"\n        \"Please check that the fb2nep-epi repository structure is intact.\"\n    )\n\n# Create a module specification from the file\nspec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\nbootstrap = importlib.util.module_from_spec(spec)\nsys.modules[\"fb2nep_bootstrap\"] = bootstrap\n\n# Execute the bootstrap script in the context of this module\nspec.loader.exec_module(bootstrap)\n\n# The init() function is defined in scripts/bootstrap.py.\n# It returns:\n# - df   : the main synthetic cohort as a pandas DataFrame.\n# - CTX  : a small context object with paths, flags and settings.\ndf, CTX = bootstrap.init()\n\n# Optionally expose a few additional useful variables from the\n# bootstrap module (if they exist). These are not essential for\n# most analyses, but can be helpful for advanced use.\nfor name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n    if hasattr(bootstrap, name):\n        globals()[name] = getattr(bootstrap, name)\n\nprint(\"Bootstrap completed successfully.\")\nprint(\"The main dataset is available as the variable `df`.\")\nprint(\"The context object is available as `CTX`.\")\n\"\"\"Import required libraries for regression and survival analysis.\n\nThis cell:\n\n- Imports core numerical and plotting libraries.\n- Imports regression tools (statsmodels, scipy).\n- Imports patsy for spline functions.\n- Ensures that the 'lifelines' package is available (local Jupyter and Colab).\n\"\"\"\n\n# Core data handling and plotting\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Linear and generalised linear models\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\n\n# Statistical utilities (for example, Q–Q plots)\nfrom scipy import stats\n\n# Patsy for design matrices and spline terms.\n# 'cr' is the cubic regression spline used in restricted cubic spline examples.\nfrom patsy import dmatrix, dmatrices, cr\n\n# For neat display of tables in notebooks\nfrom IPython.display import display\n\n# Helper function to ensure 'lifelines' is installed.\n# This is defined in scripts/helpers_tables.py for the FB2NEP materials.\nfrom scripts.helpers_tables import ensure_lifelines\n\n# Ensure that 'lifelines' is available, installing it if needed.\nlifelines = ensure_lifelines()\n\n# Import survival analysis tools from lifelines:\n# - KaplanMeierFitter for Kaplan–Meier curves\n# - CoxPHFitter for Cox proportional hazards models\nfrom lifelines import KaplanMeierFitter, CoxPHFitter\n\"\"\"Inspect the first few rows and the variable types.\n\nThis provides a quick overview of the FB2NEP cohort and its variables.\n\"\"\"\n\n\ndisplay(df.head())\ndisplay(df.dtypes.head(30))"
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#what-regression-is",
    "href": "notebooks/1.08_regression_modelling_01.html#what-regression-is",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "1 1. What regression is",
    "text": "1 1. What regression is\nRegression modelling is a central tool in epidemiology. In its most basic form, regression estimates the expected value of an outcome variable given one or more predictors:\n\\[\nE(Y \\mid X_1, X_2, \\ldots, X_p).\n\\]\nThe regression model describes a systematic component (the part explained by the predictors) and a random component (the unexplained variability, or error term).\nIn this workbook we will:\n\nStart with simple regression models.\nExtend to different outcome types.\nIntroduce models for non-linear relationships and for different parts of the outcome distribution.\n\n\n1.1 1.1 Prediction versus inference\nRegression can be used for different purposes:\n\nPrediction: obtain accurate predictions \\(\\hat{Y}\\) for new individuals.\nInference: estimate and interpret the parameters (for example, β, OR, HR) and their uncertainty.\n\nIn nutritional epidemiology we are often interested primarily in inference:\n\nHow much higher is blood pressure, on average, in individuals with high sodium intake?\nWhat is the hazard ratio for cardiovascular disease per 5 kg/m² higher BMI?\n\nPrediction is also important, for example when developing risk scores, but the focus in this workbook is on understanding parameters and assumptions.\nSimple visual example: BMI and age.\nHere we:\n\nCreate a scatter plot of BMI against age.\nAdd rudimentary formatting to make the figure readable.\n\nWe do not fit a model yet; this is purely descriptive.\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.scatter(df[\"age\"], df[\"BMI\"], alpha=0.3, edgecolor=\"none\")\n\nax.set_xlabel(\"Age (years)\")\nax.set_ylabel(\"Body mass index (kg/m²)\")\nax.set_title(\"Scatter plot of BMI against age (FB2NEP cohort)\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n1.2 Interpreting the scatter plot\nThis plot shows each participant’s body mass index (BMI) against age.\nWhen inspecting such a plot, it is useful to consider:\n\nOverall pattern: does BMI tend to increase, decrease, or stay roughly constant with age?\nLinearity: does a straight line seem reasonable, or is there evidence of curvature?\nSpread: is the variability of BMI similar across ages, or does it increase or decrease?\nOutliers or subgroups: are there points or clusters that look unusual?\n\nIn later sections we will use regression models to describe this relationship more formally. For now, focus on whether a simple linear model appears plausible as a first approximation."
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#types-of-regression-models",
    "href": "notebooks/1.08_regression_modelling_01.html#types-of-regression-models",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "2 2. Types of regression models",
    "text": "2 2. Types of regression models\nIn this section we introduce three commonly used regression models in epidemiology:\n\nLinear regression for continuous outcomes.\nLogistic regression for binary outcomes.\nCox proportional hazards regression for time-to-event outcomes.\n\nThe underlying idea is similar in all three cases: we model how the expected outcome (mean, probability, hazard) changes with predictors.\n\n2.1 2.1 Linear regression\nLinear regression models a continuous outcome as a linear function of predictors:\n\\[\nY = \\beta_0 + \\beta_1 X_1 + \\beta_2 X_2 + \\cdots + \\beta_p X_p + \\varepsilon.\n\\]\n\n$ Y $ is a continuous variable (for example, BMI or systolic blood pressure).\n$ X_1, X_2, , X_p $ are predictors (for example, age, sex, smoking status).\n$ $ is a random error term.\n\nThe key quantity is the conditional mean:\n\\[\nE(Y \\mid X_1, \\ldots, X_p) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n\\]\nThe coefficient $ _j $ describes the expected difference in Y associated with a one-unit difference in ( X_j ), holding the other predictors constant.\nLinear regression example: BMI on age (and sex).\nWe will:\n\nFit a simple linear regression model.\nInspect the summary output.\nOverlay the fitted line on a scatter plot.\n\nAssumptions and diagnostics will be discussed later; for now the aim is to see the basic mechanics.\n\nFor this example we assume the following variables exist: - ‘bmi’: continuous outcome - ‘age’: continuous predictor - ‘sex’: binary or categorical (for example ‘Male’, ‘Female’)\n\n\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\n\n# Fit an ordinary least squares (OLS) model using a formula interface.\nmodel_lin = smf.ols(\"BMI ~ age + C(sex)\", data=df)\nresult_lin = model_lin.fit()\n\n# Display a standard model summary.\nresult_lin.summary()\n\n\n\n2.2 Interpreting the linear regression output\nThe model estimates the association between age, sex, and BMI. The key points are:\n\nAge is positively associated with BMI.\n\nThe coefficient for age is 0.071.\n\nFor each additional year of age, the mean BMI is estimated to be 0.071 kg/m² higher, on average, holding sex constant.\n\nThe 95 % confidence interval (0.065 to 0.077) indicates a precise estimate.\n\nSex shows no clear association with BMI in this model.\n\nThe coefficient for C(sex)[T.M] is 0.055 with a P-value of 0.335.\n\nMean BMI does not differ materially between men and women in this dataset.\n\nIntercept (≈ 23.0) represents the estimated mean BMI for the reference group (women) at age 0.\nThis has no substantive interpretation but is necessary for the model.\nR² = 0.023: age and sex explain about 2.3 % of the variability in BMI.\nThis is typical in epidemiological cohorts where many factors influence BMI.\nModel significance\n\nThe F-statistic is large with P &lt; 0.001, indicating that the model explains more variation than a null model with no predictors.\n\nThe small R² emphasises that statistical significance does not necessarily imply a strong association.\n\nResidual diagnostics\n\nSkewness ≈ 0 and kurtosis ≈ 2.94 suggest residuals close to normality.\n\nDurbin–Watson ≈ 2.0 indicates no meaningful autocorrelation (expected for cross-sectional data).\n\nThe Omnibus and Jarque–Bera tests are statistically significant, but with large samples even very small deviations from normality yield significance.\n\n\nSummary:\nAge is strongly and positively associated with BMI in the FB2NEP cohort, but age and sex together explain only a modest proportion of the variability in BMI.\n\n\n2.3 Plot the fitted regression line for BMI ~ age.\nFor visual simplicity we will:\n\nRestrict to one sex (for example, ‘Female’).\nFit a simple model with age as the only predictor in this subgroup.\nOverlay the fitted line on the scatter plot.\n\nThis is purely for illustration.\n\n\n# Subset to one sex (adjust the label if your dataset uses different coding).\ndf_female = df[df[\"sex\"] == \"F\"].copy()\n\nmodel_lin_f = smf.ols(\"BMI ~ age\", data=df_female)\nresult_lin_f = model_lin_f.fit()\n\n# Create a grid of ages spanning the observed range.\nage_grid = np.linspace(df_female[\"age\"].min(), df_female[\"age\"].max(), 100)\npred_df = pd.DataFrame({\"age\": age_grid})\npred_df[\"bmi_hat\"] = result_lin_f.predict(pred_df)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.scatter(df_female[\"age\"], df_female[\"BMI\"], alpha=0.3, edgecolor=\"none\", label=\"Observed BMI\")\nax.plot(pred_df[\"age\"], pred_df[\"bmi_hat\"], linewidth=2, label=\"Fitted line\")\n\nax.set_xlabel(\"Age (years)\")\nax.set_ylabel(\"Body mass index (kg/m²)\")\nax.set_title(\"Linear regression: BMI ~ age (example subset)\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n2.4 2.2 Logistic regression\nLogistic regression is used when the outcome is binary, for example the presence or absence of hypertension.\nLet $ Y {0, 1} $ with $ Y = 1 $ indicating that the event (for example, hypertension) is present. The logistic model specifies the log odds of the event as a linear function of predictors:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p,\n\\]\nwhere $ p = P(Y = 1 X_1, , X_p) $.\nIf we exponentiate a coefficient $ _j $, we obtain an odds ratio:\n\\[\n\\exp(\\beta_j)\n\\]\nwhich describes the multiplicative change in the odds of the outcome associated with a one-unit increase in $ X_j $, holding other predictors constant.\n\n2.4.1 Understanding the logit transformation\nProbabilities are restricted to the interval (0, 1). A linear model of the form\n\\[\np = \\beta_0 + \\beta_1 X\n\\]\nis unsuitable because it can produce values outside this range.\nTo use linear predictors sensibly, we transform the probability using the logit:\n\\[\n\\text{logit}(p) = \\log\\left(\\frac{p}{1-p}\\right).\n\\]\nThe logit has two key properties:\n\nIt maps probabilities $ p (0, 1) $ to the whole real line $ (-, +) $.\n\nIt is strictly increasing: higher probability corresponds to higher logit.\n\nBy modelling the logit of the probability as a linear function of predictors, logistic regression ensures that all predicted values are valid probabilities when we convert back using the inverse-logit:\n\\[\np = \\frac{\\exp(\\eta)}{1 + \\exp(\\eta)}, \\qquad \\eta = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n\\]\nThis connection between the probability ( p ), the logit, and the linear predictor is the basis of logistic regression.\n\n\"\"\"Visualising the logit transformation.\n\nWe:\n\n- Create a grid of probabilities between 0 and 1.\n- Compute logit(p) = log(p / (1 - p)) for each value.\n- Plot logit(p) against p.\n\nThis illustrates how the logit maps probabilities in (0, 1) to the whole real line.\n\"\"\"\n\n\n# Avoid exactly 0 and 1 to prevent division by zero in log(p / (1 - p))\np = np.linspace(0.001, 0.999, 200)\nlogit_p = np.log(p / (1 - p))\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(p, logit_p)\n\nax.set_xlabel(\"Probability p\")\nax.set_ylabel(\"logit(p) = log(p / (1 - p))\")\nax.set_title(\"The logit transformation\")\n\n# Add a horizontal line at 0 to highlight p = 0.5 (where odds = 1)\nax.axhline(0, linewidth=1)\n\nplt.tight_layout()\nplt.show()\n\n\n\n2.4.2 Interpreting the logit plot\n\nAs ( p ), logit(p) goes to −∞.\n\nAs ( p ), logit(p) goes to +∞.\n\nAt ( p = 0.5 ), logit(p) = 0 (odds = 1:1).\n\nThis explains why modelling the logit as a linear function of predictors still gives valid probabilities once we apply the inverse-logit transformation.\nLogistic regression example: hypertension on BMI and age.\nWe assume there is a binary outcome variable ‘hypertension’ coded 0/1.\nThe model:\nlogit(P(hypertension = 1)) = β0 + β1 * BMI + β2 * age + β3 * sex\nWe fit the model and inspect the estimated odds ratios.\n\n\n2.4.3 Creating a binary hypertension variable\nLogistic regression requires a binary outcome. In this workbook we define a variable hypertension as:\n\n1 if the participant has systolic blood pressure ≥ 140 mmHg, or\n\ndiastolic blood pressure ≥ 90 mmHg, or\n\nis on antihypertensive medication,\n\nand 0 otherwise.\nThis is one of several possible definitions; different studies may use slightly different cut-offs. The important point here is to obtain a clearly defined 0/1 variable.\n\n\"\"\"Create a binary hypertension variable for logistic regression.\n\nDefinition (example):\n- Hypertension = 1 if sbp ≥ 140 mmHg or dbp ≥ 90 mmHg or on blood pressure medication.\n- Hypertension = 0 otherwise.\n\nAdjust the variable names and thresholds if your dataset uses different coding.\n\"\"\"\n\n\n# Start with threshold-based hypertension\nhypertension = (df[\"SBP\"] &gt;= 140) \n\ndf[\"hypertension\"] = hypertension.astype(int)\n\ndf[\"hypertension\"].value_counts()\n\nLogistic regression example: hypertension on BMI and age.\nWe assume there is a binary outcome variable ‘hypertension’ coded 0/1.\nThe model:\nlogit(P(hypertension = 1)) = β0 + β1 * BMI + β2 * age + β3 * sex\nWe fit the model and inspect the estimated odds ratios.\n\n\n \n# Fit logistic regression using the formula interface.\nmodel_log = smf.logit(\"hypertension ~ BMI + age + C(sex)\", data=df)\nresult_log = model_log.fit()\n\n# Display the model summary.\ndisplay(result_log.summary())\n\n\n\n\n\n\n2.5 Interpreting the logistic regression output\nThe table above shows the results of a logistic regression model in which hypertension (0/1) is regressed on sex, BMI, and age. Logistic regression models the log odds of the outcome, so the coefficients in the summary are expressed on the logit scale.\nKey elements of the output are:\n\nCoefficients (coef):\nThese represent changes in the log odds of hypertension per unit change in the predictor, holding the other variables constant. They are not directly intuitive and are normally exponentiated to obtain more interpretable effect measures.\nStandard errors and z-values:\nUsed to assess whether the coefficients differ meaningfully from zero on the log-odds scale. With large samples, even small effects can achieve small P-values.\nP-values:\nIndicate evidence against the null hypothesis that a coefficient is zero on the log-odds scale. Here, BMI and age show very small P-values, suggesting clear associations.\nConfidence intervals ([0.025, 0.975]):\nThese are 95 % confidence intervals for the log-odds coefficients.\nPseudo-R² (0.024):\nIndicates that the model explains about 2.4 % of the variation in the log odds. This is typical for epidemiological data, where many factors contribute to hypertension.\nLL-Null and LL:\nThe log-likelihood values for the intercept-only model and for the full model respectively. The likelihood ratio test (LLR P-value) shows that the model provides a significantly better fit than an intercept-only model.\n\nBecause the coefficients are expressed on the log-odds scale, they are not directly interpretable in terms of risk or probability. In the next step we convert them into odds ratios, which provide a more intuitive multiplicative measure of association.\n\n# Extract odds ratios and 95 % confidence intervals.\nparams = result_log.params\nconf = result_log.conf_int()\nor_table = pd.DataFrame({\n    \"OR\": np.exp(params),\n    \"CI_lower\": np.exp(conf[0]),\n    \"CI_upper\": np.exp(conf[1]),\n})\n\nor_table\n\n\n\n2.6 From log-odds coefficients to odds ratios\nLogistic regression estimates effects on the log odds scale:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right) = \\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p.\n\\]\nA coefficient \\(\\beta_j\\) therefore represents the expected change in the log odds of the outcome per one-unit change in \\(X_j\\).\nTo obtain an interpretable effect measure we exponentiate the coefficient:\n\\[\n\\exp(\\beta_j).\n\\]\nThis converts the change in log odds into a multiplicative change in the odds of the outcome:\n\n\\(\\exp(\\beta_j) = 1\\) → no association.\n\n\\(\\exp(\\beta_j) &gt; 1\\) → higher odds of the outcome.\n\n\\(\\exp(\\beta_j) &lt; 1\\) → lower odds of the outcome.\n\nThese exponentiated values are known as odds ratios (ORs).\nFor example, if the coefficient for BMI is 0.059, then:\n\\[\n\\exp(0.059) \\approx 1.06,\n\\]\nmeaning that each additional unit of BMI is associated with approximately 6 % higher odds of hypertension, holding other variables constant.\n\n\n2.7 Predicted probabilities from the logistic model\nWe can use the fitted logistic regression model to obtain predicted probabilities of hypertension for specific combinations of predictors.\nIn this section we:\n\nPlot the predicted probability of hypertension across BMI with a 95 % confidence band for a reference profile (for example, age 60 years, sex = F).\nPlot predicted probabilities across BMI for several different ages to illustrate how the age effect shifts the whole curve.\n\nIn both cases, age and sex are treated as fixed at chosen values, so any changes in the curve reflect only the association with BMI (and age, where varied explicitly).\n\n\"\"\"Predicted probability of hypertension across BMI with 95 % confidence band.\n\nWe:\n\n- Fix age and sex at reference values.\n- Vary BMI from the 5th to the 95th percentile.\n- Use the fitted logistic model to compute:\n  - the linear predictor (η),\n  - its standard error,\n  - a 95 % confidence interval for η,\n  - and then transform these to probabilities using the inverse logit.\n\"\"\"\n\nimport patsy\n\n# Reference profile\nage_ref = 60\nsex_ref = \"F\"  # adjust if coding is different\n\n# BMI grid\nbmi_grid = np.linspace(\n    df[\"BMI\"].quantile(0.05),\n    df[\"BMI\"].quantile(0.95),\n    100\n)\n\npred_df = pd.DataFrame({\n    \"BMI\": bmi_grid,\n    \"age\": age_ref,\n    \"sex\": sex_ref,\n})\n\n# Build the design matrix for the new data using the same formula structure\nformula = \"hypertension ~ BMI + age + C(sex)\"\ndesign_info = result_log.model.data.design_info\nX_new = patsy.build_design_matrices([design_info], pred_df)[0]\n\n# Parameter estimates and covariance matrix\nbeta_hat = result_log.params.values\ncov_beta = result_log.cov_params().values\n\n# Linear predictor η = Xβ\neta_hat = X_new @ beta_hat\n\n# Standard error of η: sqrt(diag(X cov_beta X^T))\nvar_eta = np.einsum(\"ij,jk,ik-&gt;i\", X_new, cov_beta, X_new)\nse_eta = np.sqrt(var_eta)\n\n# 95 % CI on the η (log-odds) scale\nz = 1.96\neta_lower = eta_hat - z * se_eta\neta_upper = eta_hat + z * se_eta\n\n# Inverse logit to get probabilities and CIs\ndef inv_logit(x):\n    return np.exp(x) / (1 + np.exp(x))\n\np_hat = inv_logit(eta_hat)\np_lower = inv_logit(eta_lower)\np_upper = inv_logit(eta_upper)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(bmi_grid, p_hat, linewidth=2, label=\"Predicted probability\")\nax.fill_between(bmi_grid, p_lower, p_upper, alpha=0.2, label=\"95 % CI\")\n\nax.set_xlabel(\"Body mass index (kg/m²)\")\nax.set_ylabel(\"Predicted probability of hypertension\")\nax.set_title(f\"Hypertension vs BMI (age = {age_ref}, sex = {sex_ref})\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\nYou can change age and sex in the function to get different predictions.\n\n\n\"\"\"Predicted probability of hypertension across BMI for different ages.\n\nWe:\n\n- Choose a small set of ages (for example, 40, 60, 80 years).\n- For each age, vary BMI over the same grid.\n- Compute predicted probabilities from the logistic model.\n- Plot the curves on the same graph.\n\nThis illustrates how age shifts the entire BMI–hypertension curve.\n\"\"\"\n\nages_to_show = [40, 60, 80]  # adjust as desired\nsex_ref = \"F\"                # keep sex fixed\n\nbmi_grid = np.linspace(\n    df[\"BMI\"].quantile(0.05),\n    df[\"BMI\"].quantile(0.95),\n    100\n)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nfor a in ages_to_show:\n    pred_df = pd.DataFrame({\n        \"BMI\": bmi_grid,\n        \"age\": a,\n        \"sex\": sex_ref,\n    })\n    # Use the fitted logistic model directly for probabilities\n    p_hyp = result_log.predict(pred_df)\n    ax.plot(bmi_grid, p_hyp, linewidth=2, label=f\"Age {a} years\")\n\nax.set_xlabel(\"Body mass index (kg/m²)\")\nax.set_ylabel(\"Predicted probability of hypertension\")\nax.set_title(f\"Hypertension vs BMI for different ages (sex = {sex_ref})\")\nax.legend(title=\"Profile\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n2.8 Risk ratios (RR) and their relation to odds ratios (OR)\nIn prospective cohort studies the most common effect measure is the risk ratio (RR):\n\\[\nRR = \\frac{P(Y = 1 \\mid X = 1)}{P(Y = 1 \\mid X = 0)}.\n\\]\nThe RR compares probabilities, and is directly interpretable as a multiplicative change in risk.\nLogistic regression, however, models the odds:\n\\[\n\\text{odds} = \\frac{p}{1-p},\n\\qquad\nOR = \\frac{\\text{odds}_1}{\\text{odds}_0}.\n\\]\nThe OR and RR differ because the odds and the probability are not the same.\nTwo important points:\n\nWhen the outcome is rare (for example, prevalence below about 10 %),\n\\[\nOR \\approx RR.\n\\] In this setting, logistic regression produces estimates that can be interpreted approximately as risk ratios.\nWhen the outcome is common, the OR can be noticeably larger than the RR (sometimes substantially so). This is not an error: it reflects the mathematical behaviour of the odds.\n\nIn prospective cohort studies the RR is often preferable because it is easier to interpret—“a 30 % higher risk” is more intuitive than “a 30 % higher odds”. Logistic regression cannot estimate RRs directly because of its logit link function, but alternative models exist:\n\nlog-binomial regression (models log risk; can have convergence issues);\nPoisson regression with robust variance (commonly used and usually stable).\n\nIn this workbook we focus on logistic regression, but the distinction between OR and RR is important when interpreting results, especially when the outcome is common.\n\n\n2.9 Visualising the difference between odds ratios and risk ratios\nTo see how odds ratios (OR) and risk ratios (RR) differ in practice, we can look at what happens for different baseline risks.\nFor a given baseline risk $ $p_0 $ in the unexposed group and a chosen odds ratio (OR), we can compute:\n\nthe risk $ p_1 $ in the exposed group implied by that OR, and\n\nthe corresponding risk ratio $ RR = p_1 / p_0 $.\n\nIf the outcome is rare (small $p_0 $), OR and RR are very similar.\nWhen the outcome is common (large $p_0 $), OR and RR diverge.\nThe following plot shows RR as a function of OR for three different baseline risks.\n\n\"\"\"Plot RR vs OR for different baseline risks.\n\nWe:\n\n- Choose three baseline risks p0 (e.g. 5 %, 20 %, 50 %).\n- For a range of odds ratios (OR), compute the implied risk ratio (RR).\n- Plot RR against OR, with one curve per baseline risk.\n\nThis illustrates that OR ≈ RR when the outcome is rare, but OR increasingly\noverstates the RR when the outcome is common.\n\"\"\"\n\n\n# Baseline risks in the unexposed group\np0_values = [0.05, 0.20, 0.50]\n\n# Range of odds ratios to consider\nor_grid = np.linspace(1.0, 5.0, 100)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nfor p0 in p0_values:\n    # Odds in unexposed group\n    odds0 = p0 / (1 - p0)\n\n    # For each OR, compute odds1, p1, and RR\n    odds1 = or_grid * odds0\n    p1 = odds1 / (1 + odds1)\n    rr = p1 / p0\n\n    ax.plot(or_grid, rr, linewidth=2, label=f\"Baseline risk p0 = {p0:.2f}\")\n\nax.plot([1, 5], [1, 5], linestyle=\"--\", linewidth=1, label=\"Line RR = OR\")\n\nax.set_xlabel(\"Odds ratio (OR)\")\nax.set_ylabel(\"Risk ratio (RR)\")\nax.set_title(\"Relationship between OR and RR for different baseline risks\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n2.9.1 Interpreting the OR–RR plot\nThe dashed line shows where OR and RR would be equal.\n\nFor rare outcomes (baseline risk ( p_0 = 0.05 )), the curve lies very close to the line RR = OR. In this situation the odds ratio approximates the risk ratio well.\nFor more common outcomes (for example, ( p_0 = 0.20 ) or ( p_0 = 0.50 )), the curves lie increasingly above the line RR = OR. For the same OR, the RR is noticeably smaller.\n\nThis explains why, in prospective studies with common outcomes, odds ratios can give the impression of larger effects than risk ratios, even when both are correctly calculated.\n\n\n\n2.10 2.3 Cox proportional hazards regression\nIn many epidemiological studies we are interested in time-to-event outcomes, for example time to incident cardiovascular disease. Cox proportional hazards regression models the hazard—the instantaneous event rate at time ( t )—as:\n$ h(t X_1, , X_p) = h_0(t),(_1 X_1 + + _p X_p), $\nwhere:\n\n$ h_0(t) $ is the baseline hazard, left unspecified, and\n\n$ (_j) $ are hazard ratios comparing individuals who differ by one unit in $ X_j $, holding other variables constant.\n\nA hazard ratio $ (_j) &gt; 1 $ indicates a higher instantaneous rate of the event associated with higher $ X_j $.\n\n2.10.1 The proportional hazards assumption\nThe key assumption of the Cox model is that the hazard ratios are constant over time. Formally, for two individuals with covariates $ X $ and $ X’ $:\n\\[\n\\frac{h(t \\mid X)}{h(t \\mid X')} =\n    \\exp\\big( \\beta (X - X') \\big),\n\\]\nand this ratio does not depend on $ t $.\nThis means that:\n\nthe covariates shift the hazard multiplicatively,\n\nthe effect size is the same at all follow-up times, and\n\nthe shapes of the survival curves may differ, but their ratio on the log-hazard scale is parallel.\n\nViolation of this assumption (for example, if the effect of age or smoking weakens over time) requires either time-varying coefficients or an alternative modelling approach.\nIn later sections we examine diagnostic tools, such as Schoenfeld residuals, to assess the validity of the proportional hazards assumption.\n\n\n\n2.11 Understanding censoring in time-to-event data\nTime-to-event outcomes are rarely observed for all participants. Some individuals:\n\nhave not yet experienced the event by the end of follow-up,\nare lost to follow-up,\nwithdraw from the study, or\ndie from another cause before the event of interest occurs.\n\nIn these situations we do not know the true event time, only that it is later than the last time at which the participant was observed. This is called right-censoring.\nFormally, for each participant we record:\n\nthe observed time $ T = (T^, C) \\(, where\\) T^ $ is the true event time and $ C $ is the censoring time,\nan indicator $ = 1 $ if the event occurred and $ = 0 $ if the observation was censored.\n\nCensoring is acceptable for Cox regression provided it is non-informative, meaning that the mechanism determining censoring is unrelated to the participant’s underlying risk of the event. For example, end-of-study censoring is typically non-informative, whereas dropping out due to worsening illness may not be.\nCensoring is a central feature of survival analysis and must be handled explicitly in statistical models such as the Cox proportional hazards model.\n\n\n2.12 Visualising censoring at the individual level\nCensoring is often easier to understand if we look at individual follow-up histories.\nIn the plot below each horizontal line represents a single participant:\n\nThe left end of the line is the time when the participant enters the risk set.\n\nThe right end of the line is the time when observation stops (either because the event occurs or because follow-up ends for another reason).\n\nWe distinguish:\n\nEvent: the line ends with an event marker.\n\nRight censoring: the line ends with a censoring marker (follow-up stops before the event is observed).\n\nLeft censoring (or more commonly in cohorts, left truncation / delayed entry): the line starts after time 0, indicating that the participant only becomes observable from that time onwards.\n\nThe example is based on artificial data and is intended purely to illustrate different types of observation patterns that arise in time-to-event analyses.\n\n\n\nCensoring\n\n\n\n\n2.13 Prepare time-to-event variables for Cox regression\nIn order to analyse the data, we need to know the time-to-event. Our dataset has event-dates, so they need to be converted:\nWe have:\n\n‘baseline_date’: date of baseline assessment.\n‘CVD_date’: date of incident cardiovascular disease (NaT if no event).\n‘CVD_incident’: 1 if incident CVD occurred during follow-up, 0 otherwise.\n\nWe construct:\n\n‘event_cvd’ : event indicator for Cox (1 = event, 0 = censored).\n‘time_cvd’ : follow-up time in years from baseline to event or censoring.\n\nFor simplicity, we use a common censoring date equal to the latest of all observed CVD events or baseline dates. In a real analysis this would usually be the study end date.\n\nThe common censoring date is usually the end of follow-up\n\n\n# Ensure date variables are in datetime format\ndf[\"baseline_date\"] = pd.to_datetime(df[\"baseline_date\"])\ndf[\"CVD_date\"] = pd.to_datetime(df[\"CVD_date\"])\n\n# Event indicator for Cox model\ndf[\"event_cvd\"] = df[\"CVD_incident\"].astype(int)\n\n# Choose a global censoring date (here: latest date observed in the cohort)\nglobal_censor_date = pd.concat(\n    [df[\"CVD_date\"].dropna(), df[\"baseline_date\"].dropna()]\n).max()\n\nprint(\"Global censoring date:\", global_censor_date.date())\n\n# Time to event for those with CVD; NaN if no event\nevent_times = (df[\"CVD_date\"] - df[\"baseline_date\"]).dt.days / 365.25\n\n# Time to censoring for those without CVD\ncensor_times = (global_censor_date - df[\"baseline_date\"]).dt.days / 365.25\n\n# Combine into a single follow-up time variable\ndf[\"time_cvd\"] = np.where(df[\"event_cvd\"] == 1, event_times, censor_times)\n\n# Quick sanity check\nprint(df[[\"time_cvd\", \"event_cvd\"]].describe(include=\"all\"))\n\n\n\n2.14 Kaplan–Meier curves and censoring\nIn time-to-event analysis we often describe the data using a Kaplan–Meier survival curve. For each time ( t ), the Kaplan–Meier estimator ( (t) ) gives the estimated probability of remaining free of the event up to time ( t ).\nUsing the constructed variables:\n\ntime_cvd — follow-up time (in years) to CVD or censoring,\n\nevent_cvd — event indicator (1 = CVD event, 0 = censored),\n\nwe can estimate the survival function for incident cardiovascular disease.\nGraphically:\n\nThe stepwise curve shows the estimated survival probability over time.\n\nDownward steps occur whenever CVD events take place.\n\nCensoring marks (small vertical ticks) indicate participants who were censored at that point in time (for example, end of follow-up or loss to follow-up).\n\nThe Kaplan–Meier method is non-parametric: it does not assume any particular shape for the hazard or survival curve. It does, however, rely on the assumption of non-informative censoring — that the mechanism causing censoring is unrelated to the participant’s underlying risk of CVD.\nIn the next step we compute and plot the Kaplan–Meier curve for the FB2NEP cohort.\n\n# Prepare dataset (drop missing values just in case)\ndf_surv = df[[\"time_cvd\", \"event_cvd\"]].dropna().copy()\n\n# Basic summary for the students\nprint(\"Number of observations:\", len(df_surv))\nprint(\"Event count:\", int(df_surv[\"event_cvd\"].sum()))\nprint(\"Censored:\", int((1 - df_surv[\"event_cvd\"]).sum()))\n\n# Fit KM curve\nkmf = KaplanMeierFitter()\nkmf.fit(\n    durations=df_surv[\"time_cvd\"],\n    event_observed=df_surv[\"event_cvd\"],\n    label=\"CVD-free survival\"\n)\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 4))\n\nkmf.plot_survival_function(ax=ax, ci_show=True)\n\nax.set_xlabel(\"Follow-up time (years)\")\nax.set_ylabel(\"Estimated survival probability $\\\\hat{S}(t)$\")\nax.set_title(\"Kaplan–Meier curve: time to incident CVD\")\n\n# Keep 0–1 range clear\nax.set_ylim(0, 1.05)\n\nplt.tight_layout()\nplt.show()\n\n\n\n2.15 From Kaplan–Meier curves to Cox regression\nThe Kaplan–Meier curve provides a descriptive, non-parametric summary of time to incident CVD. It shows how the probability of remaining event free changes over time, but it has two important limitations:\n\nIt usually compares only one factor at a time (for example, survival by sex or by exposure group).\nIt does not adjust for other covariates (such as age, BMI, smoking, or blood pressure) that may confound the association.\n\nIn practice, epidemiological studies almost always need to:\n\nestimate the association between several predictors and time to event simultaneously,\n\nobtain adjusted effect estimates (for example, “hazard ratio per 5 kg/m² higher BMI, adjusted for age and sex”), and\n\ntest whether these associations are compatible with the proportional hazards assumption.\n\nThe Cox proportional hazards model addresses these needs by modelling the hazard as\n\\[\nh(t \\mid X) = h_0(t)\\,\\exp(\\beta_1 X_1 + \\cdots + \\beta_p X_p),\n\\]\nwhere $ h_0(t) $ is an unspecified baseline hazard and the exponentiated coefficients $ (_j) $ are hazard ratios. In the next step we fit such a model to the FB2NEP cohort using age, BMI and sex as predictors.\n\n\"\"\"Cox regression example: time to CVD event.\n\nWe assume the dataset contains:\n\n- 'time_cvd': follow-up time (for example, in years).\n- 'event_cvd': event indicator (1 if event occurred, 0 if censored).\n- 'age', 'sex', 'bmi' as predictors.\n\nWe use the `lifelines` package for Cox regression.\n\"\"\"\n\n\n\n# Select relevant columns and drop missing values.\ncols = [\"time_cvd\", \"event_cvd\", \"age\", \"BMI\", \"sex\"]\ndf_cox = df[cols].dropna().copy()\n\n# Lifelines expects categorical variables to be encoded appropriately.\n# Here we create a simple indicator for sex == 'Female' as an example.\ndf_cox[\"sex_female\"] = (df_cox[\"sex\"] == \"F\").astype(int)\n\ncph = CoxPHFitter()\ncph.fit(df_cox[[\"time_cvd\", \"event_cvd\", \"age\", \"BMI\", \"sex_female\"]],\n        duration_col=\"time_cvd\",\n        event_col=\"event_cvd\")\n\nsummary_df = cph.summary\nsummary_df\n\n\n\n2.16 Interpreting the Cox regression output\nThe Cox model summary shows, for each predictor:\n\nthe estimated log hazard ratio (coef),\nthe hazard ratio itself (exp(coef)),\nstandard errors and confidence intervals,\ntests of whether the coefficient differs from zero.\n\nFor example, a hazard ratio of 1.10 for BMI (per 1 kg/m²) would mean a 10 % higher instantaneous rate of CVD per unit increase in BMI, assuming the proportional hazards assumption holds.\n\n\n2.17 Survival curves for specific risk profiles\nThe Kaplan–Meier estimator provides a descriptive, unadjusted view of the overall survival experience in the cohort. However, it does not allow us to compare adjusted survival patterns for individuals who differ in specific covariates.\nOnce a Cox proportional hazards model has been fitted, we can obtain model-based survival curves for hypothetical individuals. These curves show:\n\nhow the predicted survival probability changes over time for a given set of covariates,\n\nhow two individuals with different risk factor profiles (for example, higher vs lower BMI) are expected to differ, holding all other variables constant, and\n\nhow the Cox model translates hazard ratios into differences in survival over time.\n\nThese curves are not non-parametric estimates; they are conditional survival functions derived from the fitted Cox model:\n\\[\n\\hat{S}(t \\mid X) = \\hat{S}_0(t)^{\\exp(\\beta X)},\n\\]\nwhere $ _0(t) $ is the estimated baseline survival curve and\n$ (X) $ is the relative hazard for the specified covariate pattern.\nIn the next step we compute survival curves for two contrasting profiles (for example, a lower-BMI and a higher-BMI individual) to illustrate how covariate differences influence predicted survival.\n\n\n\n# Define two example profiles.\nprofile_low = {\n    \"age\": 60,\n    \"BMI\": 24,\n    \"sex_female\": 1,\n}\n\nprofile_high = {\n    \"age\": 60,\n    \"BMI\": 32,\n    \"sex_female\": 1,\n}\n\nprofiles = pd.DataFrame([profile_low, profile_high])\nprofiles.index = [\"BMI 24\", \"BMI 32\"]\n\nsurv = cph.predict_survival_function(profiles)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nfor label in surv.columns:\n    ax.plot(surv.index, surv[label], label=label)\n\nax.set_xlabel(\"Follow-up time\")\nax.set_ylabel(\"Estimated survival probability\")\nax.set_title(\"Cox model: example survival curves\")\nax.legend()\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#using-quantile-based-categories-in-regression",
    "href": "notebooks/1.08_regression_modelling_01.html#using-quantile-based-categories-in-regression",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "3 3. Using quantile-based categories in regression",
    "text": "3 3. Using quantile-based categories in regression\nA common approach in nutritional epidemiology is to convert a continuous exposure (for example, flavanol intake, dietary fibre, plasma biomarkers) into quantile-based categories, such as tertiles, quartiles, or quintiles.\nThis creates ordered categories with approximately equal numbers of participants in each group. Analysts then use these categories as predictors in regression models.\n\n3.1 3.1 Creating quantile categories\nLet $ X $ be the continuous exposure of interest. We divide its distribution into $ K $ groups (for example, $ K = 5 $ for quintiles). Each participant is assigned a category $ Q {1, , K} $.\nIn analysis this categorical variable enters the regression through dummy variables, treating the lowest group (usually Q1) as the reference category.\nFor example, in a linear model:\n\\[\nY = \\beta_0 + \\beta_2 I(Q = 2) + \\beta_3 I(Q = 3) +\n      \\beta_4 I(Q = 4) + \\beta_5 I(Q = 5)\n      + \\gamma_1 Z_1 + \\cdots + \\gamma_p Z_p,\n\\]\nwhere $ Z_1, , Z_p $ are adjustment variables.\nThe coefficients $ _2, , _5 $ represent differences in the mean outcome in each quantile compared with the reference quantile Q1.\nThe same structure applies in logistic regression (as odds ratios) or Cox regression (as hazard ratios).\n\n\n3.2 3.2 Test for trend across quantiles\nBecause quantile groups have a natural order (Q1 &lt; Q2 &lt; Q3 &lt; Q4 &lt; Q5), we can formally test whether the outcome increases or decreases across the quantiles.\nThe usual approach is:\n\nAssign each quantile group an integer score (1, 2, 3, 4, 5).\n\nFit a regression model including this score as a continuous variable.\n\nFor example:\n\\[\nY = \\alpha_0 + \\alpha_1 \\, \\text{quantile\\_score} + \\gamma^\\top Z,\n\\]\nor in a logistic model:\n\\[\n\\log\\left(\\frac{p}{1 - p}\\right)\n  = \\alpha_0 + \\alpha_1 \\, \\text{quantile\\_score} + \\gamma^\\top Z.\n\\]\nThe coefficient $ _1 $ provides a test for linear trend across the quantile categories (“P for trend”).\nThis trend model does not replace the categorical model; both are usually presented:\n\nthe categorical model shows non-linear patterns across quantiles;\n\nthe trend model tests whether there is an overall monotonic association.\n\n\n\n3.3 3.3 Why quantiles are used\nQuantile-based approaches are often used because they:\n\nreduce the impact of extreme values,\n\nallow for simple categorical comparisons (“Q5 vs Q1”),\n\ncan highlight non-linear relationships,\n\nmatch the conventions of major epidemiological cohorts (EPIC, NHANES, UK Biobank).\n\nHowever, quantile categorisation also loses some information compared with modelling the exposure as a continuous variable.\nIn later workbooks we will compare:\n\ncontinuous models,\n\ncategorised models, and\n\nspline-based models."
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#example-continuous-vs-quintile-based-exposure",
    "href": "notebooks/1.08_regression_modelling_01.html#example-continuous-vs-quintile-based-exposure",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "4 3.4 Example: continuous vs quintile-based exposure",
    "text": "4 3.4 Example: continuous vs quintile-based exposure\nIn this example we compare two approaches for modelling the association between fruit and vegetable intake (fruit_veg_g_d, grams per day) and BMI:\n\nA continuous model: BMI regressed directly on fruit_veg_g_d.\n\nA categorical model: BMI regressed on quintiles of fruit_veg_g_d (Q1 to Q5), using Q1 as the reference group.\n\nSteps:\n\nCreate quintiles of fruit_veg_g_d using pandas.qcut.\n\nFit both models using ordinary least squares.\n\nCompare the estimated associations.\n\nVisualise the relationship using a scatter plot with a fitted regression line.\n\n\n\"\"\"Create quintiles of fruit and vegetable intake.\n\nWe:\n\n- Restrict to participants with non-missing BMI and fruit_veg_g_d.\n- Use pandas.qcut to divide fruit_veg_g_d into 5 equally sized groups (quintiles).\n- Create:\n  - an integer quintile code (1–5),\n  - a categorical label (\"Q1\"–\"Q5\") for use in regression output.\n\"\"\"\n\n\n# Subset to relevant variables and drop missing values\ndf_q = df[[\"BMI\", \"fruit_veg_g_d\"]].dropna().copy()\n\n# Construct quintiles: qcut tries to place approximately equal numbers in each group\ndf_q[\"fv_quintile\"] = pd.qcut(\n    df_q[\"fruit_veg_g_d\"],\n    q=5,\n    labels=[1, 2, 3, 4, 5]\n)\n\n# Also create a categorical version with labels \"Q1\"–\"Q5\" for clearer output\ndf_q[\"fv_quintile_cat\"] = pd.qcut(\n    df_q[\"fruit_veg_g_d\"],\n    q=5,\n    labels=[\"Q1 (lowest)\", \"Q2\", \"Q3\", \"Q4\", \"Q5 (highest)\"]\n)\n\ndf_q[[\"fruit_veg_g_d\", \"fv_quintile\", \"fv_quintile_cat\"]].head()\n\nAfter we have created quintiles, we can now conduct regression analyses.\n\n\"\"\"Compare continuous and quintile-based linear regression models.\n\nModel 1 (continuous):\n    BMI ~ fruit_veg_g_d\n\nModel 2 (quintiles, reference = lowest quintile):\n    BMI ~ C(fv_quintile_cat)\n\nWe display the full summaries and then a compact table for the quintile model.\n\"\"\"\n\n# Continuous model\nmodel_cont = smf.ols(\"BMI ~ fruit_veg_g_d\", data=df_q)\nresult_cont = model_cont.fit()\n\nprint(\"Continuous model: BMI ~ fruit_veg_g_d\")\ndisplay(result_cont.summary())\n\n\n\n\n\n\n4.0.1 Interpreting the continuous regression model\nThis model examines the association between fruit and vegetable intake (fruit_veg_g_d, grams per day) and BMI as a continuous relationship.\nKey points:\n\nThe coefficient for fruit_veg_g_d is very small (0.0005) and not statistically significant (P = 0.121).\nThis means that, within the FB2NEP cohort, higher fruit and vegetable intake is\nnot strongly associated with differences in BMI when modelled linearly.\nThe confidence interval (–0.000, 0.001) includes zero, which is consistent with the non-significant result.\nThe ( R^2 ) is effectively zero, indicating that fruit and vegetable intake explains almost none of the variability in BMI.\nThe intercept (27.02 kg/m²) represents the expected BMI when intake is zero, but is mainly a baseline reference point rather than a meaningful real-world value.\n\nThis illustrates a common situation in nutritional epidemiology:\na weak or flat association when using a simple linear model on the continuous exposure. In the next section we explore an alternative approach using quintiles, which can sometimes reveal non-linear patterns that a straight-line model does not capture.\n\n# Quintile model (fv_quintile_cat as a categorical predictor, Q1 is the reference)\nmodel_quint = smf.ols(\"BMI ~ C(fv_quintile_cat)\", data=df_q)\nresult_quint = model_quint.fit()\n\nprint(\"\\nQuintile model: BMI ~ C(fv_quintile_cat)\")\ndisplay(result_quint.summary())\n\n\n# Extract a compact coefficient table for the quintile model\ncoef_quint = result_quint.params\nci_quint = result_quint.conf_int()\ntable_quint = pd.DataFrame({\n    \"coef\": coef_quint,\n    \"CI_lower\": ci_quint[0],\n    \"CI_upper\": ci_quint[1],\n})\n\nprint(\"\\nQuintile model: estimated differences in mean BMI vs reference (Q1)\")\ntable_quint\n\n\n\n4.0.2 Interpreting the quintile-based model\nHere we compare mean BMI across quintiles of fruit and vegetable intake, with the lowest intake group (Q1) as the reference category.\nKey points:\n\nAll estimated differences (Q2–Q5 vs Q1) are small (around 0.1–0.17 BMI units).\nSuch differences are unlikely to be meaningful in practice.\nAll confidence intervals cross zero, indicating no clear evidence that mean BMI differs between fruit/vegetable intake groups.\nThe pattern across quintiles does not show a monotonic increase or decrease: estimated BMI is slightly higher in Q2–Q5, but the values are similar and imprecise.\nThe intercept (27.0 kg/m²) is the estimated mean BMI in the reference group (Q1).\n\nTaken together, this quintile model gives the same substantive conclusion as the continuous model: in the FB2NEP cohort, fruit and vegetable intake is not strongly or systematically associated with BMI. Categorising the exposure does not reveal hidden non-linear patterns in this case.\n\n\"\"\"Scatter plot of BMI vs fruit and vegetable intake, with quintiles.\n\nWe:\n\n- Plot individual participants (BMI vs fruit_veg_g_d).\n- Overlay the fitted regression line from the continuous model.\n- Add vertical lines at the quintile cut-points of fruit_veg_g_d.\n- Add one point per quintile showing the mean BMI and mean intake.\n\nThis links the continuous and quintile-based views of the same association.\n\"\"\"\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\n# Scatter plot of observed data\nax.scatter(\n    df_q[\"fruit_veg_g_d\"],\n    df_q[\"BMI\"],\n    alpha=0.3,\n    edgecolor=\"none\",\n    label=\"Observed BMI\"\n)\n\n# Fitted line from the continuous model\nfv_grid = np.linspace(\n    df_q[\"fruit_veg_g_d\"].min(),\n    df_q[\"fruit_veg_g_d\"].max(),\n    100\n)\npred_df = pd.DataFrame({\"fruit_veg_g_d\": fv_grid})\npred_df[\"BMI_hat\"] = result_cont.predict(pred_df)\n\nax.plot(\n    pred_df[\"fruit_veg_g_d\"],\n    pred_df[\"BMI_hat\"],\n    linewidth=2,\n    label=\"Fitted line (continuous model)\"\n)\n\n# Quintile boundaries (excluding min/max)\nquintile_probs = [0.2, 0.4, 0.6, 0.8]\nquintile_cuts = df_q[\"fruit_veg_g_d\"].quantile(quintile_probs)\n\nfor q_val in quintile_cuts:\n    ax.axvline(q_val, linestyle=\"--\", linewidth=1)\n    \n# Add text labels for quintile regions (optional, simple version)\n# for i, (p, v) in enumerate(quintile_cuts.items(), start=1):\n#     ax.text(v, ax.get_ylim()[1], f\"Q{i+1}\", ha=\"center\", va=\"top\")\n\n# Mean BMI and mean intake within each quintile\nquintile_means = (\n    df_q\n    .groupby(\"fv_quintile\")\n    .agg(\n        mean_fv=(\"fruit_veg_g_d\", \"mean\"),\n        mean_BMI=(\"BMI\", \"mean\"),\n    )\n    .reset_index()\n)\n\nax.scatter(\n    quintile_means[\"mean_fv\"],\n    quintile_means[\"mean_BMI\"],\n    s=60,\n    marker=\"o\",\n    label=\"Quintile means\",\n)\n\nax.set_xlabel(\"Fruit and vegetable intake (g/day)\")\nax.set_ylabel(\"Body mass index (kg/m²)\")\nax.set_title(\"BMI vs fruit and vegetable intake\\nwith quintiles and fitted line\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n4.1 3.1 Strengths and limitations of quantile regression\nStrengths:\n\nProvides a more complete description of the conditional distribution of Y.\nRobust to outliers (especially when modelling the median).\nNaturally accommodates heteroscedasticity (non-constant variance).\n\nLimitations:\n\nInterpretation can be less intuitive than mean regression.\nConfidence intervals and hypothesis tests are more complex.\nMore demanding computationally (although not an issue for this workbook).\n\nIn nutritional epidemiology quantile regression can be particularly useful when:\n\nThe upper tail of a distribution is of special interest (for example, high sodium intake).\nThe outcome distribution is strongly skewed (for example, some biomarkers)."
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#assumptions-of-regression-models",
    "href": "notebooks/1.08_regression_modelling_01.html#assumptions-of-regression-models",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "5 4. Assumptions of regression models",
    "text": "5 4. Assumptions of regression models\nAll models are simplifications of reality. To interpret results sensibly we need to be aware of their assumptions.\nHere we briefly review key assumptions for:\n\nLinear regression.\nLogistic regression.\nCox proportional hazards regression.\n\nDiagnostics and practical illustrations follow in the next section.\n\n5.1 4.1 Linearity\nIn a standard linear regression model we assume that the relationship between each continuous predictor and the outcome is linear (after any transformations we choose).\nIf the true relationship is markedly non-linear, then:\n\nThe model may fit poorly.\nEstimates of effect may be biased.\nResidual plots may show systematic patterns.\n\nLater in this workbook we will introduce non-linear models (polynomials and splines) that relax this assumption.\n\n\n5.2 4.2 Independence\nWe usually assume that the residuals (errors) are independent between individuals.\nViolations of independence can occur when:\n\nThe same individual contributes multiple observations (for example, repeated measures).\nObservations are clustered (for example, participants from the same household or clinic).\n\nMore advanced methods, such as mixed models or cluster-robust standard errors, are used in those situations. Here we make the simplifying assumption of independence.\n\n\n5.3 4.3 Homoscedasticity\nHomoscedasticity means that the variance of the residuals is constant across levels of the predictors.\nIf residual variance increases or decreases with fitted values (heteroscedasticity), then:\n\nEstimates of standard errors may be biased.\nConfidence intervals and P-values may be unreliable.\n\nResidual-versus-fitted plots can be used to detect such patterns.\n\n\n5.4 4.4 Normality of residuals\nFor linear regression, we often assume that the residuals are approximately normally distributed.\n\nThis assumption is not necessary for obtaining unbiased estimates of the mean.\nIt matters mainly for inference (confidence intervals and tests) in small samples.\n\nNormality can be explored with Q–Q plots, which compare the distribution of residuals with a theoretical normal distribution.\n\n\n5.5 4.5 Multicollinearity\nMulticollinearity arises when predictors are strongly correlated with one another.\nConsequences:\n\nCoefficients may be unstable.\nStandard errors become large.\nIt can be difficult to disentangle separate effects.\n\nThe variance inflation factor (VIF) is a commonly used diagnostic: large VIF values indicate problematic collinearity.\n\n\n5.6 4.6 Separation in logistic regression\nIn logistic regression, separation occurs when a predictor (or combination of predictors) perfectly predicts the outcome (for example, all smokers have disease, all non-smokers are healthy).\nConsequences:\n\nMaximum likelihood estimates may not exist or may be extremely large.\nStandard logistic regression fails.\n\nIn practice one may:\n\nCollapse categories.\nUse penalised logistic regression.\nRethink the model structure.\n\n\n\n5.7 4.7 Proportional hazards in Cox regression\nThe Cox model assumes that hazard ratios are constant over time (proportional hazards). In other words:\n\\[\n\\frac{h(t \\mid X = 1)}{h(t \\mid X = 0)} = \\text{constant in } t.\n\\]\nViolations of this assumption can be detected using:\n\nPlots of log(-log(survival)) curves.\nSchoenfeld residuals and associated tests.\n\nIf proportional hazards does not hold, options include:\n\nStratified Cox models.\nTime-varying coefficients.\nAlternative modelling approaches."
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#model-diagnostics",
    "href": "notebooks/1.08_regression_modelling_01.html#model-diagnostics",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "6 5. Model diagnostics",
    "text": "6 5. Model diagnostics\nWe now illustrate a few standard diagnostic tools for regression models.\nThe aim is not to be exhaustive, but to provide a first hands-on experience with:\n\nResidual plots.\nQ–Q plots.\nInfluence diagnostics.\nGoodness-of-fit metrics.\n\n\n6.1 5.1 Residual plots\nA residual is the difference between the observed outcome and the value predicted by the model:\n[ e_i = y_i - _i. ]\nResidual plots help assess whether the assumptions of the linear regression model are reasonable. Two aspects are particularly important:\n\nLinearity: the mean of the residuals should be close to zero across the range of fitted values. A curved pattern suggests that the relationship between predictors and outcome may not be adequately captured by a simple linear model.\nHomoscedasticity (constant variance): the spread of residuals should be roughly constant. A “funnel shape” (residuals widening or narrowing with fitted values) suggests heteroscedasticity, which can affect standard errors and inference.\n\nA residual-versus-fitted plot is therefore a quick visual check of whether the linear model provides a reasonable description of the data.\n\n\"\"\"Residuals vs fitted values for a linear regression model.\n\nWe:\n\n- Extract fitted values and residuals from an existing OLS model (`result_lin`).\n- Create a scatter plot of residuals vs fitted values.\n- Add a horizontal reference line at 0.\n\nInterpretation:\n\n- If the linear model is appropriate and the variance is roughly constant,\n  the residuals should be scattered randomly around 0 with no clear pattern.\n- Curvature suggests non-linearity.\n- A “funnel” shape suggests heteroscedasticity (non-constant variance).\n\"\"\"\n\n\n\n# Replace `result_lin` with the name of your fitted OLS model if different.\nfitted = result_lin.fittedvalues\nresiduals = result_lin.resid\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.scatter(fitted, residuals, alpha=0.3, edgecolor=\"none\")\n\nax.axhline(0, linewidth=1)  # reference line at 0\n\nax.set_xlabel(\"Fitted values\")\nax.set_ylabel(\"Residuals\")\nax.set_title(\"Residuals vs fitted values\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n6.2 5.2 Q–Q plots (normal probability plots)\nLinear regression also assumes that the residuals are approximately normally distributed. This is not required for the model to produce unbiased estimates, but it is relevant for the validity of confidence intervals and ( p )-values.\nA Q–Q plot compares the distribution of the model residuals with a theoretical normal distribution:\n\nPoints lying close to the diagonal line indicate that the residuals are consistent with normality.\n\nSystematic deviations (for example, curves in the tails) suggest heavier or lighter tails than expected under a normal distribution.\n\nSevere deviations may indicate that transformations or alternative modelling approaches could be helpful.\n\nResidual plots and Q–Q plots together give an initial, practical assessment of whether the assumptions underlying ordinary least squares regression are acceptable for the analysis at hand.\n\n\"\"\"Normal Q–Q plot of residuals (using scipy).\n\nWe:\n\n- Take the residuals from the linear model (`result_lin`).\n- Use scipy.stats.probplot to compare them with a theoretical normal distribution.\n\nInterpretation:\n\n- Points close to the diagonal line indicate residuals that are approximately normal.\n- Systematic deviations (especially in the tails) suggest departures from normality.\n\"\"\"\n\n\n# Residuals as a 1D NumPy array\nresiduals = np.asarray(result_lin.resid)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nstats.probplot(residuals, dist=\"norm\", plot=ax)\nax.set_title(\"Normal Q–Q plot of residuals\")\n\nplt.tight_layout()\nplt.show()\n\n\n6.2.1 Interpreting the Q–Q plot\nThe Q–Q plot compares the ordered residuals from the linear model with the quantiles of a theoretical normal distribution. If the residuals were exactly normally distributed, all points would lie close to the diagonal reference line.\nIn this plot we observe:\n\nGood overall alignment with the diagonal:\nmost points fall close to the straight line, indicating that the residuals are broadly consistent with a normal distribution.\nDepartures in the lower tail (left side):\nthe most negative residuals lie below the reference line, suggesting that the lower tail of the residual distribution is slightly heavier (more extreme values) than expected under a normal distribution.\nMild curvature at both extremes:\nslight deviations at the top end indicate similar behaviour in the upper tail. These deviations are not dramatic, but they indicate that the residuals are not perfectly normal.\n\nOverall, the pattern is typical for observational data with a moderately skewed exposure distribution: the residuals are close enough to normal for standard inference to be reasonable, but the tails show mild departures that are worth noting.\nIn combination with the residual–versus–fitted plot, this Q–Q plot suggests that:\n\nthe linear model is adequate for illustrating the regression concepts,\n\nresidual normality is not perfect but not severely violated, and\n\nany refinements (for example, transformations or robust regression) would be minor and not essential for introductory analysis."
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#non-linear-models",
    "href": "notebooks/1.08_regression_modelling_01.html#non-linear-models",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "7 6. Non-linear models",
    "text": "7 6. Non-linear models\nThe term “linear regression” refers to linearity in the parameters (β), not necessarily in the predictors themselves.\nMany epidemiological relationships are non-linear. For example:\n\nBody mass index and mortality risk.\nAge and blood pressure.\nSodium intake and blood pressure.\n\nTo capture such patterns we can:\n\nAdd polynomial terms (for example, age²).\nUse splines, which fit smooth curves made of polynomial segments.\n\nIn this section we briefly introduce both approaches.\n\n7.1 6.1 Polynomial regression\nA simple extension of linear regression is to add powers of a predictor, for example:\n\\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\varepsilon.\n\\]\nThis is still a linear model in the parameters $ _0, _1, _2 $, but represents a curved relationship between X and Y.\nCaution is required:\n\nHigh-order polynomials can behave very erratically at the boundaries of the data.\nInterpretation of individual coefficients is difficult; the focus should be on the overall shape of the fitted curve.\n\n\n\"\"\"Polynomial regression example: BMI on age and age².\n\nWe:\n\n- Create a squared age term.\n- Fit a simple linear model: BMI ~ age.\n- Fit a polynomial model: BMI ~ age + age².\n- Plot both fitted curves together with the observed data.\n\"\"\"\n\n\n# Prepare data\ndf_poly = df[[\"age\", \"BMI\"]].dropna().copy()\ndf_poly[\"age2\"] = df_poly[\"age\"] ** 2\n\n# Simple linear model\nmodel_lin = smf.ols(\"BMI ~ age\", data=df_poly).fit()\n\n# Polynomial model with age and age²\nmodel_poly = smf.ols(\"BMI ~ age + age2\", data=df_poly).fit()\n\n# Grid of ages for predictions\nage_grid = np.linspace(df_poly[\"age\"].min(), df_poly[\"age\"].max(), 100)\npred_frame = pd.DataFrame({\n    \"age\": age_grid,\n    \"age2\": age_grid ** 2,\n})\n\n# Predictions from both models\npred_lin = model_lin.predict(pred_frame[[\"age\"]])\npred_poly = model_poly.predict(pred_frame)\n\n# Plot\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.scatter(\n    df_poly[\"age\"],\n    df_poly[\"BMI\"],\n    alpha=0.2,\n    edgecolor=\"none\",\n    label=\"Observed BMI\"\n)\n\nax.plot(age_grid, pred_lin, linewidth=2, label=\"Linear model\")\nax.plot(age_grid, pred_poly, linewidth=2, linestyle=\"--\", label=\"Polynomial (age + age²)\")\n\nax.set_xlabel(\"Age (years)\")\nax.set_ylabel(\"Body mass index (kg/m²)\")\nax.set_title(\"Polynomial regression: BMI ~ age + age²\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n7.2 6.2 Splines\nSplines provide a more flexible and stable approach to modelling non-linear relationships.\nIdea:\n\nThe range of X is divided into intervals by “knots”.\nWithin each interval we fit low-degree polynomials.\nThe pieces are joined smoothly at the knots.\n\nA widely used choice in epidemiology is the restricted cubic spline, which behaves linearly beyond the outer knots and smoothly between knots.\nAdvantages:\n\nFlexible yet stable.\nInterpretation focuses on the shape of the curve.\nWorks well in large cohorts.\n\n\n\"\"\"Restricted cubic spline example: BMI on age.\n\nWe:\n\n- Use a restricted cubic spline (4 df) for age in a linear model.\n- Fit the model using the formula interface.\n- Plot the fitted spline curve together with the observed data.\n\"\"\"\n\n\n# Subset with complete data\ndf_spline = df[[\"age\", \"BMI\"]].dropna().copy()\n\n# Fit OLS model with a restricted cubic spline for age (4 df)\nmodel_spline = smf.ols(\"BMI ~ cr(age, df=4)\", data=df_spline).fit()\n\n# Prediction grid for age\nage_grid = np.linspace(df_spline[\"age\"].min(), df_spline[\"age\"].max(), 100)\ngrid = pd.DataFrame({\"age\": age_grid})\n\n# Predicted BMI from the spline model\npred_spline = model_spline.predict(grid)\n\n# Plot observed data and spline fit\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.scatter(\n    df_spline[\"age\"],\n    df_spline[\"BMI\"],\n    alpha=0.2,\n    edgecolor=\"none\",\n    label=\"Observed BMI\",\n)\n\nax.plot(\n    age_grid,\n    pred_spline,\n    linewidth=2,\n    label=\"Spline fit (df = 4)\",\n)\n\nax.set_xlabel(\"Age (years)\")\nax.set_ylabel(\"Body mass index (kg/m²)\")\nax.set_title(\"Restricted cubic spline: BMI ~ age\")\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n7.3 6.3 Comparing models\nTo decide whether a non-linear term is useful we can compare models using:\n\nVisual inspection of fitted curves.\nGoodness-of-fit measures such as the Akaike information criterion (AIC).\nLikelihood ratio tests (for nested models).\n\nFor example, we can compare:\n\nA simple linear model (BMI ~ age).\nA polynomial model (BMI ~ age + age²).\nA spline model (BMI ~ spline(age)).\n\nLower AIC values indicate better trade-off between fit and complexity.\n\n\"\"\"Compare linear, polynomial, and spline models using AIC.\n\nThis is a simple numeric comparison; interpretation still relies on graphs and subject-matter knowledge.\n\"\"\"\n\naic_results = pd.DataFrame({\n    \"model\": [\"Linear\", \"Polynomial (age + age²)\", \"Spline (df = 4)\"],\n    \"AIC\": [model_lin.aic, model_poly.aic, model_spline.aic],\n})\n\naic_results"
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#interpreting-effect-estimates",
    "href": "notebooks/1.08_regression_modelling_01.html#interpreting-effect-estimates",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "8 7. Interpreting effect estimates",
    "text": "8 7. Interpreting effect estimates\nDifferent regression models produce different types of effect estimates. It is important to be clear about their meaning.\n\nβ (beta) coefficients in linear regression: expected difference in the mean outcome per unit change in the predictor.\nOdds ratios (OR) in logistic regression: multiplicative change in the odds of the outcome.\nRisk ratios (RR): multiplicative change in risk (probability); not directly estimated in standard logistic models.\nHazard ratios (HR) in Cox regression: multiplicative change in the instantaneous hazard.\n\nIn non-linear models (polynomials, splines, quantile regression) the interpretation usually focuses on the shape of the curve rather than individual coefficients.\n\n\"\"\"Extract and summarise effect estimates from the fitted models.\n\nWe:\n\n- Summarise β estimates from the linear model.\n- Present odds ratios from the logistic model.\n- Present hazard ratios from the Cox model.\n\nThis illustrates how different models report different effect measures.\n\"\"\"\n\n# Linear regression coefficients (bmi ~ age + C(sex)).\nbeta_lin = result_lin.params.to_frame(name=\"estimate\")\nbeta_lin[\"model\"] = \"Linear (BMI)\"\n\n# Logistic regression odds ratios (hypertension ~ bmi + age + C(sex)).\nparams_log = result_log.params\nconf_log = result_log.conf_int()\nor_table = pd.DataFrame({\n    \"estimate\": np.exp(params_log),\n    \"CI_lower\": np.exp(conf_log[0]),\n    \"CI_upper\": np.exp(conf_log[1]),\n})\nor_table[\"model\"] = \"Logistic (hypertension)\"\n\n# Cox model hazard ratios.\ncox_summary = cph.summary[[\"coef\", \"exp(coef)\", \"exp(coef) lower 95%\", \"exp(coef) upper 95%\"]].copy()\ncox_summary.rename(columns={\n    \"coef\": \"coef\",\n    \"exp(coef)\": \"HR\",\n    \"exp(coef) lower 95%\": \"CI_lower\",\n    \"exp(coef) upper 95%\": \"CI_upper\",\n}, inplace=True)\ncox_summary[\"model\"] = \"Cox (time to CVD)\"\n\ndisplay(beta_lin)\ndisplay(or_table)\ndisplay(cox_summary)"
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#estimation-and-inference-brief-overview",
    "href": "notebooks/1.08_regression_modelling_01.html#estimation-and-inference-brief-overview",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "9 8. Estimation and inference (brief overview)",
    "text": "9 8. Estimation and inference (brief overview)\nMost regression models in this workbook are estimated using maximum likelihood (or, in the case of ordinary least squares, a closely related approach).\nThe key ideas are:\n\nParameters are estimated by finding values that make the observed data “most likely” under the assumed model.\nStandard errors quantify the typical variation of estimates across hypothetical repeated samples.\nWald tests and likelihood ratio tests are used to assess whether coefficients differ from zero.\nConfidence intervals indicate a range of parameter values that are compatible with the observed data and the model assumptions.\n\nA full treatment of the underlying theory is beyond the scope of FB2NEP, but it is important to know that:\n\nEstimates are subject to sampling variability.\nP-values and confidence intervals rely on model assumptions.\n\n\n\"\"\"Manual computation of a confidence interval for a linear regression coefficient.\n\nWe illustrate the basic idea using the coefficient for 'age' in the linear model.\n\nThe 95 % confidence interval is:\n\n    estimate ± 1.96 * standard_error\n\nunder a normal approximation.\n\"\"\"\n\n# Extract estimate and standard error for 'age'.\nage_est = result_lin.params[\"age\"]\nage_se = result_lin.bse[\"age\"]\n\nci_lower = age_est - 1.96 * age_se\nci_upper = age_est + 1.96 * age_se\n\nprint(\"Coefficient for age (linear model):\", f\"{age_est:.3f}\")\nprint(\"Standard error:\", f\"{age_se:.3f}\")\nprint(\"Approximate 95 % CI:\", f\"[{ci_lower:.3f}, {ci_upper:.3f}]\")"
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#predictions-from-fitted-models",
    "href": "notebooks/1.08_regression_modelling_01.html#predictions-from-fitted-models",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "10 9. Predictions from fitted models",
    "text": "10 9. Predictions from fitted models\nOne of the most practical uses of regression models is to obtain predicted values for specified combinations of predictors.\nExamples:\n\nPredicted mean BMI at age 65 years in women.\nPredicted probability of hypertension at age 65 years for different BMI values.\nPredicted survival curves for different risk profiles.\n\nIn all cases it is important to remember:\n\nPredictions depend on the assumed model and its fitted parameters.\nUncertainty in predictions can be quantified (for example, by confidence intervals or prediction intervals).\n\n\n\"\"\"Prediction from a linear model: BMI at age 65.\n\nWe:\n\n- Create a small DataFrame with the desired predictor values.\n- Use the `predict` method of the fitted model.\n\nFor simplicity we focus on a single sex.\n\"\"\"\n\n# Example: predict BMI at age 65 for women.\nnew_data = pd.DataFrame({\n    \"age\": [65],\n    \"sex\": [\"F\"],\n})\n\npred_bmi = result_lin.predict(new_data)\n\nprint(\"Predicted mean BMI at age 65 (Female):\", float(pred_bmi.iloc[0]))"
  },
  {
    "objectID": "notebooks/1.08_regression_modelling_01.html#summary-and-further-reading",
    "href": "notebooks/1.08_regression_modelling_01.html#summary-and-further-reading",
    "title": "1.08 – Regression and Modelling: Foundations",
    "section": "11 10. Summary and further reading",
    "text": "11 10. Summary and further reading\nIn this workbook you have:\n\nReviewed the basic idea of regression as modelling conditional expectations.\nFitted and interpreted linear, logistic, and Cox proportional hazards models.\nSeen how quantile regression extends the idea to conditional quantiles.\nExamined key model assumptions and basic diagnostics.\nIntroduced non-linear models using polynomial terms and splines.\nObtained predictions from fitted models.\n\nThese tools are building blocks for more advanced topics in nutritional epidemiology:\n\nConfounding and adjustment.\nCausal diagrams (DAGs).\nMediation analysis.\nMissing data and more complex model structures.\n\nThese topics are developed further in Workbook 7.\nSuggested further reading:\n\nKleinbaum, D. G., and Klein, M. Logistic Regression: A Self-Learning Text.\nHarrell, F. E. Regression Modelling Strategies.\nRothman, K. J., Greenland, S., and Lash, T. L. Modern Epidemiology.\nKoenker, R. Quantile Regression."
  },
  {
    "objectID": "notebooks/1.03_introduction_colab_jupyter.html",
    "href": "notebooks/1.03_introduction_colab_jupyter.html",
    "title": "1.03 – Introduction to Colab and Jupyter",
    "section": "",
    "text": "FB2NEP – Nutritional Epidemiology and Public Health\nIn the epidemiology component of FB2NEP, we will use Jupyter notebooks (in particular Google Colab) as a practical environment to:\nThe purpose of this notebook is to give you a first overview of:\nYou do not need prior programming experience. The examples are small, explained line by line, and you will use them mainly as tools to understand epidemiological ideas.\n# ============================================================\n# FB2NEP bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the fb2nep-epi repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads and runs scripts/bootstrap.py.\n# - Makes the main dataset available as the variable `df`.\n#\n# Important:\n# - You may see messages printed below (for example from pip\n#   or from the bootstrap script). This is expected.\n# - You may also see WARNINGS (often in yellow). In most cases\n#   these are harmless and can be ignored for this module.\n# - The main thing to watch for is a red error traceback\n#   (for example FileNotFoundError, ModuleNotFoundError).\n#   If that happens, please re-run this cell first. If the\n#   error persists, ask for help.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\nimport importlib.util\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\n# REPO_URL: address of the GitHub repository.\n# REPO_DIR: folder name that will be created when cloning.\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the fb2nep-epi repository\n# ------------------------------------------------------------\n# In local Jupyter, you may already be inside the repository,\n# for example in fb2nep-epi/notebooks.\n#\n# In Colab, the default working directory is /content, so\n# we need to clone the repository into /content/fb2nep-epi\n# and then change into that folder.\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/bootstrap.py exists here)\nif (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n    repo_root = cwd\n\n# Case B: we are outside the repository (for example in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if it is not present yet\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change the working directory to the repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nprint(f\"Repository root set to: {repo_root}\")\n\n# ------------------------------------------------------------\n# 2. Load scripts/bootstrap.py as a module and call init()\n# ------------------------------------------------------------\n# The shared bootstrap script contains all logic to:\n# - Ensure that required Python packages are installed.\n# - Ensure that the synthetic dataset exists (and generate it\n#   if needed).\n# - Load the dataset into a pandas DataFrame.\n#\n# We load the script as a normal Python module (fb2nep_bootstrap)\n# and then call its init() function.\nbootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n\nif not bootstrap_path.is_file():\n    raise FileNotFoundError(\n        f\"Could not find {bootstrap_path}. \"\n        \"Please check that the fb2nep-epi repository structure is intact.\"\n    )\n\n# Create a module specification from the file\nspec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\nbootstrap = importlib.util.module_from_spec(spec)\nsys.modules[\"fb2nep_bootstrap\"] = bootstrap\n\n# Execute the bootstrap script in the context of this module\nspec.loader.exec_module(bootstrap)\n\n# The init() function is defined in scripts/bootstrap.py.\n# It returns:\n# - df   : the main synthetic cohort as a pandas DataFrame.\n# - CTX  : a small context object with paths, flags and settings.\ndf, CTX = bootstrap.init()\n\n# Optionally expose a few additional useful variables from the\n# bootstrap module (if they exist). These are not essential for\n# most analyses, but can be helpful for advanced use.\nfor name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n    if hasattr(bootstrap, name):\n        globals()[name] = getattr(bootstrap, name)\n\nprint(\"Bootstrap completed successfully.\")\nprint(\"The main dataset is available as the variable `df`.\")\nprint(\"The context object is available as `CTX`.\")"
  },
  {
    "objectID": "notebooks/1.03_introduction_colab_jupyter.html#where-are-the-notebooks-and-how-do-i-open-them-in-colab",
    "href": "notebooks/1.03_introduction_colab_jupyter.html#where-are-the-notebooks-and-how-do-i-open-them-in-colab",
    "title": "1.03 – Introduction to Colab and Jupyter",
    "section": "1 1. Where are the notebooks and how do I open them in Colab?",
    "text": "1 1. Where are the notebooks and how do I open them in Colab?\nAll teaching notebooks for FB2NEP live in a read-only GitHub repository:\n\nGitHub repository:\nhttps://github.com/ggkuhnle/fb2nep-epi\nPublished site (easier browsing):\nhttps://ggkuhnle.github.io/fb2nep-epi/\n\nYou will usually access notebooks via the published site. For each notebook there is a link or badge labelled something like “Open in Colab”.\nTypical workflow during the module:\n\nGo to the published site and navigate to the notebook for the week.\nClick the “Open in Colab” link or badge.\nColab will open the notebook in your browser.\nAt the top of the notebook Colab may display a warning such as: &gt; This notebook was not authored by Google.\nThis is a standard message. In the context of this module it simply means that the notebook comes from the course repository, not from Google. You can safely choose “Run anyway” for FB2NEP notebooks.\nOnce the notebook is open in Colab, use File → Save a copy in Drive to create your own copy. All your edits will then be stored in your Google Drive.\n\nThe original notebooks in GitHub remain unchanged. You cannot accidentally damage them. You work in your own copy."
  },
  {
    "objectID": "notebooks/1.03_introduction_colab_jupyter.html#notebook-basics-cells-and-markdown",
    "href": "notebooks/1.03_introduction_colab_jupyter.html#notebook-basics-cells-and-markdown",
    "title": "1.03 – Introduction to Colab and Jupyter",
    "section": "2 2. Notebook basics: cells and Markdown",
    "text": "2 2. Notebook basics: cells and Markdown\nA notebook consists of cells arranged from top to bottom.\n\nCode cells contain Python code and produce outputs such as numbers, tables, or plots.\nText cells (Markdown cells) contain formatted text for headings, lists, and explanations.\n\nTo run a code cell: 1. Click inside the cell. 2. Press Shift + Enter (or click the small play button on the left in Colab).\nThe output will appear directly below the cell.\nMarkdown is a light-weight mark-up language that controls basic formatting (headings, lists, bold, italics). In this module you only need a very small subset, and you can look it up when needed. A concise reference is available at:\n\nhttps://www.markdownguide.org/basic-syntax/\n\nIn the rest of this notebook we will focus on code cells and data handling.\n\n2.1 2.1 First code cell: a simple message\nRun the cell below. It prints a short message and demonstrates the basic code → output pattern.\nWhen you run a cell in Colab you may see one or both of the following:\n\nA yellow bar at the top saying something like\n“This notebook was not authored by Google” with a button Run anyway.\nFor FB2NEP notebooks this is expected: the code comes from the module GitHub repository, not from Google. Choose Run anyway to continue.\nExtra text in the output area such as\n“Connecting to kernel…” or “Setting up notebook environment”.\nThese are system messages from Colab, not errors. As long as the expected output (a short printed message) appears under the cell, the code has run successfully.\n\n\n# Run this cell (Shift + Enter)\nprint(\"Hello, FB2NEP\")\n\n\n2.1.1 Try it\n\nChange the text inside the quotation marks and run the cell again.\nAdd a second line, for example:\n\nprint(2 + 3)\nRun the cell again and observe that both lines of output appear under the cell."
  },
  {
    "objectID": "notebooks/1.03_introduction_colab_jupyter.html#a-very-brief-introduction-to-python",
    "href": "notebooks/1.03_introduction_colab_jupyter.html#a-very-brief-introduction-to-python",
    "title": "1.03 – Introduction to Colab and Jupyter",
    "section": "3 3. A very brief introduction to Python",
    "text": "3 3. A very brief introduction to Python\nThis section introduces three ideas that are useful throughout FB2NEP:\n\nWhat a Python programme is and why indentation matters.\nWhat libraries are and how to use them.\nVery basic programme structure: a condition (if) and a loop (for).\n\n\n3.1 3.1 Core Python data structures (lists, sets, dictionaries)\nBefore we look at programmes and libraries, it is useful to know the basic data structures that appear throughout the notebooks. These are the building blocks for more advanced tools such as pandas.\nWe will mainly use:\n\nList\nAn ordered collection of items. Lists can contain duplicates and can be changed.\nExample: a list of hippo names:\n[\"Helga\", \"Bruno\", \"Ama\"]\nSet\nAn unordered collection of unique items. Sets automatically remove duplicates.\nExample: the unique habitats in a list:\n{\"River\", \"Lake\", \"Zoo\"}\nDictionary (dict)\nA collection of key–value pairs. Each key maps to a value.\nExample: information about one hippo:\n{\"name\": \"Helga\", \"age_years\": 5, \"habitat\": \"River\"}\n\nThese are standard Python structures. In practice we often start from lists and dictionaries, and then build a pandas DataFrame from them.\n\n# Lists: ordered collections of items.\n\n# A list of hippo names.\nhippo_names = [\"Helga\", \"Bruno\", \"Ama\", \"Jessica\"]\n\n# A list of ages, in the same order.\nhippo_ages = [5, 12, 9, 15]\n\nprint(\"Names:\", hippo_names)\nprint(\"Ages: \", hippo_ages)\n\nprint(\"\\nType of hippo_names:\", type(hippo_names))\n\n# Accessing elements by position (indexing starts at 0).\nprint(\"\\nFirst hippo:\", hippo_names[0])\nprint(\"Age of first hippo:\", hippo_ages[0])\n\n# Adding a new hippo to the list.\nhippo_names.append(\"Fiona\")\nhippo_ages.append(3)\n\nprint(\"\\nAfter appending a new hippo:\")\nprint(\"Names:\", hippo_names)\nprint(\"Ages: \", hippo_ages)\n\n\n# Sets: collections of unique items.\n\n# Suppose we have a list of habitats with duplicates.\nhabitats_list = [\"River\", \"River\", \"Lake\", \"Zoo\", \"Lake\", \"River\"]\n\nprint(\"Habitats list:\", habitats_list)\n\n# Convert to a set to obtain only unique habitats.\nhabitats_set = set(habitats_list)\n\nprint(\"Unique habitats (set):\", habitats_set)\nprint(\"Type of habitats_set:\", type(habitats_set))\n\n\n# Dictionaries: key–value pairs.\n\n# Information about one hippo.\nhippo_info = {\n    \"name\": \"Helga\",\n    \"age_years\": 5,\n    \"habitat\": \"River\"\n}\n\nprint(\"Hippo info dictionary:\", hippo_info)\nprint(\"Type:\", type(hippo_info))\n\n# Access values by key.\nprint(\"\\nHippo name:\", hippo_info[\"name\"])\nprint(\"Hippo age:\", hippo_info[\"age_years\"])\nprint(\"Hippo habitat:\", hippo_info[\"habitat\"])\n\n# A dictionary is also useful for look-up tables.\n# For example, baseline grass intake (kg/day) by habitat.\nbaseline_grass = {\n    \"River\": 55.0,\n    \"Lake\": 45.0,\n    \"Zoo\": 50.0\n}\n\nprint(\"\\nBaseline grass intake for river hippos:\",\n      baseline_grass[\"River\"], \"kg per day\")\n\nLater in this notebook we will use these basic structures to build a pandas DataFrame, for example by combining:\n\na list of hippo_id values,\na list of name values,\nand a dictionary that maps column names to those lists.\n\nThis will give us a table that is easier to analyse and plot.\n\n\n3.2 3.2 What is a Python programme?\nA Python programme is a sequence of statements that will be executed from top to bottom. In a notebook the programme is effectively the combination of all code cells that you run.\nImportant points:\n\nPython uses the line order: earlier lines usually run before later lines.\nPython uses indentation (spaces at the beginning of a line) to define structure. Indentation is not cosmetic formatting; it is part of the language.\nLines starting with # are comments and are ignored by Python. They are for humans.\n\nThe small example below shows indentation and comments.\n\n# A tiny example that uses indentation and a comment.\n\nhippo_age = 12  # age in years\n\nif hippo_age &gt;= 10:\n    # This line is indented and belongs to the 'if' block.\n    print(\"This is an older hippo.\")\nelse:\n    # This line belongs to the 'else' block.\n    print(\"This is a younger hippo.\")\n\n\n\n3.3 3.3 Conditions and loops (basic structure)\nThe two most common control structures are:\n\nCondition: if condition: ... else: ... to choose between two branches.\nLoop: for item in collection: ... to repeat an action for each element of a sequence.\n\nThe following example uses a for loop to look at several hippo ages.\n\n# Example: loop over a list of hippo ages.\n\nhippo_ages = [3, 7, 12]\n\nfor age in hippo_ages:\n    if age &gt;= 10:\n        print(\"Age\", age, \"→ older hippo\")\n    else:\n        print(\"Age\", age, \"→ younger hippo\")\n\n\n\n3.4 3.4 Libraries and how to use them\nThe Python standard library is small. Most data analysis tools live in libraries that you import when you need them.\nIn FB2NEP we will mainly use three libraries:\n\n\n\n\n\n\n\n\nLibrary\nTypical import\nMain purpose\n\n\n\n\nNumPy\nimport numpy as np\nFast numerical operations and random numbers\n\n\npandas\nimport pandas as pd\nReading, cleaning, and summarising tabular data\n\n\nMatplotlib\nimport matplotlib.pyplot as plt\nCreating plots\n\n\n\nWe will now import these libraries. In Colab they are already installed.\n\n# Only run this cell if Colab reports that a library is missing.\n# In the teaching environment this step is usually not necessary.\n%pip install numpy pandas matplotlib --quiet\n\n\n# Import the core libraries used in this module.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Print versions (useful for reproducibility and debugging).\nimport sys, matplotlib\nprint(\"Python:\", sys.version.split()[0])\nprint(\"NumPy:\", np.__version__)\nprint(\"pandas:\", pd.__version__)\nprint(\"Matplotlib:\", matplotlib.__version__)\n\n\n\n3.5 3.5 Objects, attributes, and methods (very briefly)\nLibraries such as pandas and Matplotlib are built around objects. Examples include:\n\na pandas DataFrame (a table),\na pandas Series (a single column),\na Matplotlib Figure (a plot canvas).\n\nObjects usually provide:\n\nattributes (properties that you access with a dot, for example hippos.shape),\nmethods (functions that belong to the object, for example hippos.mean() or hippos.describe()).\n\nYou will see this in practice when we work with the hippo dietary survey."
  },
  {
    "objectID": "notebooks/1.03_introduction_colab_jupyter.html#hippo-dietary-survey-loading-a-small-dataset",
    "href": "notebooks/1.03_introduction_colab_jupyter.html#hippo-dietary-survey-loading-a-small-dataset",
    "title": "1.03 – Introduction to Colab and Jupyter",
    "section": "4 4. Hippo dietary survey: loading a small dataset",
    "text": "4 4. Hippo dietary survey: loading a small dataset\nFor this module we will use a small hippo dietary survey as a toy example. In the teaching repository it is stored as a CSV file at:\n\ndata/hippo_diet_survey.csv\n\nEach row represents one hippo. Columns include for example:\n\nhippo_id – unique identifier,\n\nname – hippo name,\n\nage_years – age in years,\n\nhabitat – for example River, Lake, Zoo,\n\nfruit_portions – fruit portions per day,\n\nveg_portions – vegetable portions per day,\n\ngrass_kg – kilograms of grass consumed per day.\n\nThis is a typical situation in applied work: someone has prepared a dataset, and your task is to load it, inspect it, and analyse it.\nIn this section we will use the hippo survey to illustrate:\n\nloading data from CSV,\n\nbasic inspection (rows, columns, types),\n\nlooking at individual variables,\n\nusing object methods such as .mean() and .groupby(),\n\ncreating a simple plot.\n\n\n4.1 4.1 Loading the hippo dataset from CSV\nIn a typical workflow the data file already exists and you only need to read it. The code below reads data/hippo_diet_survey.csv into a pandas DataFrame called hippos and shows the first few rows.\n\n# Load the hippo dietary survey from the CSV file.\n# This assumes you are running the notebook from the root of the repository,\n# with the data file present in the \"data\" subfolder.\n\nimport os\n\nhippo_path = \"data/hippo_diet_survey.csv\"\n\nif not os.path.exists(hippo_path):\n    raise FileNotFoundError(\n        f\"Could not find {hippo_path}. \"\n        \"Please check that you are running this notebook in the FB2NEP repository \"\n        \"and that the data file is present.\"\n    )\n\nhippos = pd.read_csv(hippo_path)\nhippos.head()\n\n\n\n4.2 4.2 Inspecting the data and variables\nThe hippos object is a pandas DataFrame. It has attributes and methods that help you to understand the structure of the data.\nCommonly used methods and attributes include:\n\nhippos.shape – attribute with number of rows and columns.\nhippos.columns – attribute with column names.\nhippos.info() – method with data types and missing values.\nhippos.describe() – method with summary statistics for numeric columns.\n\nRun the cell below and examine the output carefully.\n\n# Inspect the structure of the hippo dataset.\n\nprint(\"Shape (rows, columns):\", hippos.shape)\n\nprint(\"\\nColumn names:\")\nprint(hippos.columns.tolist())\n\nprint(\"\\nBasic information:\")\nhippos.info()\n\nprint(\"\\nSummary statistics for numeric columns:\")\nhippos.describe()\n\n\n\n4.3 4.3 Using methods to summarise the hippo data\nMany operations are available as methods. For example:\n\nhippos[\"fruit_portions\"].mean() computes the mean of the fruit_portions column.\nhippos.groupby(\"habitat\")[\"grass_kg\"].mean() computes the mean grass intake per habitat.\n\nThese methods are part of the object-oriented design of pandas: the DataFrame and Series objects provide the relevant functionality via the dot notation.\n\n# Mean fruit portions per day (all hippos).\nmean_fruit = hippos[\"fruit_portions\"].mean()\nprint(f\"Mean fruit portions per day (all hippos): {mean_fruit:.2f}\")\n\n# Mean grass intake per habitat.\nmean_grass_by_habitat = hippos.groupby(\"habitat\")[\"grass_kg\"].mean()\nprint(\"\\nMean grass intake (kg per day) by habitat:\")\nprint(mean_grass_by_habitat)\n\n# Example of a simple condition on the DataFrame: hippos older than 10 years.\nolder_hippos = hippos[hippos[\"age_years\"] &gt; 10]\nprint(\"\\nNumber of hippos older than 10 years:\", len(older_hippos))\n\n\n\n4.4 4.4 Plotting the hippo data\nWe can now create a simple plot using Matplotlib. A common pattern is:\n\nPrepare a summary table in pandas.\nPass the summary values to Matplotlib.\n\nBelow we create a bar chart of mean grass intake by habitat.\n\n# Prepare the summary again (for clarity).\nmean_grass_by_habitat = hippos.groupby(\"habitat\")[\"grass_kg\"].mean()\n\n# Create a bar chart.\nplt.figure()\nplt.bar(mean_grass_by_habitat.index, mean_grass_by_habitat.values)\nplt.xlabel(\"Habitat\")\nplt.ylabel(\"Mean grass intake (kg per day)\")\nplt.title(\"Hippo grass intake by habitat\")\nplt.xticks(rotation=15)\nplt.tight_layout()\nplt.show()\n\n\n4.4.1 Try it\nUsing the hippos DataFrame:\n\nCompute the mean fruit portions per habitat using groupby and mean.\nCreate a bar chart for mean fruit portions by habitat.\nChange the title and axis labels of the plot so that they describe your new chart.\n\nOptional: - Check for missing values using hippos.isna().sum(). - Select only hippos from one habitat, for example river hippos:\nriver_hippos = hippos[hippos[\"habitat\"] == \"River\"]."
  },
  {
    "objectID": "notebooks/1.03_introduction_colab_jupyter.html#reproducibility-and-open-science-principles",
    "href": "notebooks/1.03_introduction_colab_jupyter.html#reproducibility-and-open-science-principles",
    "title": "1.03 – Introduction to Colab and Jupyter",
    "section": "5 5. Reproducibility and open-science principles",
    "text": "5 5. Reproducibility and open-science principles\nOne key reason to use notebooks and version-controlled repositories (GitHub) in nutritional epidemiology is reproducibility.\nIn a reproducible analysis:\n\nThe path from data to results is visible.\nAnother researcher (or your future self) can rerun the analysis and obtain the same numbers and plots.\nImportant choices (for example exclusion criteria, variable definitions) are documented in text near the code.\n\nNotebooks help with this because they combine:\n\ncode (what you did),\noutputs (what you obtained),\nand explanations (why you did it).\n\nThe teaching notebooks for FB2NEP are stored in a Git repository on GitHub. Git records the history of changes over time. In later parts of your degree you may use Git directly for your own projects.\nFor now, a few simple good practices are sufficient:\n\nKeep notebooks and data in a consistent folder structure.\nUse clear, descriptive variable names in code.\nRecord decisions in short Markdown notes.\nFix a random seed (np.random.seed(...)) when you use random numbers, so that results are repeatable.\nWhen possible, use open formats such as CSV for data and share both data (if appropriate) and analysis code.\n\n\n# Small demonstration of a fixed random seed.\n# Run this cell several times and check that the numbers stay the same.\n\nnp.random.seed(11088)\nvalues = np.random.normal(loc=0, scale=1, size=5)\nprint(\"Random values:\", values)"
  },
  {
    "objectID": "notebooks/1.03_introduction_colab_jupyter.html#recap",
    "href": "notebooks/1.03_introduction_colab_jupyter.html#recap",
    "title": "1.03 – Introduction to Colab and Jupyter",
    "section": "6 6. Recap",
    "text": "6 6. Recap\nIn this introductory notebook you have:\n\nseen how the FB2NEP notebooks live in a GitHub repository and are opened in Google Colab,\nrun and edited simple Python code cells,\nlearned that indentation and line order matter for Python programmes,\nimported and briefly used the core libraries NumPy, pandas, and Matplotlib,\ncreated and loaded a small hippo dietary survey dataset from a CSV file,\ninspected the dataset with methods such as .head(), .info(), .describe(),\nused methods such as .mean() and .groupby() to summarise variables,\nproduced a simple bar chart from summarised data,\nand discussed how notebooks and Git support reproducible and transparent analyses.\n\nThese elements will recur throughout the FB2NEP epidemiology materials. The aim is that the tools become familiar so that you can concentrate on the underlying nutritional and epidemiological questions."
  },
  {
    "objectID": "notebooks/1.03_introduction_colab_jupyter.html#appendix-running-notebooks-locally",
    "href": "notebooks/1.03_introduction_colab_jupyter.html#appendix-running-notebooks-locally",
    "title": "1.03 – Introduction to Colab and Jupyter",
    "section": "7 Appendix: running notebooks locally",
    "text": "7 Appendix: running notebooks locally\nIf you prefer, you can also run the notebooks on your own computer instead of Colab. This requires a local installation of Python and Jupyter.\nTwo common approaches are:\n\nConda / Miniconda (recommended for beginners):\nconda create -n fb2nep python=3.11 -y\nconda activate fb2nep\nconda install jupyterlab numpy pandas matplotlib -y\njupyter lab\nvenv and pip:\npython -m venv fb2nep\n# macOS / Linux\nsource fb2nep/bin/activate\n# Windows (PowerShell)\nfb2nep\\\\Scripts\\\\activate\npip install jupyterlab numpy pandas matplotlib\njupyter lab\n\nKey concepts:\n\nEnvironment: an isolated Python installation with its own set of packages.\nKernel: the Python process that executes the code of a notebook.\nWorking directory: the folder from which the notebook reads and writes files."
  },
  {
    "objectID": "notebooks/1.03_introduction_colab_jupyter.html#appendix-what-the-setup-bootstrap-cell-does",
    "href": "notebooks/1.03_introduction_colab_jupyter.html#appendix-what-the-setup-bootstrap-cell-does",
    "title": "1.03 – Introduction to Colab and Jupyter",
    "section": "8 Appendix: What the setup / bootstrap cell does",
    "text": "8 Appendix: What the setup / bootstrap cell does\nAt the top of this notebook you were asked to run a setup (bootstrap) cell and to ignore the details for now.\nThis appendix explains what that cell and the underlying scripts/bootstrap.py file actually do.\n\n8.1 1. The setup cell in the notebook\nThe notebook code you saw looked roughly like this:\nimport os\nimport sys\nimport runpy\nimport pathlib\nimport subprocess\n\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_NAME = \"fb2nep-epi\"\n\n# 1. If we are in Colab and scripts/bootstrap.py is not present,\n#    clone the repository and change into it.\nif \"google.colab\" in sys.modules and not pathlib.Path(\"scripts/bootstrap.py\").exists():\n    root = pathlib.Path(\"/content\")\n    repo_dir = root / REPO_NAME\n\n    if not repo_dir.exists():\n        print(f\"Cloning {REPO_URL} …\")\n        subprocess.run([\"git\", \"clone\", REPO_URL], check=True)\n\n    os.chdir(repo_dir)\n    print(\"Changed working directory to:\", os.getcwd())\n\n# 2. Now try to locate and run scripts/bootstrap.py\nfor p in [\"scripts/bootstrap.py\", \"../scripts/bootstrap.py\", \"../../scripts/bootstrap.py\"]:\n    if pathlib.Path(p).exists():\n        print(f\"Bootstrapping via: {p}\")\n        runpy.run_path(p)\n        break\nelse:\n    print(\"⚠️ scripts/bootstrap.py not found – \"\n          \"please check that the FB2NEP repository is available.\")\nThis does two main things:\n\nIf running in Google Colab and the repository is not present, it clones the fb2nep-epi repository from GitHub into /content/fb2nep-epi and changes the working directory to that folder.\nIt then searches the current directory (and parents) for scripts/bootstrap.py and runs it as soon as it is found.\n\n\n\n\n8.2 2. Goals of scripts/bootstrap.py\nbootstrap.py is designed to solve three practical problems:\n\nFinding the repository root\nIn Colab the working directory is often /content.\nThe script ensures the notebook ends up in the FB2NEP repository (the folder containing scripts/ and notebooks/).\nEnsuring that required Python packages are available\nIt checks whether libraries like numpy, pandas, matplotlib, and statsmodels can be imported.\n\nIn Colab, if they are missing, it installs them.\n\nOn a local machine, it prints a warning rather than installing anything automatically.\n\nEnsuring that the main teaching dataset exists\nIt checks whether the primary synthetic FB2NEP dataset (e.g. data/synthetic/fb2nep.csv) is present.\n\nIf missing, it runs the generator script (e.g. scripts/generate_dataset.py).\n\nIn Colab it can also prompt for manual upload.\n\n\nThese steps are handled by the helper functions ensure_repo_root, ensure_deps, and ensure_data.\n\n\n\n8.3 3. Details of the main functions\n(a) ensure_repo_root()\n\nLooks for a directory that contains both scripts/ and notebooks/.\n\nMoves up one directory if the notebook was opened from within notebooks/.\n\nIn Colab, clones the repository if it is not present.\n\n\n(b) ensure_deps()\n\nAttempts to import numpy, pandas, matplotlib, and statsmodels.\n\nIn Colab, installs missing dependencies.\n\nLocally, prints a warning but does not modify the environment.\n\n\n(c) ensure_data(csv_rel, gen_script)\n\nChecks whether the dataset exists.\n\nAttempts to generate it via the relevant script if necessary.\n\nIn Colab, as a final fallback, requests manual upload.\n\n\n\n\n8.4 4. The init() function\nThe init() function in bootstrap.py:\n\nCalls the helper functions (ensure_repo_root, ensure_deps, ensure_data).\n\nLoads the primary dataset into a DataFrame df.\n\nCreates a small context object ctx.\n\nInjects helper variables (df, CTX, CSV_REL, REPO_ROOT, IN_COLAB) into the notebook environment.\n\nExample use:\nfrom scripts.bootstrap import init\ndf, ctx = init()\ndf.head()\n\n\n\n8.5 5. Why this is hidden at the top\nThe purpose of the setup cell is to ensure that:\n\nthe notebook runs in the correct repository directory,\n\nrequired Python libraries are available,\n\nrequired datasets exist and can be loaded.\n\nOnce this is done, the rest of the notebook can focus on epidemiology and analysis, not technical setup."
  },
  {
    "objectID": "howto_sandbox/Playground.html",
    "href": "howto_sandbox/Playground.html",
    "title": "Playground (Sandbox)",
    "section": "",
    "text": "This is a safe space to experiment. It generates a small synthetic dataset so you can practise plotting and simple analyses used in FB2NEP.\n# Reset & verify environment (run first if anything is odd)\nimport sys, numpy as np, pandas as pd, matplotlib\nprint(\"Python:\", sys.version.split()[0])\nprint(\"NumPy:\", np.__version__, \"| pandas:\", pd.__version__, \"| Matplotlib:\", matplotlib.__version__)\nSEED = 11088  # FB2NEP reproducibility convention\nnp.random.seed(SEED)\n# Setup: import required libraries for the sandbox\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport scipy.stats as stats\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\npd.set_option(\"display.max_columns\", 50)\n# If a package is missing in Colab, uncomment and run, then Runtime → Restart runtime\n# %pip -q install statsmodels scipy"
  },
  {
    "objectID": "howto_sandbox/Playground.html#parameters-change-these-and-re-run",
    "href": "howto_sandbox/Playground.html#parameters-change-these-and-re-run",
    "title": "Playground (Sandbox)",
    "section": "1 1) Parameters — change these and re-run",
    "text": "1 1) Parameters — change these and re-run\n\n# ▶ Try changing N or the effect sizes and re-run the next cells\nN = 400          # sample size\nRATIO_F = 0.6    # fraction female\nGROUP_EFFECT = -3.5  # mean SBP difference (B vs A), in mmHg\n\nnp.random.seed(SEED)"
  },
  {
    "objectID": "howto_sandbox/Playground.html#generate-a-simple-dataset",
    "href": "howto_sandbox/Playground.html#generate-a-simple-dataset",
    "title": "Playground (Sandbox)",
    "section": "2 2) Generate a simple dataset",
    "text": "2 2) Generate a simple dataset\n\nages = np.random.normal(45, 12, N).round(1)\nbmi  = np.random.normal(26, 4, N).round(1)\n\nsex = np.where(np.random.rand(N) &lt; RATIO_F, \"F\", \"M\")\ngroup = np.where(np.random.rand(N) &lt; 0.5, \"A\", \"B\")\n\n# SBP depends on age, BMI, sex, and group (B has lower mean by GROUP_EFFECT)\nbase = 110 + 0.35*ages + 0.9*bmi + np.where(sex==\"M\", 4.0, 0.0)\nsbp = base + np.where(group==\"B\", GROUP_EFFECT, 0.0) + np.random.normal(0, 8, N)\n\n# Total cholesterol (mmol/L) loosely related to age/BMI\nchol = 3.8 + 0.015*ages + 0.05*bmi + np.random.normal(0, 0.4, N)\n\n# Binary outcome: high SBP (≥140)\nhigh_sbp = (sbp &gt;= 140).astype(int)\n\ndf = pd.DataFrame({\n    \"age\": ages,\n    \"bmi\": bmi,\n    \"sex\": sex,\n    \"group\": group,\n    \"sbp\": sbp.round(1),\n    \"chol\": chol.round(2),\n    \"high_sbp\": high_sbp\n})\ndf.head()"
  },
  {
    "objectID": "howto_sandbox/Playground.html#quick-exploration",
    "href": "howto_sandbox/Playground.html#quick-exploration",
    "title": "Playground (Sandbox)",
    "section": "3 3) Quick exploration",
    "text": "3 3) Quick exploration\nRun the following cells; then try changing parameters above (e.g. N, GROUP_EFFECT) and re-run.\n\nprint(\"Shape:\", df.shape)\ndf.info()\n\n\ndf.describe(include=\"all\")\n\n\ndf['sex'].value_counts(), df['group'].value_counts()\n\n\ndf.isna().mean()  # missingness per column"
  },
  {
    "objectID": "howto_sandbox/Playground.html#plots",
    "href": "howto_sandbox/Playground.html#plots",
    "title": "Playground (Sandbox)",
    "section": "4 4) Plots",
    "text": "4 4) Plots\n\n# Histogram of SBP\ndf['sbp'].hist(bins=25)\nplt.xlabel(\"SBP (mmHg)\"); plt.ylabel(\"Count\"); plt.title(\"SBP distribution\")\nplt.tight_layout()\nplt.show()\n\n\n# Boxplot by group\ndf.boxplot(column=\"sbp\", by=\"group\")\nplt.suptitle(\"\")\nplt.title(\"SBP by group\"); plt.xlabel(\"group\"); plt.ylabel(\"SBP (mmHg)\")\nplt.tight_layout()\nplt.show()\n\n\n# Scatter: BMI vs SBP\nplt.scatter(df['bmi'], df['sbp'])\nplt.xlabel(\"BMI\"); plt.ylabel(\"SBP (mmHg)\"); plt.title(\"BMI vs SBP\")\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "howto_sandbox/Playground.html#table-1-style-summary",
    "href": "howto_sandbox/Playground.html#table-1-style-summary",
    "title": "Playground (Sandbox)",
    "section": "5 5) Table 1-style summary",
    "text": "5 5) Table 1-style summary\n\n# Continuous variables by group\ncont = df.groupby('group')[['age','bmi','sbp','chol']].agg(['mean','std','count'])\ncont\n\n\n# Categorical variables by group\npd.crosstab(df['group'], df['sex'], margins=True, normalize='index')"
  },
  {
    "objectID": "howto_sandbox/Playground.html#basic-hypothesis-tests",
    "href": "howto_sandbox/Playground.html#basic-hypothesis-tests",
    "title": "Playground (Sandbox)",
    "section": "6 6) Basic hypothesis tests",
    "text": "6 6) Basic hypothesis tests\n\n# Two-sample t-test: SBP between A and B\na = df.loc[df['group']=='A','sbp']\nb = df.loc[df['group']=='B','sbp']\nstats.ttest_ind(a, b, equal_var=False)\n\n\n# Chi-square test: sex distribution by group\ntab = pd.crosstab(df['group'], df['sex'])\ntab, stats.chi2_contingency(tab)[:2]  # (table, (chi2, p))"
  },
  {
    "objectID": "howto_sandbox/Playground.html#simple-models",
    "href": "howto_sandbox/Playground.html#simple-models",
    "title": "Playground (Sandbox)",
    "section": "7 7) Simple models",
    "text": "7 7) Simple models\nWe’ll use statsmodels with formula syntax. C(var) treats a variable as categorical.\n\n# OLS: SBP ~ age + BMI + sex + group\nols = smf.ols(\"sbp ~ age + bmi + C(sex) + C(group)\", data=df).fit()\nols.summary()\n\n\n# Logistic regression: high_sbp (0/1) ~ predictors\nlogit = smf.logit(\"high_sbp ~ age + bmi + C(sex) + C(group)\", data=df).fit(disp=False)\nlogit.summary()"
  },
  {
    "objectID": "howto_sandbox/Playground.html#data-io-helpers-optional-colab",
    "href": "howto_sandbox/Playground.html#data-io-helpers-optional-colab",
    "title": "Playground (Sandbox)",
    "section": "8 8) Data I/O helpers (optional, Colab)",
    "text": "8 8) Data I/O helpers (optional, Colab)\n\n# Upload a CSV from your computer (Colab only)\n# from google.colab import files\n# up = files.upload()  # pick a file\n# import pandas as pd\n# df = pd.read_csv(next(iter(up.keys())))\n# df.head()\n\n\n# Mount your Google Drive (persistent files across sessions)\n# from google.colab import drive\n# drive.mount('/content/drive')\n# Example save path:\n# df.to_csv('/content/drive/MyDrive/fb2nep/sandbox_output.csv', index=False)"
  },
  {
    "objectID": "howto_sandbox/Playground.html#export-your-work",
    "href": "howto_sandbox/Playground.html#export-your-work",
    "title": "Playground (Sandbox)",
    "section": "9 9) Export your work",
    "text": "9 9) Export your work\n\n# Save your sandbox dataset/results locally in this session\ndf.to_csv(\"sandbox_output.csv\", index=False)\nprint(\"Saved as sandbox_output.csv. In Colab, open the Files panel (left) → three dots → Download.\")"
  },
  {
    "objectID": "howto_sandbox/Playground.html#your-turn-try-these",
    "href": "howto_sandbox/Playground.html#your-turn-try-these",
    "title": "Playground (Sandbox)",
    "section": "10 10) Your turn — try these",
    "text": "10 10) Your turn — try these\n\nChange GROUP_EFFECT to +2.0 (so group B has higher SBP) and re-generate. What happens to the t-test and model coefficients?\n\nIncrease N to 2000. Do p-values change? Why?\n\nAdd C(group):C(sex) interaction to the OLS formula. Does it help?\n\nCreate a new variable waist = 2.5*bmi + noise and see how it relates to SBP."
  },
  {
    "objectID": "howto_sandbox/index.html",
    "href": "howto_sandbox/index.html",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "",
    "text": "This page shows you how to open and use the module notebooks in Google Colab (or locally in Jupyter). It includes direct links and easy to understand guidance. If you have never used Python before, you are in the right place.",
    "crumbs": [
      "Home",
      "Sandbox",
      "FB2NEP — How‑To & Sandbox (Colab/Jupyter)"
    ]
  },
  {
    "objectID": "howto_sandbox/index.html#quick-links",
    "href": "howto_sandbox/index.html#quick-links",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "1 Quick links",
    "text": "1 Quick links\nOpen the notebooks directly in Colab:\n\nIntroduction Notebook\n  · View on GitHub\nPlayground (Sandbox)\n  · View on GitHub\n\nPrintable cheat‑sheet:\n- python‑cheatsheet.md (open “Raw”, then save/print)\n\nIf you fork or use a different repository, update the links by replacing ggkuhnle/fb2nep-epi with your repo slug.\n\n\n\nWhat does “fork” mean on GitHub?\n\nA fork is your own copy of someone else’s GitHub repository under your GitHub account.\nYou can change it freely without affecting the original. Your fork can still receive updates from the original (“upstream”).\nWhy fork? - You want your own version of the materials. - You don’t have write permission on the original repo. - You plan to customise and maybe propose changes back later (via pull requests).\nHow to fork (GitHub web): 1. Open the original repo (e.g. https://github.com/ggkuhnle/fb2nep-epi). 2. Click Fork (top-right) → choose your account → Create fork. 3. You now have https://github.com/&lt;YOUR-USER&gt;/&lt;YOUR-REPO&gt;.\nUpdate links after forking Replace the owner/repo part with your fork’s repo slug: - GitHub view:\nhttps://github.com/&lt;YOUR-USER&gt;/&lt;YOUR-REPO&gt;/blob/main/path/to/notebook.ipynb - Colab badge:\nhttps://colab.research.google.com/github/&lt;YOUR-USER&gt;/&lt;YOUR-REPO&gt;/blob/main/path/to/notebook.ipynb\nFork vs. clone vs. branch (quick) - Fork: makes your own repo on GitHub. - Clone: makes a local copy on your computer. - Branch: a line of development inside one repo (yours or the original).\nKeeping your fork up-to-date - On GitHub: open your fork and click Sync fork / Fetch upstream. - With Git (advanced):\n```bash git remote add upstream https://github.com/ggkuhnle/fb2nep-epi.git git fetch upstream git checkout main git merge upstream/main git push",
    "crumbs": [
      "Home",
      "Sandbox",
      "FB2NEP — How‑To & Sandbox (Colab/Jupyter)"
    ]
  },
  {
    "objectID": "howto_sandbox/index.html#how-to-open-a-notebook-in-colab-stepbystep",
    "href": "howto_sandbox/index.html#how-to-open-a-notebook-in-colab-stepbystep",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "2 How to open a notebook in Colab (step‑by‑step)",
    "text": "2 How to open a notebook in Colab (step‑by‑step)\n\nWhat is Colab? Colab is a hosted Jupyter Notebook service that requires no setup to use and provides free access to computing resources, including GPUs and TPUs. Colab is especially well suited to machine learning, data science, and education. Requires a Google account.\n\n\nRight-clicking will open Colab in a new browser window.\n\n\nClick an Open in Colab button above. Colab will load the notebook from GitHub.\nAt the top‑right, click Connect if it isn’t already connected. This starts a fresh, temporary Python session (a “runtime”).\n\nYou may see a warning like “This notebook was not authored by Google.”\n\nClick Run anyway. Our notebooks are plain text and safe to run.\n\n\nYou might also see “Warning: This notebook requires permissions to run.”\n\nClick Run anyway. Colab sandboxes code; nothing runs on your computer.\n\n\nTo run a cell, click the small ▶ button on its left, or press Shift + Enter.\n\nTo run every cell from top to bottom, go to Runtime → Run all.\n\nTo save your own editable copy, go to File → Save a copy in Drive. You now have a personal copy you can edit freely.\n\n\n2.1 Useful Colab options\n\nRuntime → Restart runtime resets the session if things get stuck.\n\nRuntime → Change runtime type (we use Python 3; no GPU needed).\n\nFile → Download lets you save the executed notebook as .ipynb or PDF.",
    "crumbs": [
      "Home",
      "Sandbox",
      "FB2NEP — How‑To & Sandbox (Colab/Jupyter)"
    ]
  },
  {
    "objectID": "howto_sandbox/index.html#how-to-open-a-notebook-in-binder-stepbystep",
    "href": "howto_sandbox/index.html#how-to-open-a-notebook-in-binder-stepbystep",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "3 How to open a notebook in Binder (step‑by‑step)",
    "text": "3 How to open a notebook in Binder (step‑by‑step)\n\nWhat is Binder? Binder turns a GitHub repository into a live, interactive Jupyter environment running in your browser. No account or login required — just click and go.\n\n\nNote: Binder may take 1–2 minutes to start the first time, as it builds the environment. Your work is not saved between sessions — download your notebook if you want to keep it.\n\n\nClick an Open in Binder badge above. A new tab will open and Binder will begin building the environment.\nWait for the progress bar to complete. You will see log messages scrolling — this is normal.\nOnce loaded, the notebook will open automatically in Jupyter.\nTo run a cell, click it and press Shift + Enter, or use the ▶ button in the toolbar.\nTo run all cells, go to Cell → Run All in the menu.\nTo save your work, go to File → Download as → Notebook (.ipynb) before closing. Binder sessions are temporary and do not persist.\n\n\n3.1 Useful Binder notes\n\nSession timeout: Binder sessions close after ~10 minutes of inactivity. Save your work regularly.\nNo GPU needed for these notebooks — the default Binder environment is fine.\nSlow start? This is normal on first launch. Once running, the notebook behaves identically to any other Jupyter environment.",
    "crumbs": [
      "Home",
      "Sandbox",
      "FB2NEP — How‑To & Sandbox (Colab/Jupyter)"
    ]
  },
  {
    "objectID": "howto_sandbox/index.html#working-locally-in-jupyter-optional",
    "href": "howto_sandbox/index.html#working-locally-in-jupyter-optional",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "4 Working locally in Jupyter (optional)",
    "text": "4 Working locally in Jupyter (optional)\nIf you prefer local execution: 1. Install Anaconda or Miniconda, then open Jupyter Lab.\n2. Download the .ipynb files from GitHub and open them in Jupyter Lab.\n3. Run cells with Shift + Enter as in Colab.",
    "crumbs": [
      "Home",
      "Sandbox",
      "FB2NEP — How‑To & Sandbox (Colab/Jupyter)"
    ]
  },
  {
    "objectID": "howto_sandbox/index.html#troubleshooting",
    "href": "howto_sandbox/index.html#troubleshooting",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "5 Troubleshooting",
    "text": "5 Troubleshooting\n\nModuleNotFoundError (e.g. statsmodels): in Colab, run the cell that starts with !pip install ..., then Runtime → Restart runtime, and re‑run the code. In Binder, all packages are pre-installed — if a module is missing, try Kernel → Restart.\n\nKernel crashed / out of memory: restart the runtime. Don’t open huge datasets in this sandbox.\n\nLong‑running cells: click the stop icon (■) next to the cell number, or Runtime → Interrupt execution (Colab) / Kernel → Interrupt (Binder).\nBinder won’t load: try refreshing the page. If it continues to fail, use Colab as a fallback.",
    "crumbs": [
      "Home",
      "Sandbox",
      "FB2NEP — How‑To & Sandbox (Colab/Jupyter)"
    ]
  },
  {
    "objectID": "howto_sandbox/index.html#what-you-will-learn-here",
    "href": "howto_sandbox/index.html#what-you-will-learn-here",
    "title": "FB2NEP — How‑To & Sandbox (Colab/Jupyter)",
    "section": "6 What you will learn here",
    "text": "6 What you will learn here\n\nHow to run Python cells safely in the cloud (Colab or Binder).\n\nHow to make small edits and immediately see their effect.\n\nHow to generate simple synthetic data and make basic plots & analyses you’ll reuse in FB2NEP.\n\nNow jump into the Introduction Notebook first, then play in the Playground.",
    "crumbs": [
      "Home",
      "Sandbox",
      "FB2NEP — How‑To & Sandbox (Colab/Jupyter)"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "FB2NEP — Nutritional Epidemiology",
    "section": "",
    "text": "This page links to the FB2NEP practical workbooks.\nEach notebook can be opened in two ways — choose whichever works best for you:\n\nGoogle Colab — requires a Google account, but offers a reliable environment with easy file saving to Google Drive.\nBinder — no account required; opens directly in your browser. May take a minute to start up.\n\nYou may also download the notebooks and run them locally in Jupyter.",
    "crumbs": [
      "Home",
      "FB2NEP — Nutritional Epidemiology"
    ]
  },
  {
    "objectID": "index.html#practical-workbooks",
    "href": "index.html#practical-workbooks",
    "title": "FB2NEP — Nutritional Epidemiology",
    "section": "",
    "text": "This page links to the FB2NEP practical workbooks.\nEach notebook can be opened in two ways — choose whichever works best for you:\n\nGoogle Colab — requires a Google account, but offers a reliable environment with easy file saving to Google Drive.\nBinder — no account required; opens directly in your browser. May take a minute to start up.\n\nYou may also download the notebooks and run them locally in Jupyter.",
    "crumbs": [
      "Home",
      "FB2NEP — Nutritional Epidemiology"
    ]
  },
  {
    "objectID": "index.html#how-to-and-sandbox-optional-but-recommended",
    "href": "index.html#how-to-and-sandbox-optional-but-recommended",
    "title": "FB2NEP — Nutritional Epidemiology",
    "section": "How-To and sandbox (optional, but recommended)",
    "text": "How-To and sandbox (optional, but recommended)\nIf you have not used Python or Google Colab before, start with the How-To & Sandbox materials.\nThey show you how to open notebooks, run code safely, and try things out without breaking anything.\n\nFB2NEP — How-To & Sandbox (Colab/Jupyter)",
    "crumbs": [
      "Home",
      "FB2NEP — Nutritional Epidemiology"
    ]
  },
  {
    "objectID": "index.html#part-1---nutritional-epidemiology",
    "href": "index.html#part-1---nutritional-epidemiology",
    "title": "FB2NEP — Nutritional Epidemiology",
    "section": "Part 1 - Nutritional Epidemiology",
    "text": "Part 1 - Nutritional Epidemiology\n\n\n1.03 - Introduction to Jupyter and Google Colab\n     \n\n\n\n1.04 - Data collection and cleaning\n     \n\n\n\n1.05 - Representativeness and sampling\n     \n\n\n\n1.06 - Data exploration and “Table 1”\n     \n\n\n\n1.07 - Data transformation\n     \n\n\n\n1.08 - Regression and modelling (Part 1)\n     \n\n\n\n1.09 - Regression and modelling (Part 2)\n     \n\n\n\n1.10 - Missing data and sensitivity analysis",
    "crumbs": [
      "Home",
      "FB2NEP — Nutritional Epidemiology"
    ]
  },
  {
    "objectID": "index.html#part-2---public-health-nutrition-policy-and-evaluation",
    "href": "index.html#part-2---public-health-nutrition-policy-and-evaluation",
    "title": "FB2NEP — Nutritional Epidemiology",
    "section": "Part 2 - Public Health Nutrition: Policy and Evaluation",
    "text": "Part 2 - Public Health Nutrition: Policy and Evaluation\nThe following workbooks focus on quantitative methods for evaluating public health nutrition policies.\n\n\n2.02 - DALYs and QALYs\nQuantifying population health burden using Disability-Adjusted Life Years and Quality-Adjusted Life Years. Includes an interactive exercise where you set your own disability weights and compare to GBD values.\n     \n\n\n\n2.03 - Health inequalities\nMeasuring the social gradient in health using the Slope Index of Inequality (SII), Relative Index of Inequality (RII), and concentration curves. Applied to dietary intake and nutrition-related outcomes.\n     \n\n\n\n2.04 - Case study: Salt reduction (with objective measures)\nEvaluating the UK salt reduction programme using 24-hour urinary sodium — a model example of policy evaluation with an objective biomarker. Includes health impact modelling and sensitivity analysis.\n     \n\n\n\n2.05 - Case study: Sugar reduction (without objective measures)\nEvaluating the Soft Drinks Industry Levy (SDIL) without a biomarker — exploring the evidence hierarchy from product reformulation to health outcomes. Addresses the counterfactual problem in policy evaluation.\n     \n\n\n\n2.06 - Policy simulation and resource allocation\nInteractive budget allocation game: distribute £50M across public health nutrition interventions. Explore trade-offs between efficiency and equity, the impact of diminishing returns, and uncertainty in cost-effectiveness estimates.",
    "crumbs": [
      "Home",
      "FB2NEP — Nutritional Epidemiology"
    ]
  },
  {
    "objectID": "index.html#assessment",
    "href": "index.html#assessment",
    "title": "FB2NEP — Nutritional Epidemiology",
    "section": "Assessment",
    "text": "Assessment\nPlease find assessment details here",
    "crumbs": [
      "Home",
      "FB2NEP — Nutritional Epidemiology"
    ]
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html",
    "href": "howto_sandbox/python-cheatsheet.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#imports",
    "href": "howto_sandbox/python-cheatsheet.html#imports",
    "title": "",
    "section": "1.1 0) Imports",
    "text": "1.1 0) Imports\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Stats & modelling (install if missing)\nimport scipy.stats as stats\nimport statsmodels.api as sm\nimport statsmodels.formula_api as smf\nIf a library is missing in Colab:\n!pip -q install statsmodels\n# then: Runtime → Restart runtime"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#load-save-data",
    "href": "howto_sandbox/python-cheatsheet.html#load-save-data",
    "title": "",
    "section": "1.2 1) Load / save data",
    "text": "1.2 1) Load / save data\n# CSV from local upload or Drive\ndf = pd.read_csv(\"my_data.csv\")\n\n# CSV from GitHub (raw)\nurl = \"https://raw.githubusercontent.com/ggkuhnke/fb2nep-eoi/main/data/synthetic/fb2nep.csv\"\ndf = pd.read_csv(url)\n\n# Save\ndf.to_csv(\"output.csv\", index=False)"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#quick-look",
    "href": "howto_sandbox/python-cheatsheet.html#quick-look",
    "title": "",
    "section": "1.3 2) Quick look",
    "text": "1.3 2) Quick look\ndf.head()\ndf.tail()\ndf.shape\ndf.info()\ndf.describe(include=\"all\")\ndf[\"sex\"].value_counts(dropna=False)\ndf.isna().mean()  # fraction missing per column"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#select-filter-transform",
    "href": "howto_sandbox/python-cheatsheet.html#select-filter-transform",
    "title": "",
    "section": "1.4 3) Select / filter / transform",
    "text": "1.4 3) Select / filter / transform\n# Columns\ndf[[\"age\", \"bmi\"]]\n\n# Rows\ndf[df[\"age\"] &gt;= 50]\n\n# New columns\ndf[\"bmi_sq\"] = df[\"bmi\"] ** 2\n\n# Rename\ndf = df.rename(columns={\"cholesterol\": \"chol\"})\n\n# Sort\ndf = df.sort_values([\"age\", \"bmi\"], ascending=[True, False])"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#grouping-summaries",
    "href": "howto_sandbox/python-cheatsheet.html#grouping-summaries",
    "title": "",
    "section": "1.5 4) Grouping & summaries",
    "text": "1.5 4) Grouping & summaries\ndf.groupby(\"group\")[\"bmi\"].mean()\ndf.groupby([\"group\", \"sex\"])[\"sbp\"].agg([\"mean\", \"std\", \"count\"])\n\n# Crosstab\npd.crosstab(df[\"group\"], df[\"sex\"], margins=True, normalize=\"index\")"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#plotting-quick",
    "href": "howto_sandbox/python-cheatsheet.html#plotting-quick",
    "title": "",
    "section": "1.6 5) Plotting (quick)",
    "text": "1.6 5) Plotting (quick)\ndf[\"bmi\"].hist(bins=20)\nplt.title(\"BMI distribution\")\nplt.xlabel(\"BMI\"); plt.ylabel(\"Count\")\nplt.show()\n\n# Boxplot by group\ndf.boxplot(column=\"sbp\", by=\"group\")\nplt.suptitle(\"\"); plt.title(\"SBP by group\"); plt.xlabel(\"group\"); plt.ylabel(\"SBP\")\nplt.show()\n\n# Scatter\nplt.scatter(df[\"bmi\"], df[\"sbp\"])\nplt.xlabel(\"BMI\"); plt.ylabel(\"SBP\"); plt.show()"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#basic-stats",
    "href": "howto_sandbox/python-cheatsheet.html#basic-stats",
    "title": "",
    "section": "1.7 6) Basic stats",
    "text": "1.7 6) Basic stats\n# Two-sample t-test\na = df.loc[df[\"group\"] == \"A\", \"sbp\"]\nb = df.loc[df[\"group\"] == \"B\", \"sbp\"]\nstats.ttest_ind(a, b, equal_var=False, nan_policy=\"omit\")\n\n# Chi-square test on a 2x2\ntab = pd.crosstab(df[\"group\"], df[\"sex\"])\nstats.chi2_contingency(tab)"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#simple-models-statsmodels",
    "href": "howto_sandbox/python-cheatsheet.html#simple-models-statsmodels",
    "title": "",
    "section": "1.8 7) Simple models (statsmodels)",
    "text": "1.8 7) Simple models (statsmodels)\n# OLS regression\nmodel = smf.ols(\"sbp ~ age + bmi + C(sex) + C(group)\", data=df).fit()\nprint(model.summary())\n\n# Logistic regression (binary outcome)\n# e.g., 'high_sbp' is 0/1\nlogit = smf.logit(\"high_sbp ~ age + bmi + C(sex) + C(group)\", data=df).fit()\nprint(logit.summary())"
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#jupyter-basics",
    "href": "howto_sandbox/python-cheatsheet.html#jupyter-basics",
    "title": "",
    "section": "1.9 8) Jupyter basics",
    "text": "1.9 8) Jupyter basics\n\nRun cell: Shift + Enter\nInsert cell above/below: A / B\nInterrupt: stop button (■) or Kernel/Runtime → Interrupt\nRestart: Kernel/Runtime → Restart\nMarkdown cell: text with # headings, **bold**, lists, etc."
  },
  {
    "objectID": "howto_sandbox/python-cheatsheet.html#reproducibility",
    "href": "howto_sandbox/python-cheatsheet.html#reproducibility",
    "title": "",
    "section": "1.10 9) Reproducibility",
    "text": "1.10 9) Reproducibility\nSEED = 11088\nnp.random.seed(SEED)\n\nRecord: dataset version, random seed, and exact code you ran."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html",
    "href": "howto_sandbox/Introduction-Notebook.html",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "",
    "text": "Welcome — this notebook provides a brief introduction to Colab and Python. This can be used for practice, but also for reference. Like everything else in this module, this notebook assumes no prior Python experience. You will learn how to run cells, accept Colab warnings, and make tiny edits to code."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#what-does-python-code-look-like",
    "href": "howto_sandbox/Introduction-Notebook.html#what-does-python-code-look-like",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "1 0. What does Python code look like?",
    "text": "1 0. What does Python code look like?\n\n\nWhat is Python?\n\nPython is a popular, general-purpose programming language known for being readable, beginner-friendly, and widely used in science, data analysis, and education.\nKey points - Easy to read: clear, English-like syntax; indentation (spacing) shows structure. - Interpreted: you run code line-by-line (great for notebooks and exploration). - “Batteries included”: comes with many built-in tools; huge ecosystem of libraries. - Cross-platform: works on Windows, macOS, and Linux; also runs in the cloud (Colab).\nWhy we use it in FB2NEP - Excellent libraries for data work: NumPy (numbers), pandas (tables), Matplotlib (plots). - Integrates smoothly with Jupyter/Colab, so you can mix code + text + results. - Perfect for transparent, reproducible analysis.\nDo I need to be a programmer? No. You’ll use short, explained snippets to explore concepts. You’re learning with code, not learning to code per se.\nWhich version? We use Python 3 (the modern standard). Colab already provides it; local users can install it via Miniconda/Anaconda or venv.\nTiny example:\nprint(\"Hello from Python!\")\n\nPython is a set of instructions written line by line.\nAnything after a # on a line is a comment — it’s for humans to read and Python ignores it.\nExample:\n# This is a comment — it explains the next line\nx = 2 + 3            # ← comment at the end of a line\nprint(x)             # prints 5 on the screen\nYou run code by clicking the little ▶ button to the left of a cell, or by pressing Shift + Enter.\n\nIf you see a warning like “This notebook was not authored by Google”, click Run anyway.\nColab runs code safely in the cloud and won’t change anything on your computer.\n\n\n1.1 Python has commands and data\nWhen you run a cell, Python executes commands (instructions) that act on data (values).\n\nCommands: things like print(...), len(...), sum(...), or your own functions.\n\nData: numbers, text, lists, tables, etc., usually stored in variables (names you choose).\n\nTiny example:\nname = \"Fiona\"       # data (text)\nn = 3                # data (number)\nprint(\"Hello\", name) # command acting on data\nprint(\"n + 2 =\", n + 2)\n\n\n1.2 What do we mean by “data”?\nData are the values your code works with. Common kinds you’ll see here:\n\nNumbers: 3, 2.5 (counts, measurements)\nText (strings): \"apple\", \"F\" (labels, categories)\nBooleans: True / False (yes/no flags)\nLists: [\"Mon\",\"Tue\",\"Wed\"], [2, 1, 3] (ordered collections)\nDictionaries: {\"age\": 10, \"height_cm\": 140} (named pieces of data)\nTables (pandas DataFrame): spreadsheet-like rows & columns for analysis\n\nMini examples:\nage = 10                           # number\nfruit = [\"apple\", \"banana\"]        # list of text\nheights = {\"Alex\":150, \"Sam\":146}  # dictionary\nIn FB2NEP we mostly use numbers, text labels, and tables (pandas DataFrame) because they map naturally to real datasets (participants × variables). You will also encounter missing values (shown as NaN) — that simply means “no recorded value”."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#running-a-cell",
    "href": "howto_sandbox/Introduction-Notebook.html#running-a-cell",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "2 1. Running a cell",
    "text": "2 1. Running a cell\nClick the ▶ icon to the left of a cell, or press Shift + Enter.\nTry it now — run the cell below.\n\n# ----------------------------------------------\n# Adding two numbers\n# This cell has comments that explain each step.\n# Lines starting with '#' are comments and do not run.\n# ----------------------------------------------\n# 🦛 Friendly hippo says: Maths can be fun!\n2 + 2    # add two numbers\n\nNow edit the code above (e.g. change it to 2 + 3) and run it again. You should see the new result immediately."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#what-python-are-we-using",
    "href": "howto_sandbox/Introduction-Notebook.html#what-python-are-we-using",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "3 2. What Python are we using?",
    "text": "3 2. What Python are we using?\n\n# ----------------------------------------------\n# Show Python version and executable\n# ----------------------------------------------\nimport sys, platform   # import = bring tools into Python\nprint(\"Python executable:\", sys.executable)      # where Python lives in Colab\nprint(\"Python version:\", sys.version.split()[0])  # which Python version\n\n\n3.1 What are “libraries” (a.k.a. packages/modules)?\nLibraries are add-on toolboxes for Python. They provide ready-made functions so you don’t have to write everything from scratch.\n\nTerminology (you’ll see these used loosely):\n\nLibrary / package: the installable toolbox (e.g. pandas, NumPy).\nModule: a file inside a package (e.g. matplotlib.pyplot).\n\nInstall vs import:\n\nInstall (usually once per environment): makes the library available to Python.\nImport (every time you run a notebook): brings the library into your session.\n\nWhere do they “live”?\nIn your current environment/kernel (Colab’s runtime or your local conda/venv). If you install into one environment but run a notebook with a different kernel, imports will fail.\n\nCommon libraries in FB2NEP - NumPy (import numpy as np) — fast maths and arrays. - pandas (import pandas as pd) — tables (DataFrames), reading CSV/Excel, tidying. - Matplotlib (import matplotlib.pyplot as plt) — plots and figures.\nTypical workflow 1) (If needed) install\npython    # In notebooks, prefer %pip so it targets the current kernel    %pip -q install numpy pandas matplotlib 2) Import python    import numpy as np    import pandas as pd    import matplotlib.pyplot as plt 3) Use python    arr = np.array([1, 2, 3])    df = pd.DataFrame({\"x\": [1, 2, 3]})    plt.plot(df[\"x\"]); plt.show()\n\nColab note: many libraries are already installed. If you install new ones, Runtime → Restart runtime and re-run the notebook from the top.\n\nExample: how to use libraries.\n\n# Quick library demo (imports + versions)\nimport numpy as np, pandas as pd, matplotlib\nprint(\"NumPy:\", np.__version__)\nprint(\"pandas:\", pd.__version__)\nprint(\"Matplotlib:\", matplotlib.__version__)\n\n\n\n3.2 3. Variables: storing and using information\nA variable is a named container that holds a piece of information — such as text, numbers, or data.\nYou can give variables meaningful names (e.g. name, year) and then use them in commands.\nIn this example: - name = \"Jessica\" assigns a piece of text (called a string) to the variable name. - year = datetime.now().year automatically retrieves the current year from your computer or Colab environment. - print(\"Hello\", name, \"— welcome to FB2NEP\", year) combines both variables in one output line.\nVariables make your code flexible — if you change name, the message updates automatically.\n\n# ----------------------------------------------\n# Say hello with variables\n# ----------------------------------------------\nfrom datetime import datetime   # import module to get current date/time\nname = \"Jessica\"            # a text value (a 'string')\nyear = datetime.now().year  # a number\nprint(\"Hello\", name, \"— welcome to FB2NEP\", year)   # show something on the screen"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#working-with-small-collections-of-data-lists-and-simple-calculations",
    "href": "howto_sandbox/Introduction-Notebook.html#working-with-small-collections-of-data-lists-and-simple-calculations",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "4 4. Working with small collections of data: lists and simple calculations",
    "text": "4 4. Working with small collections of data: lists and simple calculations\nIn Python, a list is an ordered collection of values. It can hold numbers, text, or a mixture of both, and is written inside square brackets [ ], with items separated by commas.\nIn this example, we have a small list of numbers representing, for instance, the number of fruit portions eaten by four participants.\nWe then use a few built-in functions to calculate basic descriptive statistics:\n\nsum(nums) — adds up all numbers in the list.\n\nlen(nums) — counts how many items are in the list (its length).\n\ntotal / n — divides the total by the number of items to obtain the mean (average).\n\nprint(...) — displays the results clearly below the cell.\n\nYou can change the numbers, re-run the cell, and observe how the total and mean change accordingly.\n\n# ----------------------------------------------\n# List of numbers — total and mean\n# ----------------------------------------------\nnums = [3, 5, 8, 12]  # a list: an ordered box of values\ntotal = sum(nums)     # sum() adds up numbers\nn     = len(nums)     # Length of list = number of observations\nmean = total / n      # average = total divided by how many\nprint(\"Numbers:\", nums)\nprint(\"Total:\", total, \"Observations:\", n, \"Mean:\", mean)"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#first-steps-with-data",
    "href": "howto_sandbox/Introduction-Notebook.html#first-steps-with-data",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "5 5. First steps with data",
    "text": "5 5. First steps with data\nNumPy is the library Python uses for numbers and maths with arrays.\nAn array is like a list, but stored very efficiently — so you can do big calculations quickly.\nFor example, you can add two arrays together in one go, instead of looping through numbers.\nPandas actually uses NumPy under the hood to store and crunch the numbers inside its tables.\nPandas is a Python library that makes it easy to handle tables of data (like spreadsheets).\nThe main building block is the DataFrame, which is like a whole sheet with rows and columns.\nYou can put numbers or text inside, and then quickly look at, sort, filter, or calculate things.\nThink of pandas as your spreadsheet inside Python — but much more powerful and flexible.\n\n# ----------------------------------------------\n# Create a tiny table (DataFrame)\n# ----------------------------------------------\nimport numpy as np       # import = bring tools into Python\nimport pandas as pd\n\n# 🦛 Hippo hint: NumPy = numbers, pandas = tables\n\n# --- NumPy examples ---\n# Make a small NumPy array (like a list, but faster for maths)\narr = np.array([1, 2, 3, 4, 5])\nprint(\"Array:\", arr)\n\n# Do maths with the whole array at once\nprint(\"Array + 10:\", arr + 10)     # add 10 to each element\nprint(\"Array squared:\", arr ** 2)  # square every element\n\n\n# --- pandas example ---\n# Create a tiny DataFrame (like a mini spreadsheet)\ndf = pd.DataFrame({\n  \"age\": [8, 9, 10, 11, 12],\n  \"height_cm\": [130, 135, 140, 145, 150]\n})\n\n# Show the DataFrame\ndf\n\nChange a number in the table creation above (e.g. one of the heights) and run again. Do you see the change?"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#a-first-plot",
    "href": "howto_sandbox/Introduction-Notebook.html#a-first-plot",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "6 6. A first plot",
    "text": "6 6. A first plot\nMatplotlib is Python’s main tool for making graphs and pictures.\nIt lets you draw lines, bars, scatter plots, histograms, and much more.\nThink of it as the drawing toolbox:\n- You tell it what data to use (x and y values).\n- You can add labels, titles, and colours.\n- When you’re ready, plt.show() displays the finished picture.\n(pandas can also plot directly, but under the hood it still uses matplotlib).\n\n# ----------------------------------------------\n# Line plot of age vs height\n# ----------------------------------------------\nimport matplotlib.pyplot as plt   # plotting library\n\n# 🦛 Friendly hippo says: A picture tells a story!\nplt.plot(df[\"age\"], df[\"height_cm\"], marker=\"o\")   # draw a line with dots\nplt.xlabel(\"Age (years)\")   # label the x-axis\nplt.ylabel(\"Height (cm)\")   # label the y-axis\nplt.title(\"Example plot\")   # add a title\nplt.show()                   # display the picture"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#save-your-own-copy-important",
    "href": "howto_sandbox/Introduction-Notebook.html#save-your-own-copy-important",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "7 7. Save your own copy (important)",
    "text": "7 7. Save your own copy (important)\nGo to File → Save a copy in Drive. That creates a personal copy you can edit without affecting the teaching version."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#if-something-breaks",
    "href": "howto_sandbox/Introduction-Notebook.html#if-something-breaks",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "8 8. If something breaks",
    "text": "8 8. If something breaks\n\nRuntime → Restart runtime, then run cells again from the top.\n\nIf a library is missing, install it with !pip install ... then restart the runtime."
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#some-further-reading",
    "href": "howto_sandbox/Introduction-Notebook.html#some-further-reading",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "9 9. Some further reading",
    "text": "9 9. Some further reading\nThese are ideas you’ll meet later. It’s okay if they feel new!\n\nList: an ordered collection that you can change.\n\nTuple: like a list, but fixed once created.\n\nDictionary (dict): pairs of key: value (like a mini address book).\n\nSet: a bag of unique items, no duplicates.\n\nOOP (Object-Oriented Programming): a way to bundle data and behaviour together.\n\n\n# ----------------------------------------------\n# Advanced containers (lists, tuples, dicts, sets)\n# ----------------------------------------------\n\n# List: ordered, changeable\nfruits = [\"apple\", \"banana\", \"pear\"]\nfruits.append(\"apple\")        # duplicate allowed\n# 🦛 notice: lists keep order and allow repeats\nprint(\"List:\", fruits)\n\n# Tuple: ordered, not changeable (immutable)\ncoords = (51.44, -0.94)       # (lat, lon) for Reading-ish\nprint(\"Tuple:\", coords)\n\n# Dict: key → value mapping\nheights = {\"Alex\": 150, \"Sam\": 145}\nheights[\"Sam\"] = 146          # update a value\nprint(\"Dict:\", heights)\n\n# Set: unique items, no order\nunique_fruits = set(fruits)\nprint(\"Set (unique):\", unique_fruits)\n\n\n9.1 A tiny taste of OOP (Object-Oriented Programming)\nThink of OOP as a way to bundle data and the things you can do with that data into one unit.\n\n9.1.1 Core ideas (mapped to our example)\n\nClass → a blueprint that defines what data an object has and what it can do.\nExample: Hippo is the blueprint.\nObject / instance → a concrete thing made from a class.\nExample: h = Hippo(\"Hildegard\", favourite_food=\"lotus leaves\").\nAttributes → the data an object stores.\nExample: self.name, self.favourite_food.\nMethods → the behaviours (functions) an object can perform.\nExample: introduce(), eat(food).\n__init__ → the constructor: runs when you create a new object, initialising its attributes.\nself → the current object; it lets methods access the object’s own data.\n\n\n\n9.1.2 Why OOP can be useful\n\nEncapsulation: keep related data and actions together (tidy, easier to reason about).\nReusability: create many objects from one blueprint (e.g., lots of hippos with different names).\nExtensibility: add new methods or attributes later without breaking existing code.\n\n\n\n9.1.3 Reading the Hippo example\n\nThe class defines what every hippo has (name, favourite_food) and what every hippo does (introduce, eat).\nWhen we write h = Hippo(\"Hildegard\", ...), Python calls __init__ to set up that object’s data.\nCalling h.eat(\"cabbage\") runs the eat method on that object’s data (via self).\n\n\n\n9.1.4 When should you use OOP here?\nFor most FB2NEP notebooks, functions + data frames are enough.\nUse a small class only when it clarifies structure (e.g., simulating participants, instruments, or study arms with shared behaviour).\n\n\n\nOptional: two more OOP ideas\n\n\nInheritance: make a new class that extends another (e.g., BabyHippo(Hippo)).\nPolymorphism: different classes can share a method name but behave differently (e.g., eat for different animals).\n\nYou won’t need these for this module, but you’ll see them in larger software projects.\n\n\n\n\n9.2 Try it\n\nMake another object: h2 = Hippo(\"Fiona\"); call h2.introduce().\nChange the behaviour: inside eat, add a line to count how many times the hippo has eaten.\nAdd an attribute: e.g., age_years, and show it in introduce().\n\n\n# ----------------------------------------------\n# A tiny class: Hippo — with data and behaviours\n# ----------------------------------------------\nclass Hippo:\n  # The special __init__ method sets up new hippos\n  def __init__(self, name, favourite_food=\"water plants\"):\n    self.name = name                    # data (an attribute)\n    self.favourite_food = favourite_food\n\n  # A behaviour (a method): the hippo can introduce itself\n  def introduce(self):\n    print(f\"Hello, I'm {self.name} the hippo. I like {self.favourite_food}.\")\n\n  # Another behaviour\n  def eat(self, food):\n    if food == self.favourite_food:\n      print(f\"Yum! {self.name} loves {food}. 🦛\")\n    else:\n      print(f\"{self.name} tries {food}… not bad!\")\n\n# Make an object (an instance) from the class\nh = Hippo(\"Hildegard\", favourite_food=\"lotus leaves\")\nh.introduce()      # call a method\nh.eat(\"lotus leaves\")\nh.eat(\"cabbage\")"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#further-information-resources",
    "href": "howto_sandbox/Introduction-Notebook.html#further-information-resources",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "10 10. Further information & resources",
    "text": "10 10. Further information & resources\nIf you’d like to explore beyond this notebook:\n\n📘 Course-specific projects\nData Analysis Projects — examples, workflows, and ideas for project students.\n🌐 Official documentation:\n\nNumPy — arrays and fast maths.\n\npandas — data tables and analysis.\n\nmatplotlib — plotting and visualisation.\n\n💡 Gentle tutorials:\n\nW3Schools Python Tutorial (step-by-step basics).\n\nReal Python (clear, example-driven articles).\n\nGoogle Colab Guide (how Colab works).\n\n\n(These are optional reading — you don’t need to learn everything at once!)"
  },
  {
    "objectID": "howto_sandbox/Introduction-Notebook.html#loading-data-reference",
    "href": "howto_sandbox/Introduction-Notebook.html#loading-data-reference",
    "title": "Introduction to Colab & Python (for FB2NEP)",
    "section": "11 11. Loading data (reference)",
    "text": "11 11. Loading data (reference)\n\nGood to know: In FB2NEP teaching notebooks, data are loaded automatically. You do not need to do this yourself for the module activities. The examples below are here in case you want to practise with your own files.\n\n\n11.1 Option A — Load a small CSV from the web (GitHub raw)\nUse this for public CSVs hosted online (fastest way to test).\n\nimport pandas as pd\nurl = \"https://raw.githubusercontent.com/ggkuhnle/fb2nep-epi/main/data/synthetic/fb2nep.csv\"  # replace\ndf = pd.read_csv(url)\ndf.head()\n\n\n\n11.2 Option B — Upload a file from your computer (Colab)\nUncomment and run the lines below, then pick a CSV. Colab will store it temporarily in the session.\n\n# from google.colab import files\n# up = files.upload()                            # choose a file\n# import io\n# import pandas as pd\n# name = next(iter(up))                          # first uploaded filename\n# df = pd.read_csv(io.BytesIO(up[name]))\n# df.head()\n\n\n\n11.3 Option C — Load/save via Google Drive (persistent across Colab sessions)\nMount Drive, then read/write using a path in your Drive.\n\n# from google.colab import drive\n# drive.mount('/content/drive')\n# import pandas as pd\n# df = pd.read_csv('/content/drive/MyDrive/fb2nep/example.csv')\n# df.head()\n\n# Save back to Drive\n# df.to_csv('/content/drive/MyDrive/fb2nep/output.csv', index=False)\n\n\n\n11.4 Quick checks (useful with any dataset)\n\nWhat are the dimensions? What columns exist? Any missing values?\n\n\nprint(\"Shape:\", df.shape)\ndf.info()\ndf.isna().mean()  # fraction missing per column\n\n\n\n11.5 Optional — Save a copy locally (Colab session storage)\nFiles saved here can be downloaded from the Files pane on the left.\n\ndf.to_csv('my_output.csv', index=False)\nprint(\"Saved as my_output.csv — in Colab, open the Files pane → three dots → Download.\")"
  },
  {
    "objectID": "metadata/provenance.html",
    "href": "metadata/provenance.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "metadata/provenance.html#design-intents",
    "href": "metadata/provenance.html#design-intents",
    "title": "",
    "section": "1.1 Design intents",
    "text": "1.1 Design intents\n\nTeach realistic cohort structure with two endpoints: CVD and Cancer.\n\nProvide both incident indicators and event dates.\n\nEncode SES (ABC1/C2DE) and IMD gradients.\n\nAdd clinically relevant covariates: SBP, family history, menopausal status.\n\nInclude selected non‑linear associations (BMI U‑shape, alcohol J‑shape, SBP quadratic in age, vitamin C saturation).\n\nAdd red meat intake as a positive risk factor for Cancer above ~50 g/d."
  },
  {
    "objectID": "metadata/provenance.html#variable-generation-summary",
    "href": "metadata/provenance.html#variable-generation-summary",
    "title": "",
    "section": "1.2 Variable generation (summary)",
    "text": "1.2 Variable generation (summary)\n\nAge ~ truncated Normal(μ=58, σ=10, 40–90). Sex: 52% F, 48% M.\n\nIMD_quintile skewed to 2–4.\n\nSES_class depends on IMD.\n\nMenopausal status age‑patterned.\n\nSmoking ~15% current.\n\nPA: depends on IMD.\n\nBMI: Normal(27, 4.5), rises with age/smoking.\n\nEnergy: log‑normal around 1900 kcal, scaled by PA/sex.\n\nDiet: FV, red meat, SSB, fibre, salt depend on IMD, SES, energy.\n\nBiomarkers: plasma vitC saturates with FV; urinary Na tracks salt.\n\nSBP: non‑linear in age plus salt, BMI, smoking, PA."
  },
  {
    "objectID": "metadata/provenance.html#outcomes-and-dates",
    "href": "metadata/provenance.html#outcomes-and-dates",
    "title": "",
    "section": "1.3 Outcomes and dates",
    "text": "1.3 Outcomes and dates\n\nCVD: age, BMI (U‑shape), smoking, sex, IMD, SES, salt, SBP, alcohol J‑shape, FHx CVD. Target 10–15%.\n\nCancer: age, BMI, smoking, SES, sex, FHx cancer, red meat &gt;50 g/d. Target 8–12%.\n\nEvent dates: baseline + simulated time for incidents."
  },
  {
    "objectID": "metadata/provenance.html#missingness",
    "href": "metadata/provenance.html#missingness",
    "title": "",
    "section": "1.4 Missingness",
    "text": "1.4 Missingness\n\nMCAR: ~2–3%.\n\nMAR: +5–8% tied to IMD/age.\n\nMNAR (tiny): alcohol→alcohol missing; high BMI→BMI missing."
  },
  {
    "objectID": "metadata/provenance.html#validation-targets",
    "href": "metadata/provenance.html#validation-targets",
    "title": "",
    "section": "1.5 Validation targets",
    "text": "1.5 Validation targets\n\nCorr(FV, vitC) &gt; 0.45.\n\nCorr(salt, urinary Na) &gt; 0.55.\n\nSBP vs age Spearman ρ &gt; 0.35.\n\nSES/IMD diet gradients as expected.\n\nCVD incidence 10–15%; Cancer 8–12%.\n\nRed meat higher in cancer cases.\n\nEvent dates present only if incident=1."
  },
  {
    "objectID": "metadata/provenance.html#notes",
    "href": "metadata/provenance.html#notes",
    "title": "",
    "section": "1.6 Notes",
    "text": "1.6 Notes\nThis is teaching data: tuned for pedagogy, not inference."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html",
    "href": "notebooks/1.09_regression_modelling_02.html",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "",
    "text": "Version 0.0.6\nThis workbook builds on Workbook 6.\nIn Workbook 6, we treated regression as a practical tool for estimating associations between an exposure and an outcome, and we focused on model types (linear, logistic, Cox), assumptions, diagnostics, and basic interpretation of coefficients (β, OR, RR, HR).\nIn this workbook, we move from association to causal thinking. We introduce:\nA more formal treatment of causal inference, including modern notation and methods, is given in Workbook 9. Here we focus on intuition and on how causal structure affects regression analyses in practice.\nWe will use the synthetic FB2NEP cohort throughout. The precise variable names may differ slightly from those used here; if you obtain an error (for example, KeyError), carefully check the column names of the dataset and adapt the code accordingly.\n# ============================================================\n# FB2NEP bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the fb2nep-epi repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads and runs scripts/bootstrap.py.\n# - Makes the main dataset available as the variable `df`.\n#\n# Important:\n# - You may see messages printed below (for example from pip\n#   or from the bootstrap script). This is expected.\n# - You may also see WARNINGS (often in yellow). In most cases\n#   these are harmless and can be ignored for this module.\n# - The main thing to watch for is a red error traceback\n#   (for example FileNotFoundError, ModuleNotFoundError).\n#   If that happens, please re-run this cell first. If the\n#   error persists, ask for help.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\nimport importlib.util\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\n# REPO_URL: address of the GitHub repository.\n# REPO_DIR: folder name that will be created when cloning.\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the fb2nep-epi repository\n# ------------------------------------------------------------\n# In local Jupyter, you may already be inside the repository,\n# for example in fb2nep-epi/notebooks.\n#\n# In Colab, the default working directory is /content, so\n# we need to clone the repository into /content/fb2nep-epi\n# and then change into that folder.\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/bootstrap.py exists here)\nif (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n    repo_root = cwd\n\n# Case B: we are outside the repository (for example in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if it is not present yet\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change the working directory to the repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nprint(f\"Repository root set to: {repo_root}\")\n\n# ------------------------------------------------------------\n# 2. Load scripts/bootstrap.py as a module and call init()\n# ------------------------------------------------------------\n# The shared bootstrap script contains all logic to:\n# - Ensure that required Python packages are installed.\n# - Ensure that the synthetic dataset exists (and generate it\n#   if needed).\n# - Load the dataset into a pandas DataFrame.\n#\n# We load the script as a normal Python module (fb2nep_bootstrap)\n# and then call its init() function.\nbootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n\nif not bootstrap_path.is_file():\n    raise FileNotFoundError(\n        f\"Could not find {bootstrap_path}. \"\n        \"Please check that the fb2nep-epi repository structure is intact.\"\n    )\n\n# Create a module specification from the file\nspec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\nbootstrap = importlib.util.module_from_spec(spec)\nsys.modules[\"fb2nep_bootstrap\"] = bootstrap\n\n# Execute the bootstrap script in the context of this module\nspec.loader.exec_module(bootstrap)\n\n# The init() function is defined in scripts/bootstrap.py.\n# It returns:\n# - df   : the main synthetic cohort as a pandas DataFrame.\n# - CTX  : a small context object with paths, flags and settings.\ndf, CTX = bootstrap.init()\n\n# Optionally expose a few additional useful variables from the\n# bootstrap module (if they exist). These are not essential for\n# most analyses, but can be helpful for advanced use.\nfor name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n    if hasattr(bootstrap, name):\n        globals()[name] = getattr(bootstrap, name)\n\nprint(\"Bootstrap completed successfully.\")\nprint(\"The main dataset is available as the variable `df`.\")\nprint(\"The context object is available as `CTX`.\")\n\"\"\"\nImports and quick inspection\n============================\n\nIn this cell we:\n\n- Import common packages used in this workbook.\n- Display basic information about the dataset to confirm that it is loaded.\n\nThe imports are deliberately explicit. Many students using this workbook\nwill not yet have much experience with Python, so we avoid implicit\nmagic and keep the code readable.\n\"\"\"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom IPython.display import display\nfrom scipy.stats import norm\n\nfrom scripts.helpers_tables import or_from_linear_combination\n\nprint(\"DataFrame shape (rows, columns):\", df.shape)\nprint(\"\\nFirst five rows of the dataset:\")\ndisplay(df.head())\n\nprint(\"\\nVariable types (first 20 columns):\")\ndisplay(df.dtypes.head(20))"
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#association-and-causation",
    "href": "notebooks/1.09_regression_modelling_02.html#association-and-causation",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "0.1 1. Association and causation",
    "text": "0.1 1. Association and causation\nRegression models (linear, logistic, Cox) quantify associations:\n\nIn linear regression, a coefficient describes how the mean outcome changes with the exposure.\nIn logistic regression, we obtain odds ratios.\nIn Cox regression, we obtain hazard ratios.\n\nHowever, public health decisions concern causal effects:\n\nIf we changed this exposure (for example, salt intake), what would happen to the outcome?\n\nIn observational data, a non-zero regression coefficient does not automatically imply a causal effect. Several types of variables can distort or create associations:\n\nConfounders: common causes of exposure and outcome that, if unadjusted, bias estimated effects.\nColliders: variables that are caused by two other variables; conditioning on them can create spurious associations.\nMediators: variables lying on the causal pathway; adjusting for them can remove part of a genuine effect.\n\nTo move from association to causation we need to consider the causal structure of the variables. In this workbook we introduce DAGs and basic adjustment strategies. Workbook 9 provides a more formal framework for causal inference."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#dags-for-model-development-and-identification-of-confounders-colliders-and-mediators",
    "href": "notebooks/1.09_regression_modelling_02.html#dags-for-model-development-and-identification-of-confounders-colliders-and-mediators",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "0.2 2. DAGs for model development and identification of confounders, colliders, and mediators",
    "text": "0.2 2. DAGs for model development and identification of confounders, colliders, and mediators\nA directed acyclic graph (DAG) is a diagram with arrows that represents assumptions about which variables cause which. It is:\n\nDirected: arrows have a direction (cause → effect).\nAcyclic: there are no feedback loops.\n\nDAGs are useful because they make assumptions explicit and allow us to reason about which variables we should adjust for in regression models.\n\n0.2.1 2.1 Constructing a DAG\nTo construct a DAG for a research question:\n\nList key variables\n\nExposure (for example, red meat intake).\nOutcome (for example, incident cancer).\nPlausible causes of exposure and outcome (for example, socio-economic status, age, smoking).\n\nDraw arrows according to subject-matter knowledge\n\nIf variable A can plausibly influence variable B, draw A → B.\nDo not add arrows simply because two variables are correlated in the data.\n\nIdentify paths between exposure and outcome\n\nCausal paths (exposure → … → outcome).\nNon-causal “backdoor” paths (exposure ← … → outcome) that create confounding.\n\nDecide on an adjustment set\n\nChoose variables to adjust for so that all non-causal backdoor paths are blocked, without conditioning on colliders or mediators.\n\n\n\n\n\nDAG\n\n\n\n\n0.2.2 2.2 Why we do not include everything\nIt might be tempting to adjust for every available variable. This is usually a bad idea because:\n\nAdjusting for colliders can create bias.\nAdjusting for mediators can remove part of the effect we are interested in (for example, when estimating the total effect of an exposure).\nAdjusting for variables that are neither confounders nor mediators can increase variance and complicate interpretation without reducing bias.\n\nA good DAG includes enough variables to capture the main causal structure, but not every variable in the dataset. We use subject-matter knowledge and parsimony: include variables that are plausible causes of exposure and outcome, and that are important for the research question."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#confounders",
    "href": "notebooks/1.09_regression_modelling_02.html#confounders",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "0.3 3. Confounders",
    "text": "0.3 3. Confounders\n\n0.3.1 3.1 Definition and informal examples\nA variable is a confounder for the association between an exposure and an outcome if:\n\nIt is associated with the exposure.\nIt is a cause of, or associated with a cause of, the outcome.\nIt is not on the causal pathway from exposure to outcome.\n\nIntuitively, a confounder is a variable that makes exposed and unexposed individuals systematically different, in a way that also affects the outcome.\nClassic informal examples include:\n\nNumber of children and BRCA risk: women with more children tend to be older, and age affects the probability of having developed breast cancer; age can confound the association between “number of children” and “current breast cancer status”.\nHair length and income: there may appear to be an association between hair length and income if, in a given setting, women tend to have longer hair than men and also have different average incomes; sex is a confounder.\n\n\n\n0.3.2 Hippo example (confounding)\nHippo size and daily grass intake\nSuppose we observe that larger hippos eat more grass per day.\nWe might be tempted to conclude that being large makes hippos eat more.\nBut consider the underlying biology:\n\nOlder hippos tend to be larger (simply through growth).\nOlder hippos also spend more hours grazing because they no longer play in the water as much as juveniles.\n\nAge is therefore a confounder:\n       age\n      /    \\\n hippo size  grass intake\nIn causal terms:\nSize ← Age → Grass intake\nIf we ignore age, the association between hippo size and grass intake partly reflects the fact that older hippos both weigh more and eat more, not necessarily that body size itself increases grazing behaviour.\n\n\n0.3.3 3.2 Approaches to adjustment\nThere are two basic ways to adjust for confounders:\n\nStratification\n\nAnalyse the exposure–outcome association within levels of the confounder.\nFor example, estimate the association separately in high and low socio-economic status (SES) groups.\n\nInclusion in a regression model\n\nInclude the confounder as a covariate (predictor) in the regression model.\nFor a linear model: \\[\nY = \\beta_0 + \\beta_1 X + \\beta_2 C + \\varepsilon,\n\\] where $ X $ is the exposure and $ C $ is the confounder.\nThe coefficient $ _1 $ is then interpreted as the association between $ X $ and $ Y $ for individuals with the same value of $ C $.\n\n\nIn practice, regression models with appropriate covariates are the most common approach, but stratified analyses are useful for checking assumptions and for illustrating confounding.\nIn the next section we use the FB2NEP cohort to demonstrate confounding in a setting where the data-generating mechanism includes known confounders."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#example-physical-activity-ses-and-incident-cvd",
    "href": "notebooks/1.09_regression_modelling_02.html#example-physical-activity-ses-and-incident-cvd",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "0.4 3.3 Example: physical activity, SES, and incident CVD",
    "text": "0.4 3.3 Example: physical activity, SES, and incident CVD\nWe now consider a concrete example using the FB2NEP cohort.\n\nExposure: physical activity (physical_activity).\nOutcome: incident cardiovascular disease during follow-up (CVD_incident).\nPotential confounder: socio-economic status (SES_class).\n\nFrom the data generator we know that:\n\nPhysical activity (physical_activity) is socially patterned: higher SES and less deprivation are associated with higher activity.\nCVD risk is affected by several factors related to SES (for example, smoking, SBP, BMI, diet).\n\nA simplified DAG is:\nSES_class  →  physical_activity\nSES_class  →  CVD_incident\nphysical_activity  →  CVD_incident   (modest protective effect)\nHere, SES_class is a confounder: it influences both physical activity and CVD. If we estimate the association between physical activity and CVD without adjusting for SES, the result may be biased. We therefore:\n\nCreate a binary indicator of “high” physical activity.\nFit a crude logistic regression model of CVD on high activity.\nFit an SES-adjusted model.\nCompare the results and repeat the analysis stratified by SES.\n\n\nfrom scripts.helpers_tables import summarise_logit_coef\n\nOUTCOME_VAR = \"CVD_incident\"\nEXPOSURE_RAW = \"physical_activity\"      # categorical: low / moderate / high\n    \nCONF_VAR = \"SES_class\"                  # ABC1 / C2DE\n\n# ---------------------------------------------------------------------\n# 1. Check variables and prepare analysis dataset\n# ---------------------------------------------------------------------\nfor v in [OUTCOME_VAR, EXPOSURE_RAW, CONF_VAR]:\n    if v not in df.columns:\n        raise KeyError(\n            f\"Variable '{v}' not found in df. \"\n            f\"Available columns (first 20): {list(df.columns)[:20]}\"\n        )\n\n# We define high physical activity as the exposure of interest\ndf_pa = df[[OUTCOME_VAR, EXPOSURE_RAW, CONF_VAR]].dropna().copy()\ndf_pa[\"highPA\"] = (df_pa[EXPOSURE_RAW] == \"high\").astype(int)\n\nprint(f\"Complete-case sample size: {df_pa.shape[0]} observations\\n\")\nprint(\"Distribution of physical_activity and SES_class:\\n\")\ndisplay(pd.crosstab(df_pa[EXPOSURE_RAW], df_pa[CONF_VAR], normalize=\"columns\"))\n\n# ---------------------------------------------------------------------\n# 2. Crude and SES-adjusted logistic regression models\n# ---------------------------------------------------------------------\nformula_crude = f\"{OUTCOME_VAR} ~ highPA\"\nformula_adj   = f\"{OUTCOME_VAR} ~ highPA + C({CONF_VAR})\"\n\nm_crude = smf.logit(formula_crude, data=df_pa).fit(disp=False)\nm_adj   = smf.logit(formula_adj,   data=df_pa).fit(disp=False)\n\nrows = []\nrows.append(\n    summarise_logit_coef(\n        m_crude,\n        var_name=\"highPA\",\n        label=\"Crude model (no SES adjustment)\"\n    )\n)\nrows.append(\n    summarise_logit_coef(\n        m_adj,\n        var_name=\"highPA\",\n        label=\"Adjusted model (including SES_class)\"\n    )\n)\n\nsummary_pa = pd.DataFrame(rows)\n\nprint(\"\\nHigh physical activity vs incident CVD: crude and SES-adjusted models\\n\")\ndisplay(summary_pa.round(3))\n\n\n0.4.1 Interpreting crude vs SES-adjusted models (physical activity)\nThe table summarises the association between high physical activity (highPA) and incident CVD:\n\nThe crude model compares high vs non-high physical activity without adjustment for SES.\nThe adjusted model includes SES_class as a covariate.\n\nTypical patterns to look for:\n\nIf high physical activity is genuinely protective, you might expect an OR &lt; 1.\nBecause SES is associated with both physical activity and CVD, failing to adjust for SES can bias the crude OR towards or away from 1.\n\nIn this synthetic cohort you will usually find that:\n\nThe crude OR for highPA is only modestly below 1 (weak protective effect).\nThe SES-adjusted OR is somewhat further from 1 (stronger apparent protection).\n\nThis is consistent with positive confounding: high SES participants tend to be more physically active and at lower CVD risk for other reasons. When SES is left unadjusted, part of this benefit is incorrectly attributed to physical activity. Once SES is included in the model, the association for highPA more closely reflects the effect of physical activity for individuals with the same SES.\n\n\n0.4.2 3.4 Stratification by SES\nRegression adjustment is one way to account for confounding. Another is to analyse the exposure–outcome association within strata of the confounder.\nHere we:\n\nSplit the data into two strata: SES_class = ABC1 and SES_class = C2DE.\nIn each stratum, fit a logistic regression model: CVD_incident ~ highPA\nCompare the stratum-specific odds ratios.\n\nIf SES is a confounder rather than an effect modifier, we would expect the stratum-specific ORs for highPA to be more similar to each other and to the SES-adjusted OR, and different from the crude OR.\n\n\"\"\"Stratified analysis: highPA and CVD_incident within SES strata.\"\"\"\n\nrows_strata = []\n\nprint(\"Stratified analyses by SES_class (separate models in each stratum):\\n\")\n\nfor level in sorted(df_pa[CONF_VAR].unique()):\n    df_stratum = df_pa[df_pa[CONF_VAR] == level]\n    print(f\"  Stratum {level}: n = {df_stratum.shape[0]}\")\n\n    # Need variation in exposure and outcome\n    \n    if df_stratum[OUTCOME_VAR].nunique() &lt; 2 or df_stratum[\"highPA\"].nunique() &lt; 2:\n        print(\"    Not enough variation in outcome or exposure for logistic regression.\\n\")\n        continue\n\n    m_stratum = smf.logit(f\"{OUTCOME_VAR} ~ highPA\", data=df_stratum).fit(disp=False)\n    rows_strata.append(\n        summarise_logit_coef(\n            m_stratum,\n            var_name=\"highPA\",\n            label=f\"SES_class = {level}\"\n        )\n    )\n\nif rows_strata:\n    summary_strata_pa = pd.DataFrame(rows_strata)\n    print(\"\\nStratum-specific odds ratios for highPA (by SES_class):\\n\")\n    display(summary_strata_pa.round(3))\n\n\n\n0.4.3 Interpreting the SES-stratified highPA–CVD associations\nThe table above shows the association between high physical activity (highPA) and incident CVD within strata of socio-economic status (SES_class):\n\nABC1\n\nOR ≈ 1.05 (95 % CI 0.91 to 1.20), p ≈ 0.52\n\nPoint estimate slightly above 1.0, but the confidence interval is wide and clearly includes 1.0 → compatible with no association.\n\nC2DE\n\nOR ≈ 0.89 (95 % CI 0.76 to 1.03), p ≈ 0.11\n\nPoint estimate slightly below 1.0, again with a confidence interval that includes 1.0 → also compatible with no association.\n\n\nA few points to emphasise:\n\nThe effect sizes are small in both strata, and statistically weak.\nThere is no strong evidence that high physical activity is clearly protective or clearly harmful for CVD in either SES group in this synthetic cohort.\nThe directions differ slightly (OR &gt; 1 in ABC1, OR &lt; 1 in C2DE), but the confidence intervals are wide and largely overlapping. This means that the apparent difference between strata is easily explained by random variation.\nAs a worked example, this is closer to “what often happens in practice” than to a textbook Simpson’s paradox:\nadjusting or stratifying can change effect estimates a little, but not every plausible confounder produces a dramatic shift.\n\nFor here, key messages are:\n\nDo not over-interpret small differences in odds ratios when confidence intervals are wide and overlapping.\nStratified analyses are still useful: they make us look at whether the exposure–outcome association is similar across subgroups, and they remind us that confounding and effect modification are empirical questions, not assumptions built into the model."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#special-case-energy-intake-in-nutritional-epidemiology",
    "href": "notebooks/1.09_regression_modelling_02.html#special-case-energy-intake-in-nutritional-epidemiology",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "0.5 4. Special case: energy intake in nutritional epidemiology",
    "text": "0.5 4. Special case: energy intake in nutritional epidemiology\n\n0.5.1 4.1 Why total energy intake is different\nIn nutritional epidemiology, total energy intake (for example, energy_kcal) is not a classical confounder in the usual sense. Instead, it is a kind of “scaling” variable:\n\nIndividuals who eat more total energy tend to consume more of many nutrients and foods simply because they eat more food.\nMany nutrients are also biologically related to energy intake (for example, higher energy intake is often associated with higher body size and physical activity).\n\nIf we ignore total energy intake, we may incorrectly attribute the effect of “eating more food overall” to a specific nutrient or food.\n\n\n0.5.2 4.2 Common energy-adjustment methods\nSeveral approaches are used to adjust nutrient intakes for total energy:\n\nNutrient density method\n\nExpress the nutrient per unit of energy, for example g/MJ or % of energy.\nExample: grams of fibre per 10 MJ.\n\nResidual method\n\nRegress the nutrient of interest on total energy intake.\nUse the residuals (observed minus expected nutrient intake given energy) as an energy-adjusted exposure.\nThis removes the part of the nutrient intake that is explained by total energy intake.\n\nEnergy-adjusted models\n\nInclude both the nutrient and total energy intake as covariates in the regression model of interest.\n\n\nEach method has advantages and disadvantages. The residual method and energy-adjusted models are particularly useful when working with food-frequency questionnaires (FFQs), where measurement error and strong correlations between nutrients can be substantial.\n\n\n0.5.3 4.3 Special case of FFQs\nFFQs typically record relative frequencies of consumption over long periods. Reported intakes of many foods and nutrients are highly correlated, and systematic measurement error is common. Adjusting for total energy intake can:\n\nReduce measurement error that is common to many foods (for example, general over-reporting or under-reporting).\nFocus analyses on diet composition rather than total amount of food."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#colliders-and-mediators",
    "href": "notebooks/1.09_regression_modelling_02.html#colliders-and-mediators",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "0.6 5. Colliders and mediators",
    "text": "0.6 5. Colliders and mediators\n\n0.6.1 5.1 Colliders\nA collider is a variable that is caused by two (or more) other variables. In a simple diagram:\nexercise  →\n            fitness\ngenes     →\nHere, fitness is a collider on the path between exercise and genes. If we condition on fitness (for example, by restricting the analysis to individuals with high fitness, or adjusting for fitness in a model), we can induce an association between exercise and genes even if none exists in the population.\nThis is known as collider bias or selection bias when the collider is related to being included in the study.\n\n\n0.6.2 5.2 Mediators\nA mediator lies on the causal pathway between exposure and outcome:\nsalt intake → blood pressure → stroke\nIf we are interested in the total effect of salt intake on stroke risk, we should not adjust for blood pressure, because this would remove part of the genuine effect (the indirect pathway through blood pressure).\nIf we are specifically interested in the direct effect of salt that is not mediated by blood pressure, then adjusting for blood pressure is appropriate, but the interpretation changes.\nThe key message is that we should adjust for confounders, avoid adjusting for colliders, and think carefully before adjusting for mediators. DAGs help us to reason about which variables fall into which category.\n\n\n0.6.3 5.3 Example of mediation: salt, SBP, and incident CVD\nWe return to the example of salt intake and CVD, now focusing on mediation.\n\nExposure: salt_g_d (daily salt intake).\nMediator: SBP (systolic blood pressure).\nOutcome: CVD_incident.\n\nA simple DAG is:\nsalt_g_d  →  SBP  →  CVD_incident\nSalt has a modest direct effect on CVD in the data generator, but the main pathway is through raising SBP. We compare two models:\n\nCVD_incident ~ salt_g_d (total effect: salt → CVD, including via SBP)\nCVD_incident ~ salt_g_d + SBP (direct effect: salt → CVD, holding SBP constant)\n\nBy comparing the odds ratios for salt_g_d in these two models, we can see how adjusting for a mediator changes the estimand.\n\n\"\"\"Mediation example: salt_g_d → SBP → CVD_incident.\"\"\"\n\nOUTCOME_SALT = \"CVD_incident\"\nEXPOSURE_SALT = \"salt_g_d\"\nMEDIATOR_SBP = \"SBP\"\n\nfor v in [OUTCOME_SALT, EXPOSURE_SALT, MEDIATOR_SBP]:\n    if v not in df.columns:\n        raise KeyError(f\"Variable '{v}' not found in df.\")\n\ndf_salt = df[[OUTCOME_SALT, EXPOSURE_SALT, MEDIATOR_SBP]].dropna().copy()\nprint(f\"Complete-case sample size: {df_salt.shape[0]} observations\\n\")\n\n# Crude (total-effect-oriented) model\nm_salt_crude = smf.logit(f\"{OUTCOME_SALT} ~ {EXPOSURE_SALT}\", data=df_salt).fit(disp=False)\n\n# SBP-adjusted (direct-effect-oriented) model\nm_salt_adj = smf.logit(\n    f\"{OUTCOME_SALT} ~ {EXPOSURE_SALT} + {MEDIATOR_SBP}\",\n    data=df_salt\n).fit(disp=False)\n\nrows_salt = []\nrows_salt.append(\n    summarise_logit_coef(\n        m_salt_crude,\n        var_name=EXPOSURE_SALT,\n        label=\"Crude model (CVD_incident ~ salt_g_d)\"\n    )\n)\nrows_salt.append(\n    summarise_logit_coef(\n        m_salt_adj,\n        var_name=EXPOSURE_SALT,\n        label=\"SBP-adjusted model (CVD_incident ~ salt_g_d + SBP)\"\n    )\n)\n\nsummary_salt_med = pd.DataFrame(rows_salt)\n\nprint(\"Salt intake and incident CVD: crude vs SBP-adjusted models\\n\")\ndisplay(summary_salt_med.round(3))\n\n\n\n0.6.4 Interpreting the mediation example (salt and SBP)\nIn this synthetic dataset you will typically find that:\n\nThe crude OR for salt_g_d is close to 1 (weak or no association).\nThe SBP-adjusted OR is also close to 1, sometimes slightly below or above.\n\nThis reflects how the data generator was constructed:\n\nSalt has a small direct effect on CVD.\nMost of the effect of salt operates through SBP.\nWhen we adjust for SBP, we remove the mediation pathway and focus on the direct effect.\n\nEven though the numerical effect is small, the example illustrates the key conceptual point:\n\nIf our target is the total effect of salt on CVD, we should not adjust for SBP.\nIf our target is the direct effect of salt, holding SBP constant, then we should adjust for SBP, but we must interpret the result as a direct effect.\n\nThis is different from the confounding example with SES_class, where adjustment reduces bias in estimating the causal effect of physical activity on CVD.\n\n\n0.6.5 5.4 Predicted probability of incident CVD across salt intake\nTo make the mediation example more concrete, we can translate the SBP-adjusted logistic model into predicted probabilities.\nWe:\n\nUse the SBP-adjusted logistic model.\nFix SBP at a reference value (for example, the median SBP).\nVary daily salt intake (salt_g_d) across its observed range.\nPlot the predicted probability of incident CVD.\n\nThis visualises how the model predicts CVD risk to change with salt intake, conditional on SBP being held constant. It emphasises that we are now looking at the direct effect of salt, not the total effect including its influence through SBP.\n\n\"\"\"Predicted probability of incident CVD across salt intake (SBP fixed).\"\"\"\n\n# Choose a reference value for SBP (for example, the median)\nsbp_ref = df_salt[MEDIATOR_SBP].median()\n\n# Construct a grid of salt values over the central range\nsalt_grid = np.linspace(\n    df_salt[EXPOSURE_SALT].quantile(0.05),\n    df_salt[EXPOSURE_SALT].quantile(0.95),\n    100\n)\n\npred_df_salt = pd.DataFrame({\n    EXPOSURE_SALT: salt_grid,\n    MEDIATOR_SBP: sbp_ref,\n})\n\npred_df_salt[\"p_cvd\"] = m_salt_adj.predict(pred_df_salt)\n\nfig, ax = plt.subplots(figsize=(6, 4))\n\nax.plot(pred_df_salt[EXPOSURE_SALT], pred_df_salt[\"p_cvd\"], linewidth=2)\n\nax.set_xlabel(\"Salt intake (g/day)\")\nax.set_ylabel(\"Predicted probability of incident CVD\")\nax.set_title(\n    \"Adjusted logistic model: CVD_incident ~ salt_g_d + SBP\\n\"\n    f\"(SBP fixed at {sbp_ref:.1f} mmHg)\"\n)\n\nplt.tight_layout()\nplt.show()\n\n\n\n0.6.6 Interpreting the predicted probability curve (salt, SBP fixed)\nIn this synthetic dataset the predicted probability curve is typically almost flat, with at most a slight upward or downward slope as salt intake increases.\nThis is expected because:\n\nSBP carries most of the effect of salt on CVD in the data generator.\nWhen SBP is fixed at a reference value, we remove the main pathway by which salt influences CVD.\nWhat remains is a small direct effect plus any residual correlation with other variables.\n\nThe key message is not the exact shape of the curve, but the change of estimand:\n\nBy conditioning on SBP, we are no longer looking at the total effect of salt on CVD, but at the risk pattern given equal SBP.\n\n\nDifferent models answer different causal questions. Prediction curves are useful for visualising these differences, but they must always be interpreted in the light of the assumed causal structure (here, salt → SBP → CVD)."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#interaction-and-effect-modification",
    "href": "notebooks/1.09_regression_modelling_02.html#interaction-and-effect-modification",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "0.7 6. Interaction and effect modification",
    "text": "0.7 6. Interaction and effect modification\nSo far we have mainly treated covariates as confounders: variables that we adjust for to obtain a less biased estimate of the exposure–outcome association.\nSometimes we are interested in whether the effect of an exposure is different in different groups. This is called effect modification (or interaction).\nExamples in nutritional epidemiology include:\n\nDoes the association between physical activity and CVD differ by SES?\nIs the association between salt intake and CVD stronger in people with pre-existing hypertension?\nDoes the association between alcohol and outcomes differ between men and women?\n\nIn a regression model, we can represent effect modification using an interaction term. In a logistic regression model:\n\\[\n\\log\\left(\\frac{P(Y=1)}{P(Y=0)}\\right)\n  = \\beta_0 + \\beta_1 X_{\\text{PA}} + \\beta_2 X_{\\text{SES}} + \\beta_3 (X_{\\text{PA}} \\times X_{\\text{SES}}),\n\\]\nwhere, for example:\n\n$ X_{} $ is an indicator of high physical activity (highPA).\n$ X_{} $ is an indicator of C2DE vs ABC1.\n\nThen:\n\n$ _1 $ describes the log-odds ratio for high vs non-high physical activity in the reference SES group (for example, ABC1).\n$ _1 + _3 $ describes the log-odds ratio for high vs non-high physical activity in the other SES group (for example, C2DE).\n$ _3 $ itself is the difference in log-odds ratios between SES groups.\n\nIf $ _3 = 0 $, then the effect of physical activity on CVD is the same in both SES groups (on the odds ratio scale). If (_3 ), there is multiplicative interaction between physical activity and SES.\nIn practice we often:\n\nFit a model with an interaction term.\nDerive and report the group-specific odds ratios.\nTest the interaction term (for example, using a Wald test).\n\nBelow we extend the earlier physical activity–SES example by adding an interaction term and computing SES-specific odds ratios for high physical activity.\n\n# 6. Interaction between physical activity and SES (highPA × SES_class)\n#\n# We reuse the df_pa dataset defined earlier:\n# - OUTCOME_VAR = \"CVD_incident\"\n# - EXPOSURE_RAW = \"physical_activity\" (used to define highPA)\n# - CONF_VAR     = \"SES_class\" (ABC1 / C2DE)\n#\n# The model is:\n#     CVD_incident ~ highPA * C(SES_class)\n#\n# This allows the association between high physical activity and CVD to differ\n# between SES groups.\n\n\n# Safety check: ensure df_pa and required variables exist\nrequired_cols = {OUTCOME_VAR, \"highPA\", CONF_VAR}\nmissing_cols = required_cols.difference(df_pa.columns)\nif missing_cols:\n    raise KeyError(\n        f\"The following required columns are missing from df_pa: {missing_cols}. \"\n        \"Please run the earlier confounding example cell first.\"\n    )\n\n# Fit logistic regression model with interaction term\nformula_int = f\"{OUTCOME_VAR} ~ highPA * C({CONF_VAR})\"\nm_int = smf.logit(formula_int, data=df_pa).fit(disp=False)\n\nprint(\"Logistic model with interaction: \"\n      f\"{OUTCOME_VAR} ~ highPA * C({CONF_VAR})\\n\")\n\n# Extract coefficients and covariance matrix\nparams = m_int.params\ncov = m_int.cov_params()\n\n# SES levels (we assume a binary SES variable here)\nses_levels = sorted(df_pa[CONF_VAR].dropna().unique())\nif len(ses_levels) != 2:\n    raise ValueError(\n        f\"This example assumes exactly two {CONF_VAR} levels; found: {ses_levels}.\"\n    )\n\nref_ses = ses_levels[0]   # reference level used by C(SES_class)\nother_ses = ses_levels[1]\n\ninteraction_term = f\"highPA:C({CONF_VAR})[T.{other_ses}]\"\nif interaction_term not in params.index:\n    raise KeyError(\n        f\"Could not find interaction term '{interaction_term}' in model parameters.\\n\"\n        f\"Available parameters: {list(params.index)}\"\n    )\n\nrows = []\n\n# 1) Reference SES group (e.g. ABC1): uses the main highPA coefficient\n(\n    beta_ref,\n    ci_l_ref,\n    ci_u_ref,\n    p_ref,\n    OR_ref,\n    OR_l_ref,\n    OR_u_ref,\n) = or_from_linear_combination(\n    coeff_names=[\"highPA\"],\n    weights=[1.0],\n    params=params,\n    cov=cov,\n)\n\nrows.append(\n    {\n        \"SES_class\": ref_ses,\n        \"beta\": beta_ref,\n        \"ci_lower\": ci_l_ref,\n        \"ci_upper\": ci_u_ref,\n        \"p_value\": p_ref,\n        \"OR\": OR_ref,\n        \"OR_ci_lower\": OR_l_ref,\n        \"OR_ci_upper\": OR_u_ref,\n        \"note\": \"Reference SES group (no interaction term added)\",\n    }\n)\n\n# 2) Other SES group (e.g. C2DE): main effect + interaction term\n(\n    beta_other,\n    ci_l_other,\n    ci_u_other,\n    p_other,\n    OR_other,\n    OR_l_other,\n    OR_u_other,\n) = or_from_linear_combination(\n    coeff_names=[\"highPA\", interaction_term],\n    weights=[1.0, 1.0],\n    params=params,\n    cov=cov,\n)\n\nrows.append(\n    {\n        \"SES_class\": other_ses,\n        \"beta\": beta_other,\n        \"ci_lower\": ci_l_other,\n        \"ci_upper\": ci_u_other,\n        \"p_value\": p_other,\n        \"OR\": OR_other,\n        \"OR_ci_lower\": OR_l_other,\n        \"OR_ci_upper\": OR_u_other,\n        \"note\": \"Main effect + interaction term\",\n    }\n)\n\nsummary_int = pd.DataFrame(rows)\n\n# 3) The interaction term itself (difference in log-ORs between SES groups)\nbeta_int = params[interaction_term]\nse_int = np.sqrt(cov.loc[interaction_term, interaction_term])\nz_crit = 1.96\nci_l_int = beta_int - z_crit * se_int\nci_u_int = beta_int + z_crit * se_int\n# p-value is less central here for teaching, so we skip computing it again\n\ninteraction_summary = pd.Series(\n    {\n        \"beta\": beta_int,\n        \"ci_lower\": ci_l_int,\n        \"ci_upper\": ci_u_int,\n    },\n    name=\"highPA × SES_class\",\n)\n\nprint(\"SES-specific odds ratios for high physical activity \"\n      \"(from interaction model):\\n\")\ndisplay(summary_int.round(3))\n\n\n\n\n\nprint(\"\\nInteraction term (difference in log-ORs between SES groups):\\n\")\ndisplay(interaction_summary.to_frame().round(3))"
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#what-an-interaction-term-represents",
    "href": "notebooks/1.09_regression_modelling_02.html#what-an-interaction-term-represents",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "1.1 1. What an interaction term represents",
    "text": "1.1 1. What an interaction term represents\nAn interaction term allows the effect of an exposure to differ across levels of another variable.\nIn this model:\n\nhighPA captures the association between high physical activity and CVD\nin the reference SES group (e.g. ABC1).\nSES_class captures the difference in baseline CVD risk between SES groups.\nhighPA × SES_class captures whether the association between physical activity and CVD differs between SES groups.\n\nIn other words, the interaction term answers:\n\nIs the association between physical activity and CVD the same in ABC1 and C2DE,\nor does it differ?"
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#how-to-read-the-interaction-coefficient",
    "href": "notebooks/1.09_regression_modelling_02.html#how-to-read-the-interaction-coefficient",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "1.2 2. How to read the interaction coefficient",
    "text": "1.2 2. How to read the interaction coefficient\nOn the log-odds (logit) scale:\n\nA zero interaction coefficient means the effect of high physical activity\nis the same in both SES groups.\nA positive coefficient means the association is stronger (more protective or more harmful, depending on direction) in the non-reference SES group.\nA negative coefficient means the association is weaker in the non-reference SES group.\n\nWhen exponentiated, the interaction coefficient becomes the ratio of odds ratios:\n\\[\n\\exp(\\beta_{\\text{interaction}})  \n  = \\frac{\\text{OR(highPA in non-ref SES)}}{\\text{OR(highPA in ref SES)}}.\n\\]\nValues:\n\n= 1 → no difference\n\n&gt; 1 → stronger association in the non-reference group\n\n&lt; 1 → weaker association in the non-reference group"
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#how-the-interaction-affects-group-specific-effects",
    "href": "notebooks/1.09_regression_modelling_02.html#how-the-interaction-affects-group-specific-effects",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "1.3 3. How the interaction affects group-specific effects",
    "text": "1.3 3. How the interaction affects group-specific effects\nThe model estimates:\n\nEffect of highPA in the reference SES group:\n\\[\n\\beta_{\\text{highPA}}.\n\\]\nEffect of highPA in the other SES group:\n\\[\n\\beta_{\\text{highPA}} + \\beta_{\\text{interaction}}.\n\\]\n\nThus the interaction term determines how much the highPA effect changes between SES strata.\nThis is the key reason we need the interaction term: without it, the model assumes the effect is identical in all SES groups."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#confidence-intervals-and-interpretation",
    "href": "notebooks/1.09_regression_modelling_02.html#confidence-intervals-and-interpretation",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "1.4 4. Confidence intervals and interpretation",
    "text": "1.4 4. Confidence intervals and interpretation\nWhen interpreting an interaction:\n\nA confidence interval that includes zero (on the log scale) or includes 1 (on the OR scale) implies that the data are compatible with no interaction.\nInteraction terms typically require large samples to detect modest differences.\nAbsence of statistical evidence for interaction does not prove the effects are identical; it simply indicates we cannot conclude they differ.\n\nFor teaching purposes:\nInteractions are often noisier and harder to estimate than main effects. This is normal and not a sign of a mistake."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#practical-takeaway-for-applied-epidemiology",
    "href": "notebooks/1.09_regression_modelling_02.html#practical-takeaway-for-applied-epidemiology",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "1.5 5. Practical takeaway for applied epidemiology",
    "text": "1.5 5. Practical takeaway for applied epidemiology\nWhen students encounter an interaction term:\n\nIdentify which group is the reference for SES.\n\nInterpret the main effect of highPA as applying to that reference group.\n\nAdd the interaction term to obtain the effect in the other SES group.\n\nUse the confidence interval of the interaction to judge whether a meaningful difference between groups is supported by the data.\n\nAvoid over-interpreting small, non-significant interactions.\n\nIn nutritional epidemiology it is common to test for effect modification but rare to find strong evidence unless the effect truly differs across groups or the study is very large.\n\nIf you’d like, I can also prepare a short “student-friendly” sidebar version for the workbook margins, or add an optional visual explaining interaction on the logit and OR scales."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#counterfactuals",
    "href": "notebooks/1.09_regression_modelling_02.html#counterfactuals",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "1.6 7. Counterfactuals",
    "text": "1.6 7. Counterfactuals\nModern causal inference often uses counterfactual or potential outcome language. For each individual we imagine:\n\n$ Y(1) $: the outcome that would occur if the individual were exposed.\n$ Y(0) $: the outcome that would occur if the same individual were not exposed.\n\nThe causal effect for that individual is the (usually unobservable) difference $ Y(1) - Y(0) $. In practice we cannot observe both outcomes for the same person, so we rely on comparisons between groups, together with assumptions about confounding, measurement, and model specification.\nAdjustment strategies (for example, regression with appropriate covariates based on a sensible DAG) are used to make the exposed and unexposed groups more comparable, so that the difference in observed outcomes approximates the difference between counterfactual outcomes.\nWorkbook 9 returns to these ideas and introduces more formal notation and methods for estimating causal effects under explicit assumptions."
  },
  {
    "objectID": "notebooks/1.09_regression_modelling_02.html#reflection-and-exercises",
    "href": "notebooks/1.09_regression_modelling_02.html#reflection-and-exercises",
    "title": "1.09 – Confounding, DAGs, and Causal Structure",
    "section": "1.7 Reflection and exercises",
    "text": "1.7 Reflection and exercises\n\nDraw a DAG for the association between red meat intake and incident cancer in the FB2NEP cohort. Include at least age, sex, SES_class, IMD_quintile, smoking_status, and family history. Identify plausible confounders, colliders, and mediators.\nConfounders in practice: Choose a different exposure (for example, fruit_veg_g_d or salt_g_d) and a relevant outcome. Propose at least two variables as potential confounders based on subject-matter knowledge. Fit crude and adjusted models and compare the estimates.\nEnergy adjustment: Using energy_kcal and a nutrient of your choice (for example, fibre_g_d), implement the nutrient density method and the residual method. Compare the associations with BMI or another suitable outcome for the raw, density-based, and residual-based exposures.\nCollider bias: Modify the collider simulation to use a different collider (for example, an indicator of study participation) and show how conditioning on participation can induce associations between variables that are otherwise independent.\nMediators: For a hypothetical causal chain in nutrition (for example, diet quality → BMI → blood pressure → CVD), decide which variables you would adjust for when estimating the total effect of diet quality on CVD, and which you would not adjust for. Explain your reasoning.\nCounterfactual thinking: In your own words, describe what it would mean to say that “reducing salt intake by 2 g/day would reduce average SBP by 5 mmHg” in terms of potential outcomes. What assumptions would be needed for this statement to be interpreted causally in an observational study?"
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html",
    "href": "notebooks/1.06_data_exploration.html",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "",
    "text": "Version 0.0.5\nThis workbook introduces:\nRun the bootstrap cell first.\n# ============================================================\n# FB2NEP bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the fb2nep-epi repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads and runs scripts/bootstrap.py.\n# - Makes the main dataset available as the variable `df`.\n#\n# Important:\n# - You may see messages printed below (for example from pip\n#   or from the bootstrap script). This is expected.\n# - You may also see WARNINGS (often in yellow). In most cases\n#   these are harmless and can be ignored for this module.\n# - The main thing to watch for is a red error traceback\n#   (for example FileNotFoundError, ModuleNotFoundError).\n#   If that happens, please re-run this cell first. If the\n#   error persists, ask for help.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\nimport importlib.util\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\n# REPO_URL: address of the GitHub repository.\n# REPO_DIR: folder name that will be created when cloning.\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the fb2nep-epi repository\n# ------------------------------------------------------------\n# In local Jupyter, you may already be inside the repository,\n# for example in fb2nep-epi/notebooks.\n#\n# In Colab, the default working directory is /content, so\n# we need to clone the repository into /content/fb2nep-epi\n# and then change into that folder.\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/bootstrap.py exists here)\nif (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n    repo_root = cwd\n\n# Case B: we are outside the repository (for example in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if it is not present yet\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change the working directory to the repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nprint(f\"Repository root set to: {repo_root}\")\n\n# ------------------------------------------------------------\n# 2. Load scripts/bootstrap.py as a module and call init()\n# ------------------------------------------------------------\n# The shared bootstrap script contains all logic to:\n# - Ensure that required Python packages are installed.\n# - Ensure that the synthetic dataset exists (and generate it\n#   if needed).\n# - Load the dataset into a pandas DataFrame.\n#\n# We load the script as a normal Python module (fb2nep_bootstrap)\n# and then call its init() function.\nbootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n\nif not bootstrap_path.is_file():\n    raise FileNotFoundError(\n        f\"Could not find {bootstrap_path}. \"\n        \"Please check that the fb2nep-epi repository structure is intact.\"\n    )\n\n# Create a module specification from the file\nspec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\nbootstrap = importlib.util.module_from_spec(spec)\nsys.modules[\"fb2nep_bootstrap\"] = bootstrap\n\n# Execute the bootstrap script in the context of this module\nspec.loader.exec_module(bootstrap)\n\n# The init() function is defined in scripts/bootstrap.py.\n# It returns:\n# - df   : the main synthetic cohort as a pandas DataFrame.\n# - CTX  : a small context object with paths, flags and settings.\ndf, CTX = bootstrap.init()\n\n# Optionally expose a few additional useful variables from the\n# bootstrap module (if they exist). These are not essential for\n# most analyses, but can be helpful for advanced use.\nfor name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n    if hasattr(bootstrap, name):\n        globals()[name] = getattr(bootstrap, name)\n\nprint(\"Bootstrap completed successfully.\")\nprint(\"The main dataset is available as the variable `df`.\")\nprint(\"The context object is available as `CTX`.\")"
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#why-we-start-with-descriptive-exploration",
    "href": "notebooks/1.06_data_exploration.html#why-we-start-with-descriptive-exploration",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "1 1. Why we start with descriptive exploration",
    "text": "1 1. Why we start with descriptive exploration\nBefore fitting any model, we need a sense of:\n\nwhat the data look like,\nwhether variables behave as expected,\nwhere outliers or odd patterns might be,\nhow different groups differ at baseline.\n\nIn epidemiology and clinical trials, the default tool for this is “Table 1”:\n\na structured summary of baseline characteristics,\nsplit by relevant groups (e.g. sex, exposure, case/control),\ncombining categorical and continuous variables.\n\nA good Table 1 is not a formal hypothesis test — it is context.\nIn this workbook we learn how to:\n\nexplore variables descriptively,\nconstruct a simple baseline Table 1 from the FB2NEP dataset,\nand run a few basic comparisons between groups."
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#first-look-at-the-dataset-df.head-and-dtypes",
    "href": "notebooks/1.06_data_exploration.html#first-look-at-the-dataset-df.head-and-dtypes",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "2 1.1 First look at the dataset (df.head() and dtypes)",
    "text": "2 1.1 First look at the dataset (df.head() and dtypes)\nBefore doing anything more complex, we want to answer a few basic questions:\n\nWhat are the variables called?\nWhat is their type (number, text, date)?\nDo the values look plausible (e.g. BMI, blood pressure, age)?\nAre there obvious missing values or strange codes?\n\nIn pandas, two very simple tools already help a lot:\n\ndf.head() → first few rows of the dataset.\ndf.dtypes → how pandas currently represents each variable.\n\nWhen you run the next cell, look specifically for:\n\nDoes baseline_date look like a date, or like plain text?\nAre things like sex, smoking_status, SES_class stored as object (string) variables?\nAre continuous variables (e.g. age, BMI, SBP, energy_kcal) stored as numeric (int64 or float64)?\n\n\nMini-task:\nAfter running the next cell, write down one thing that looks as you would expect, and one thing that surprises you.\n\n\n# 1.1 Quick look at the dataset\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\ndisplay(df.head())\ndf.dtypes.head(20)\n\n\n2.1 1.2 Quick reflection\n\nAre there any variables you thought would be continuous but appear as text, or vice versa?\nDo you see any variables where the name is unclear and might need a better label in a paper?\nDo you notice any obvious missing values in the first few rows?\n\nYou do not fix things yet here – the goal is just to get a feeling for the data."
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#descriptive-statistics-first-pass",
    "href": "notebooks/1.06_data_exploration.html#descriptive-statistics-first-pass",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "3 2. Descriptive statistics: first pass",
    "text": "3 2. Descriptive statistics: first pass\nWe start with quick summaries for selected variables:\n\nContinuous variables via .describe()\n→ number of observations, mean, standard deviation, quartiles, min and max.\nCategorical variables via .value_counts()\n→ counts (and, if we want, proportions) for each category.\n\nFrom this we can already check:\n\nAre ranges plausible? (e.g. nobody with BMI 3 or age 300.)\nIs there evidence of skewness (very long right tail for energy intake)?\nHow big are the groups (e.g. current vs former vs never smokers)?\nAre categories balanced, or is one group very small?\n\nRun the next cell and look at:\n\nage, BMI, SBP, energy_kcal – do the numbers look reasonable for a UK cohort?\nsex, SES_class, smoking_status, physical_activity – do the group sizes fit your expectations?\n\n\n# 2.1 Summary statistics for key variables\n\ncontinuous_vars = [\"age\", \"BMI\", \"SBP\", \"DBP\", \"energy_kcal\"]\ncategorical_vars = [\"sex\", \"SES_class\", \"smoking_status\", \"physical_activity\"]\n\nfor v in continuous_vars:\n    if v in df.columns:\n        print(f\"\\n{v} (continuous)\")\n        display(df[v].describe())\n\nfor v in categorical_vars:\n    if v in df.columns:\n        print(f\"\\n{v} (categorical)\")\n        print(df[v].value_counts(dropna=False))\n\n\n3.1 2.1 Interpreting these summaries\nWhen you look at the .describe() output for continuous variables, ask:\n\nAre the minimum and maximum values realistic in human populations?\nIs the standard deviation small or large compared with the mean?\nDoes the IQR (25%–75%) suggest most people are in a narrow band, or very spread out?\n\nFor categorical variables:\n\nAre there any rare categories (very small counts)?\nAre the group sizes very imbalanced (e.g. 95% never smokers)?\nDo you see any unexpected categories (e.g. misspellings)?\n\n\nMini-task:\nPick one continuous and one categorical variable.\nFor each, write one short sentence describing what you see (e.g. “Most participants are in their 50s and 60s, with ages ranging from 40 to 90.”)."
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#visual-inspection",
    "href": "notebooks/1.06_data_exploration.html#visual-inspection",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "4 3. Visual inspection",
    "text": "4 3. Visual inspection\nTables are useful, but plots help us see patterns that numbers can hide:\n\nSkewness or long tails (e.g. a few people with very high intake).\nMultimodal distributions (two “peaks” → possibly two subgroups).\nSpikes that might indicate coding or rounding (e.g. lots of values at 150 mmHg).\nGroup differences that may or may not be important.\n\nHere we use BMI as an example:\n\nA histogram to see the overall shape of the distribution.\nA boxplot by sex to see whether men and women differ in BMI.\n\nWhen you look at the plots, consider:\n\nDoes BMI look roughly normal, slightly skewed, or very skewed?\nAre there obvious outliers (points far away from the rest)?\nDo men and women have similar medians and spreads, or is one group heavier?\n\n\n# 3.1 Histogram of BMI\n\n%matplotlib inline\n\nif \"BMI\" in df.columns:\n    plt.figure(figsize=(6, 4))\n    df[\"BMI\"].hist(bins=30)\n    plt.xlabel(\"BMI (kg/m²)\")\n    plt.ylabel(\"Number of participants\")\n    plt.title(\"BMI distribution\")\n    plt.tight_layout()\n    plt.show()\n\n\n# 3.2 Boxplot of BMI by sex\n\nif {\"BMI\", \"sex\"}.issubset(df.columns):\n    plt.figure(figsize=(6, 4))\n    df.boxplot(column=\"BMI\", by=\"sex\")\n    plt.title(\"BMI by sex\")\n    plt.suptitle(\"\")  # remove automatic super-title\n    plt.xlabel(\"Sex\")\n    plt.ylabel(\"BMI (kg/m²)\")\n    plt.tight_layout()\n    plt.show()\n\n\n4.1 3.3 Quick questions\nAfter looking at the histogram and boxplot:\n\nWould you be comfortable using methods that assume an approximately normal distribution for BMI?\nIf you had to highlight one potential issue with BMI in this dataset (e.g. outliers, skew), what would it be?\nDo you think the difference between men and women in BMI, if any, is likely to be meaningful in practice?\n\nYou do not need exact numbers yet – this is about developing an eye for the data."
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#building-a-baseline-table-1",
    "href": "notebooks/1.06_data_exploration.html#building-a-baseline-table-1",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "5 4. Building a baseline “Table 1”",
    "text": "5 4. Building a baseline “Table 1”\nIn most papers, Table 1 summarises baseline characteristics by a key grouping variable:\n\nin trials: intervention vs control,\nin cohorts: exposed vs unexposed, or cases vs non-cases,\nvery commonly: by sex.\n\nWe will:\n\nchoose sex as the grouping variable,\nsummarise continuous variables as mean ± SD,\nsummarise categorical variables as counts and percentages.\n\nThe aim is clarity, not yet hypothesis testing.\n\n# 4.1 Helper function to create a simple Table 1\n\nfrom scripts.helpers_tables import make_table1\n\ncontinuous_vars = [\"age\", \"BMI\", \"SBP\", \"DBP\"]\ncategorical_vars = [\"SES_class\", \"smoking_status\", \"physical_activity\"]\n\nif \"sex\" in df.columns:\n    table1 = make_table1(df, group=\"sex\",\n                         continuous=continuous_vars,\n                         categorical=categorical_vars)\n    print(\"Baseline characteristics by sex (simple Table 1):\")\n    display(table1)\nelse:\n    print(\"Variable 'sex' not found in dataset.\")"
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#interpreting-a-simple-table-1",
    "href": "notebooks/1.06_data_exploration.html#interpreting-a-simple-table-1",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "6 4.2 Interpreting a simple Table 1",
    "text": "6 4.2 Interpreting a simple Table 1\nLook at the Table 1 output:\n\nContinuous rows (age, BMI, SBP) show mean ± SD by sex.\nCategorical rows (SES, smoking, physical activity) show counts and percentages by sex.\n\nQuestions to ask:\n\nAre age and BMI similar between men and women, or is there a noticeable difference?\nDo men and women differ in SES distribution (e.g. more ABC1 in one group)?\nAre there clear differences in smoking patterns (more current smokers in one sex)?\nIs physical activity similarly distributed, or does one group report more “high” activity?\n\n\nMini-task:\nWrite two short sentences summarising Table 1, for example: - “Men and women in this cohort are of similar age and BMI.”\n- “Smoking and SES distributions are also similar between sexes, with only small differences.”"
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#comparing-groups-t-tests-χ²-tests-anova",
    "href": "notebooks/1.06_data_exploration.html#comparing-groups-t-tests-χ²-tests-anova",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "7 5. Comparing groups: t-tests, χ²-tests, ANOVA",
    "text": "7 5. Comparing groups: t-tests, χ²-tests, ANOVA\nOnce we have a baseline table, we might ask whether observed differences are larger than we would expect by chance.\nHere we introduce three basic tools:\n\nTwo-sample t-test — difference in means between two groups (e.g. BMI in men vs women).\nχ²-test of independence — association between two categorical variables (e.g. smoking vs SES).\nOne-way ANOVA — difference in means across three or more groups (e.g. SBP by physical activity category).\n\nThese are introductory tools; later workbooks will use regression models that generalise these ideas.\n\n# 5.1 Two-sample t-test: BMI in men vs women\n\nfrom scipy.stats import ttest_ind\n\nif {\"BMI\", \"sex\"}.issubset(df.columns):\n    bmi_m = df[df[\"sex\"] == \"M\"][\"BMI\"].dropna()\n    bmi_f = df[df[\"sex\"] == \"F\"][\"BMI\"].dropna()\n\n    if len(bmi_m) &gt; 1 and len(bmi_f) &gt; 1:\n        stat, p = ttest_ind(bmi_m, bmi_f, equal_var=False)\n        print(\"T-test: BMI (M vs F)\")\n        print(\"n (M) =\", len(bmi_m), \"  n (F) =\", len(bmi_f))\n        print(\"Statistic:\", stat)\n        print(\"p-value :\", p)\n    else:\n        print(\"Not enough data in one of the groups for t-test.\")\nelse:\n    print(\"BMI or sex not found in dataset.\")\n\n\n7.1 5.1.1 Interpreting the BMI t-test (M vs F)\nThe t-test compares mean BMI between men and women under the assumption that:\n\nobservations are independent,\nBMI is approximately normal within each group,\nthe two groups have similar enough variance (we used equal_var=False, which relaxes this).\n\nWhen you look at the output:\n\nThe statistic tells you how far the observed difference is from zero, in SD units.\nThe p-value tells you how compatible the data are with the null hypothesis of no difference.\n\nHowever, even if the p-value is very small (statistically significant), always also check:\n\nEffect size: what is the actual difference in mean BMI (in kg/m²)?\nPractical relevance: would a difference of, say, 0.3 kg/m² matter clinically or for public health?\n\n\nMini-task:\nUse the Table 1 means to compute (by hand) the difference in BMI between men and women.\nThen decide: even if this were “significant”, would it be important?\n\n\n# 5.2 χ²-test: smoking_status vs SES_class\n\nfrom scipy.stats import chi2_contingency\n\nif {\"smoking_status\", \"SES_class\"}.issubset(df.columns):\n    tab = pd.crosstab(df[\"smoking_status\"], df[\"SES_class\"])\n    print(\"Contingency table: smoking_status × SES_class\")\n    display(tab)\n\n    if tab.values.min() &gt; 0:\n        chi2, p, dof, exp = chi2_contingency(tab)\n        print(\"χ² statistic:\", chi2)\n        print(\"Degrees of freedom:\", dof)\n        print(\"p-value:\", p)\n    else:\n        print(\"Some cells have zero counts; χ²-test results need caution.\")\nelse:\n    print(\"smoking_status or SES_class not found in dataset.\")\n\n\n\n7.2 5.2.1 Interpreting the χ²-test: smoking vs SES\nThe χ²-test of independence asks:\n\nIs there evidence that smoking status and SES class are associated,\nor could the observed differences be due to chance alone?\n\nSteps:\n\nThe contingency table (pd.crosstab) shows counts in each combination of categories.\nThe χ²-test compares these to expected counts if smoking and SES were independent.\nA small p-value suggests that the pattern of smoking differs by SES group.\n\nPoints to check:\n\nAre there any very small cells? These can make the χ²-test less reliable.\nIf there is an association, is it strong (e.g. current smoking concentrated in one SES group), or subtle?\n\n\nMini-task:\nLook at the contingency table and describe in words how smoking patterns differ between SES classes (if at all).\n\n\n# 5.3 One-way ANOVA: SBP by physical_activity\n\nfrom scipy.stats import f_oneway\n\nif {\"SBP\", \"physical_activity\"}.issubset(df.columns):\n    levels = [\"low\", \"moderate\", \"high\"]\n    groups = []\n\n    for lev in levels:\n        if lev in df[\"physical_activity\"].unique():\n            g = df[df[\"physical_activity\"] == lev][\"SBP\"].dropna()\n            if len(g) &gt; 1:\n                groups.append(g)\n                print(f\"Level '{lev}': n =\", len(g))\n\n    if len(groups) &gt;= 2:\n        stat, p = f_oneway(*groups)\n        print(\"\\nANOVA: SBP by physical_activity\")\n        print(\"F-statistic:\", stat)\n        print(\"p-value   :\", p)\n    else:\n        print(\"Not enough groups with data for ANOVA.\")\nelse:\n    print(\"SBP or physical_activity not found in dataset.\")\n\n\n\n7.3 5.3.1 ANOVA: what does a “significant” F-test mean here?\nThe one-way ANOVA tests:\n\nDo mean SBP values differ across the physical activity groups (low, moderate, high)?\n\nIf the p-value is small:\n\nIt tells you that at least one group mean is different.\nIt does not tell you which groups differ, or by how much.\n\nAlways combine:\n\nThe ANOVA result (F-statistic, p-value) with\nThe group means and SDs (you can compute them with df.groupby(\"physical_activity\")[\"SBP\"].mean()).\n\n\nMini-task:\nCompute mean SBP by physical activity group and consider whether the size of the differences is likely to be relevant in practice."
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#effect-sizes-and-confidence-intervals",
    "href": "notebooks/1.06_data_exploration.html#effect-sizes-and-confidence-intervals",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "8 5.4 Effect sizes and confidence intervals",
    "text": "8 5.4 Effect sizes and confidence intervals\nSo far we have looked at p-values for group comparisons.\nHowever, a p-value alone does not tell us:\n\nhow big the difference is,\nhow uncertain the estimate is,\nwhether the effect might plausibly be “small but important” or “large but irrelevant”.\n\nA confidence interval (CI) gives us exactly this.\n\n8.1 What is a confidence interval?\nA 95% CI for a mean (or a difference in means) is a range that:\n\nwould contain the true population value in 95% of repeated samples\n(if we could repeat the study infinitely many times under identical conditions).\n\nIt is not: - a 95% probability that the true value lies inside the interval,\n- nor a statement about any individual.\nIt is a statement about the method.\n\n\n8.2 Why CIs matter more than p-values\nA very small p-value could correspond to:\n\na tiny effect with high precision (large N),\na moderate effect with poor precision (small N),\nor something in between.\n\nThe CI tells you which one is happening.\nExamples: - A BMI difference of 0.2 (95% CI: 0.1 to 0.3) → tiny, precise. - A BMI difference of 2.5 (95% CI: –1.0 to 6.0) → big uncertainty; p-value alone is misleading.\n\n\n8.3 CI rule of thumb\n\nIf a CI for a difference crosses 0 → compatible with “no difference”.\nIf a CI is very narrow → estimate is precise.\nIf a CI is wide → estimate is uncertain, even if the p-value is small.\n\nWe add a simple Python function below to compute a 95% CI for the difference in means for BMI (men vs women).\n\n# 5.4 Confidence interval for difference in means (BMI: M vs F)\n\nimport numpy as np\nfrom scipy.stats import t\n\n# Extract data\nbmi_m = df[df[\"sex\"] == \"M\"][\"BMI\"].dropna().values\nbmi_f = df[df[\"sex\"] == \"F\"][\"BMI\"].dropna().values\n\n# Means and SDs\nmean_m = bmi_m.mean()\nmean_f = bmi_f.mean()\ndiff = mean_m - mean_f\n\n# Standard error of difference (Welch)\nse = np.sqrt(bmi_m.var(ddof=1)/len(bmi_m) + bmi_f.var(ddof=1)/len(bmi_f))\n\n# Degrees of freedom (Welch–Satterthwaite)\nnum = (bmi_m.var(ddof=1)/len(bmi_m) + bmi_f.var(ddof=1)/len(bmi_f))**2\nden = ( (bmi_m.var(ddof=1)/len(bmi_m))**2 / (len(bmi_m)-1) +\n        (bmi_f.var(ddof=1)/len(bmi_f))**2 / (len(bmi_f)-1) )\ndf_welch = num / den\n\n# 95% CI\nt_crit = t.ppf(0.975, df_welch)\nci_low = diff - t_crit * se\nci_high = diff + t_crit * se\n\nprint(f\"Difference in mean BMI (M − F): {diff:.3f}\")\nprint(f\"95% CI: [{ci_low:.3f}, {ci_high:.3f}]\")\n\n\n\n\n8.4 How to use this in practice\nWhen reporting group differences (e.g. in Table 1 or in papers):\n\nAlways include the effect size (difference in means).\nAlways include the 95% CI.\nTreat the p-value as supportive, not decisive.\nUse the CI to judge precision and relevance.\n\nCIs are often more informative than p-values because they show: - the range of plausible values, and\n- whether the difference is likely to matter in practice."
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#statistical-vs-practical-significance",
    "href": "notebooks/1.06_data_exploration.html#statistical-vs-practical-significance",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "9 6. Statistical vs practical significance",
    "text": "9 6. Statistical vs practical significance\nWhen we run a hypothesis test (for example a t-test or χ²-test), we usually get:\n\na test statistic (t, χ², F, …), and\n\na p-value.\n\n\n9.1 6.1 What does the p-value mean?\nThe p-value is:\n\nThe probability of observing a result at least as extreme as the one in your data,\nif the null hypothesis were true.\n\nImportant implications:\n\nIt is not the probability that the null hypothesis is true.\nIt is not the probability that your result is “just due to chance”.\nIt depends on:\n\nthe effect size (how different the groups really are),\nthe sample size (more people → smaller p-values for the same effect),\nand the variability in the data.\n\n\nWith large samples (such as the FB2NEP synthetic cohort), even very small differences can produce tiny p-values.\nExamples:\n\nA BMI difference of 0.3 kg/m² between two groups may be strongly “statistically significant”.\nIt is unlikely to be clinically or public-health relevant on its own.\n\n\n\n9.2 6.2 Why p = 0.05 is arbitrary (and not magic)\nIn many papers, p &lt; 0.05 is used as a cut-off to call a result “statistically significant”.\n\nThis threshold is largely historical and conventional, not a scientific law.\nUsing 0.05 means we accept that, if there were no true effect, we would falsely declare a difference in about 5% of studies (Type I error).\nSome fields use stricter thresholds (e.g. 0.01 or 0.001); others avoid hard cut-offs and report p-values as a continuum of evidence.\n\nBetter interpretation:\n\np = 0.049 and p = 0.051 are essentially the same in terms of evidence.\nInstead of “significant / not significant”, think:\n\nSmaller p-value → data are less compatible with “no effect”.\nLarger p-value → data are more compatible with “no effect”.\n\nAlways combine p-values with:\n\neffect sizes,\nconfidence intervals, and\nsubject-matter knowledge.\n\n\n\n\n9.3 6.3 False positives and the role of sample size\nIf we repeatedly compare two groups that are truly identical (no real difference):\n\nand we use p &lt; 0.05 as our rule,\nwe will still find “significant” differences in about 5% of comparisons, purely by chance.\n\nIn large datasets:\n\neven tiny, practically irrelevant differences can give very small p-values.\nthis is why “statistically significant” does not automatically mean “important”.\n\n\nExample using FB2NEP:\nSuppose you find that BMI is 0.4 kg/m² higher in men than in women, with p &lt; 0.001.\nThis is statistically significant in 25,000 people, but is unlikely to change clinical practice on its own.\n\n\n\n9.4 6.4 Practical significance: what actually matters?\nAlways ask:\n\nEffect size – How large is the difference or association in real units?\n\nIs a 0.4 kg/m² difference in BMI meaningful?\nIs a 0.2 mmol/L difference in cholesterol important?\n\nUncertainty – What do confidence intervals look like?\n\nIs the interval narrow (precise estimate) or wide (uncertain)?\n\nContext – Would this difference change practice, policy, or interpretation?\n\nWould a clinician treat patients differently?\nWould a policymaker change guidelines?\n\n\nPlots and descriptive summaries are often as important as p-values when deciding whether a finding is meaningful.\nIn FB2NEP and in real studies:\n\nuse p-values as one piece of evidence,\navoid treating 0.05 as a magic border between “true” and “false”,\nand always relate results back to public-health and clinical relevance.\n\n\n# 6.x Simulation: how often do we see \"significant\" differences by chance?\n\nimport numpy as np\nfrom scipy.stats import ttest_ind\nimport matplotlib.pyplot as plt\n\n# For reproducibility\nnp.random.seed(11088)\n\n# Parameters – students can play with these\nn_sim = 1000          # number of simulated experiments\nn_per_group = 100     # sample size in each group\nalpha = 0.05          # significance threshold\n\np_values = []\n\nfor i in range(n_sim):\n    # Generate two groups from the *same* distribution (no true difference)\n    x = np.random.normal(loc=0, scale=1, size=n_per_group)\n    y = np.random.normal(loc=0, scale=1, size=n_per_group)\n\n    # Two-sample t-test (Welch)\n    stat, p = ttest_ind(x, y, equal_var=False)\n    p_values.append(p)\n\np_values = np.array(p_values)\n\n# How many \"significant\" results did we get?\nn_sig = (p_values &lt; alpha).sum()\nprop_sig = n_sig / n_sim * 100\n\nprint(f\"Number of simulations       : {n_sim}\")\nprint(f\"Sample size per group       : {n_per_group}\")\nprint(f\"Alpha (significance level)  : {alpha}\")\nprint(f\"Significant results (p &lt; α) : {n_sig} ({prop_sig:.1f}%)\")\n\n# Optional: look at the distribution of p-values\nplt.figure(figsize=(6, 4))\nplt.hist(p_values, bins=20)\nplt.xlabel(\"p-value\")\nplt.ylabel(\"Count\")\nplt.title(\"Distribution of p-values when there is no true difference\")\nplt.tight_layout()\nplt.show()\n\nQuestions to consider:\n\nHow close is the percentage of “significant” results to the chosen α (0.05)?\nWhat happens if you:\n\nchange alpha to 0.01?\nincrease or decrease n_per_group?\n\nHow does this simulation illustrate the idea of Type I error and the arbitrariness of the 0.05 threshold?"
  },
  {
    "objectID": "notebooks/1.06_data_exploration.html#summary",
    "href": "notebooks/1.06_data_exploration.html#summary",
    "title": "1.06 – Data Exploration and “Table 1”",
    "section": "10 7. Summary",
    "text": "10 7. Summary\nIn this workbook you:\n\nPerformed descriptive exploration of key continuous and categorical variables.\nUsed histograms and boxplots to check distributions and group differences.\nBuilt a simple baseline Table 1 summarising characteristics by sex.\nRan basic group comparisons using t-tests, χ²-tests and one-way ANOVA.\nReflected on the difference between statistical and practical significance.\n\nThese steps are part of routine data exploration in nutritional epidemiology and provide the foundation for more advanced modelling in later workbooks."
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html",
    "href": "notebooks/1.04_data_collection_cleaning.html",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "",
    "text": "Version 0.1.0\nThis workbook introduces:\nRun the first code cell to set up the repository and load the dataset.\n# ============================================================\n# FB2NEP bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the fb2nep-epi repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads and runs scripts/bootstrap.py.\n# - Makes the main dataset available as the variable `df`.\n#\n# Important:\n# - You may see messages printed below (for example from pip\n#   or from the bootstrap script). This is expected.\n# - You may also see WARNINGS (often in yellow). In most cases\n#   these are harmless and can be ignored for this module.\n# - The main thing to watch for is a red error traceback\n#   (for example FileNotFoundError, ModuleNotFoundError).\n#   If that happens, please re-run this cell first. If the\n#   error persists, ask for help.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\nimport importlib.util\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\n# REPO_URL: address of the GitHub repository.\n# REPO_DIR: folder name that will be created when cloning.\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the fb2nep-epi repository\n# ------------------------------------------------------------\n# In local Jupyter, you may already be inside the repository,\n# for example in fb2nep-epi/notebooks.\n#\n# In Colab, the default working directory is /content, so\n# we need to clone the repository into /content/fb2nep-epi\n# and then change into that folder.\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/bootstrap.py exists here)\nif (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n    repo_root = cwd\n\n# Case B: we are outside the repository (for example in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if it is not present yet\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change the working directory to the repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nprint(f\"Repository root set to: {repo_root}\")\n\n# ------------------------------------------------------------\n# 2. Load scripts/bootstrap.py as a module and call init()\n# ------------------------------------------------------------\n# The shared bootstrap script contains all logic to:\n# - Ensure that required Python packages are installed.\n# - Ensure that the synthetic dataset exists (and generate it\n#   if needed).\n# - Load the dataset into a pandas DataFrame.\n#\n# We load the script as a normal Python module (fb2nep_bootstrap)\n# and then call its init() function.\nbootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n\nif not bootstrap_path.is_file():\n    raise FileNotFoundError(\n        f\"Could not find {bootstrap_path}. \"\n        \"Please check that the fb2nep-epi repository structure is intact.\"\n    )\n\n# Create a module specification from the file\nspec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\nbootstrap = importlib.util.module_from_spec(spec)\nsys.modules[\"fb2nep_bootstrap\"] = bootstrap\n\n# Execute the bootstrap script in the context of this module\nspec.loader.exec_module(bootstrap)\n\n# The init() function is defined in scripts/bootstrap.py.\n# It returns:\n# - df   : the main synthetic cohort as a pandas DataFrame.\n# - CTX  : a small context object with paths, flags and settings.\ndf, CTX = bootstrap.init()\n\n# Optionally expose a few additional useful variables from the\n# bootstrap module (if they exist). These are not essential for\n# most analyses, but can be helpful for advanced use.\nfor name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n    if hasattr(bootstrap, name):\n        globals()[name] = getattr(bootstrap, name)\n\nprint(\"Bootstrap completed successfully.\")\nprint(\"The main dataset is available as the variable `df`.\")\nprint(\"The context object is available as `CTX`.\")"
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html#variable-types-and-coding",
    "href": "notebooks/1.04_data_collection_cleaning.html#variable-types-and-coding",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "1 1. Variable types and coding",
    "text": "1 1. Variable types and coding\nBefore we think about where data come from, we need to understand what kind of things we can measure.\nIn this workbook we will repeatedly use three broad types of variables:\n\nContinuous variables\nNumeric measurements on a (more or less) continuous scale.\nExamples: BMI, SBP, energy_kcal, fruit_veg_g_d.\nCategorical variables (nominal)\nCategories without an inherent order.\nExamples: sex, SES_class, smoking_status.\nOrdinal variables\nCategories with a meaningful order, but without fixed distances between levels.\nExamples: IMD_quintile (1 = most deprived, 5 = least deprived), physical_activity (low / moderate / high).\n\nLater, in the workbook on sampling and representativeness, we will use these variable types to compare our synthetic FB2NEP cohort to reference surveys (such as NHANES). Here we focus on:\n\nhow different types of variables are stored and summarised in Python,\n\nwhat happens when we encode answers into numbers, and\n\nhow we can already spot some problems just from the way data are coded.\n\n\n1.1 UK Socio-Economic Status (SES) Classification – A Short Guide\nIn UK public health and survey research, socio-economic status is often measured using the NRS Social Grades.\n(NRS: National Readership Survey) They are widely used in national surveys, market research, and sometimes in epidemiology.\n\n1.1.1 1. The Six NRS Social Grades\n\n\n\n\n\n\n\n\nGrade\nMeaning\nTypical occupations\n\n\n\n\nA\nHigher managerial, administrative, professional\nCEOs, senior civil servants\n\n\nB\nIntermediate managerial, administrative, professional\nTeachers, mid-level managers, engineers\n\n\nC1\nSupervisory, clerical, junior managerial/professional\nOffice staff, technicians\n\n\nC2\nSkilled manual workers\nElectricians, plumbers, carpenters\n\n\nD\nSemi-skilled & unskilled manual workers\nFactory workers, delivery drivers\n\n\nE\nCasual/lowest-grade workers, unemployed, pensioners\nLong-term unemployed, some retirees\n\n\n\nThese grades correlate with income, education, and job security — but they are not identical to any one of those.\n\n\n1.1.2 2. The Two-Level System: ABC1 vs C2DE\nMany surveys collapse the six groups into two wider bands:\n\nABC1: higher or intermediate SES (A + B + C1)\n\nC2DE: lower SES (C2 + D + E)\n\nThis simplified classification is the one used in the FB2NEP synthetic dataset (SES_class).\n\n\n1.1.3 3. Strengths and Limitations\nStrengths - Simple, widely recognised, consistently used in UK surveys. - Shows clear associations with health behaviours and outcomes.\nLimitations - Coarse: mixes diverse occupations within broad categories. - Not portable outside the UK (occupation structures differ). - Based on occupation, which is awkward for students, retirees, carers, and precarious workers. - Does not map neatly to education or income.\n\n\n1.1.4 4. Harmonisation Issues Across Datasets (Education, Income, Occupation)\nSES looks deceptively simple, but different datasets measure different things:\n\nNHANES (US) uses education (≤ high school / some college / college+).\nNDNS (UK) often uses education or income alongside NRS social grade.\nEPIC and UK Biobank use mixtures of occupation, education, income, area-level deprivation.\nFB2NEP uses occupation-based social grades (ABC1 / C2DE).\nInternational surveys (OECD, WHO STEPS, Eurostat) usually use education because occupation and income do not translate across countries.\n\nThis immediately creates problems for harmonisation.\n\n1.1.4.1 Why SES ≠ education ≠ income ≠ occupation\nSES is a latent construct — an underlying social position that cannot be measured directly.\nDifferent operationalisations capture different aspects:\n\nEducation → long-term human capital, cognitive resources, health literacy.\n\nIncome → current material resources and capacity to purchase goods.\n\nOccupation → job security, working conditions, prestige, social capital.\n\nArea-level deprivation → neighbourhood environment, access to services.\n\nThese correlate, but not perfectly.\n&gt; Example: a highly educated PhD student may have low income; a long-serving electrician may earn more than a junior academic but be coded as C2, not ABC1.\nUsing one variable to stand in for SES always loses information.\n\n\n1.1.4.2 Why the UK is unusual\nThe UK has a long-standing, highly standardised occupation-based SES system:\n\nNRS Social Grades (A–E), created for media and market research, became widely used in public health research.\nIt maps occupation → social class → purchasing power in a historically industrial society.\nIt remains deeply embedded in UK survey infrastructure (ONS, NDNS, IPSOS, BBC, etc.).\n\nThis is unusual internationally:\n\nMost countries do not have a national occupation-based SES scale.\nEducation is the default SES variable in the US, EU, and WHO datasets because it is the most stable and portable across contexts.\nOccupations, job structures and prestige systems differ too much between countries to harmonise consistently.\n\nThus, using ABC1/C2DE outside the UK is difficult.\n\n\n1.1.4.3 What happens when two SES systems must be aligned?\nAny harmonisation requires strong assumptions.\nTypical problems:\n\nDifferent constructs: NHANES education ≠ UK NRS grades.\n\nDifferent number of categories: one is binary (ABC1/C2DE), one has three or six categories.\n\nDifferent conceptual meaning: “College+” is not equivalent to “professional/managerial”.\n\nNon-monotonic relationships: more education ≠ higher job grade in every case.\n\nExample mismatches: - A retired senior civil servant → NRS A/B but education may be “≤ High school”. - A university graduate in a temporary low-paid job → college+ + C2 or D. - A tradesperson with high income → C2 but with higher income than many ABC1 workers.\n\n\n1.1.4.4 Implications for analysis\nHarmonisation is always an approximation.\nWhen aligning SES across datasets:\n\nBe explicit about the mapping rules you apply.\n\nCheck frequencies and identify categories that map poorly.\n\nDocument data loss (e.g. collapsing categories removes nuance).\n\nAcknowledge uncertainty introduced by any recoding.\n\nPerform sensitivity analyses where possible (e.g., different mapping schemes).\n\nTeaching takeaway:\nSES is a theoretical construct, not a variable.\nEvery dataset measures it differently.\nHarmonisation therefore requires judgement, transparency, and a clear statement of limitations.\n\n\n\n1.1.5 5. Takeaway\nSES is not a single “true” attribute.\nDifferent countries, surveys and traditions measure it differently.\nFor cross-dataset comparisons (e.g. NHANES vs FB2NEP), SES must be translated with care and the uncertainties should be stated clearly.\n\n\n\n1.2 Excursion: Index of Multiple Deprivation (IMD)\nIndex of Multiple Deprivation (IMD) is the main area-based measure of deprivation used in England.\nIt does not describe an individual’s SES, but the deprivation level of the small area where they live.\n\n\n1.2.0.1 1. What IMD measures\nIMD combines seven domains of deprivation into a single index:\n\nIncome\n\nEmployment\n\nEducation, skills and training\n\nHealth and disability\n\nCrime\n\nBarriers to housing and services\n\nLiving environment\n\nEach small area (Lower Layer Super Output Area, LSOA; ~1,500 people) receives:\n\na rank (1 = most deprived area in England),\n\nand derived deciles or quintiles.\n\nIn FB2NEP we use IMD quintiles, where:\n\n1 = most deprived,\n\n5 = least deprived.\n\n\n\n\n1.2.0.2 2. IMD vs individual SES\nIMD is area-level; SES (such as NRS social grade or education) is usually individual-level.\nThey capture related but different aspects:\n\nIMD reflects the context of a neighbourhood\n(local income, employment, environment, access to services, crime).\n\nSES reflects an individual’s social position\n(education, occupation, income, security).\n\nExamples:\n\nA highly educated person may live in an IMD 1 area (cheap housing in a deprived neighbourhood).\n\nA low-income person may live in an IMD 4–5 area (long-term tenant in a relatively affluent area).\n\nKey point: IMD is very useful for understanding health inequalities and neighbourhood effects,\nbut it must not be treated as “the same thing” as individual SES.\n\n\n\n1.2.0.3 3. Why IMD appears in FB2NEP\nMany UK cohort studies and surveys (e.g. NDNS, UK Biobank, some cancer registries) link IMD to participants via postcode.\nTypical uses include:\n\ndescribing the deprivation profile of the cohort,\n\nstudying gradients in risk factors and disease by area-level deprivation,\n\nadjusting for area-level deprivation as a potential confounder.\n\nFB2NEP mirrors this practice by including IMD_quintile:\n\nIt lets us explore inequalities in diet and health,\n\nand it sets up later Public Health exercises where IMD plays a central role.\n\n\n\n\n1.2.0.4 4. Practical reminders for analysis\n\nTreat IMD as an ordered categorical variable (e.g. quintiles), not as a precise continuous scale.\n\nBe clear in write-ups that IMD is area-based, not a personal SES measure.\n\nConsider showing results by both individual SES (e.g. ABC1/C2DE) and IMD where possible.\n\n\n\n\n1.3 Excursion: Physical Activity – Why Categories Are Vague (and Why It Matters)\nPhysical activity is essential in nutritional epidemiology, yet it is one of the least precisely measured variables in any dataset.\nEven “good” surveys struggle because activity is multi-dimensional and hard to summarise in a single score.\n\n\n1.3.0.1 1. Why physical activity is hard to measure\nPhysical activity varies by:\n\nIntensity (light, moderate, vigorous)\n\nType (walking, cycling, work-related, leisure, sport, chores)\n\nDuration\n\nFrequency\n\nContext (commuting, occupation, exercise, childcare)\n\nThis creates an enormous range of behaviours that cannot easily be captured by simple categories.\n\n\n\n1.3.0.2 2. How surveys measure activity\nMost population studies rely on self-report, for example:\n\n“How many minutes per week of moderate physical activity do you do?”\n\n“How often do you walk for at least 10 minutes?”\n\n“How active is your job?”\n\nLikert scales such as low / moderate / high.\n\nMore detailed instruments exist (IPAQ, GPAQ), but they still rely on participant recall and judgement.\nEven objectively measured PA (accelerometers, wearables) must be translated into categories, and the choice of cut-points affects results.\n\n\n\n1.3.0.3 3. What FB2NEP uses (and why)\nFB2NEP includes a variable:\n\nphysical_activity = \"low\", \"moderate\", \"high\"\n\nThis is typical in cohort studies where detailed accelerometer data are not available.\nHowever, these categories are:\n\nVague: “moderate” for one person may be “vigorous” for another.\n\nSelf-assessed: depends on how active participants feel.\n\nContext-dependent: a manual worker and a sedentary office worker with the same walking minutes may not classify themselves the same way.\n\nIn other words:\n&gt; These categories provide a rough indicator of activity level, useful for broad comparisons but not precise enough to infer true energy expenditure.\n\n\n\n1.3.0.4 4. Common problems in analysis\n\nNon-comparability across surveys\n\nNHANES uses MET-minutes.\n\nNDNS uses questionnaire-based categorisation.\n\nUK Biobank uses a mixture (IPAQ-like questions + accelerometer subsample).\n\nEPIC uses a 4-level Cambridge index.\n\nThese systems are not equivalent.\nCategorisation introduces information loss\n\nCollapsing minutes, intensity, and context into one label removes detail.\n\n“Moderate” may hide large differences in actual activity.\n\nCut-off sensitivity\n\nIf “moderate” is defined differently across studies, results are not directly comparable.\n\nResidual confounding\n\nPA categories are too crude to fully capture lifestyle differences\n(e.g. fitness, occupation, active commuting).\n\n\n\n\n\n1.3.0.5 5. Why this matters for nutritional epidemiology\nPhysical activity is a major confounder for:\n\nEnergy intake\n\nObesity\n\nCardiovascular risk\n\nMetabolic markers\n\nSedentary behaviour\n\nDiet–disease associations\n\nUsing crude PA categories increases measurement error, which tends to:\n\nBias effect estimates toward the null,\n\nInflate residual confounding,\n\nMake PA look “less important” than it is.\n\n\n\n\n1.3.0.6 6. Takeaway\n\nPhysical activity categories are useful, but imprecise.\n\nNever over-interpret “low/moderate/high” as exact physiological measures.\n\nWhen comparing datasets, check how PA was measured and consider harmonisation issues.\n\nIf possible, perform sensitivity analyses using alternative cut-points.\n\n\n# 1.1 Quick look at the synthetic cohort and variable types\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\n\n# Show the first 5 rows\ndisplay(df.head())\n\n# Show the data types of the first few columns\ndf.dtypes.head(20)\n\n\n# 1.2 Examples of how we summarise different variable types\n\n# Continuous example: BMI\nif \"BMI\" in df.columns:\n    print(\"BMI – continuous variable\")\n    display(df[\"BMI\"].describe())\n\n# Categorical example: sex\nif \"sex\" in df.columns:\n    print(\"\\nsex – categorical (nominal)\")\n    print(df[\"sex\"].value_counts(dropna=False))\n\n# Ordinal example: IMD_quintile\nif \"IMD_quintile\" in df.columns:\n    print(\"\\nIMD_quintile – stored as numbers, but conceptually ordered categories\")\n    print(df[\"IMD_quintile\"].value_counts(dropna=False).sort_index())\n\n\n\n\n1.4 1.3 How we (should) use different types\n\nFor continuous variables we normally use summaries such as mean, standard deviation, histograms, boxplots, and regression models with the variable on its original scale.\nFor categorical variables we usually use counts and proportions, and cross-tabulations (contingency tables), for example smoking_status × CVD_incident.\nFor ordinal variables, we technically have two options:\n\nTreat them as ordered categories\n(e.g. proportional-odds models, cumulative logit models, or ordered bar charts).\nThis respects the fact that the categories have an order but not a measurable distance.\nCode them as 0/1/2/3 and analyse them as if they were continuous\n(e.g. using them in linear regression).\n\n⚠️ This second option is risky because it quietly assumes:\n\nthe “gap” between low → moderate is the same size as moderate → high;\n\nand that those distances are meaningful on a numeric scale.\n\nThese assumptions are almost never justified.\nTreating ordinal scales as continuous can:\n\ndistort effect sizes,\n\nproduce misleading trends,\n\nviolate model assumptions,\n\nand give regression coefficients that look precise but have no real-world meaning.\n\n\nIt works by accident, not by principle.\n\n\nBest practice:\nUse ordinal models when possible; if you collapse the scale to treat it as continuous, justify it clearly and perform sensitivity checks.\nFor this module we keep a simple rule of thumb:\n\nOrdinal variables are not automatically continuous.\nIf you code them as numbers, you must still think about whether treating “low → moderate → high” as evenly spaced is sensible for your question.\n\n\n\n1.5 1.4 Simple validation example: smoking and CVD\nAs a quick sanity check, we can look at how often incident cardiovascular disease (CVD) occurs in different smoking categories.\n\nsmoking_status is a categorical exposure (for example, never / former / current).\nCVD_incident is a binary outcome (0 = no event, 1 = event).\n\nA natural question is:\n\n“Is incident CVD more common in some smoking groups than others?”\n\nTo explore this, we can use pandas.crosstab with normalize=\"index\":\n\nEach row corresponds to one smoking category.\nEach cell is the proportion of participants in that smoking group with / without incident CVD.\nRows therefore sum to 1 (100%).\n\nThis does not prove causality, and it ignores confounders (for example, age, sex, SES),\nbut it is a very useful descriptive check:\n\nDoes the pattern roughly match what we expect (higher CVD in current smokers than never smokers)?\n\nAre there any obviously strange values (for example, zero events in a large group)?\n\nIn the next cell we compute and display this row-normalised cross-tabulation.\n\nif {\"smoking_status\", \"CVD_incident\"}.issubset(df.columns):\n    tab = pd.crosstab(df[\"smoking_status\"], df[\"CVD_incident\"], normalize=\"index\")\n    print(\"Proportion with incident CVD by smoking status:\")\n    display(tab)\n\n\n\n1.6 1.5 Making ordinal variables explicit in the data\nSome variables in df are inherently ordered, even though they are stored as numbers or plain strings:\n\nIMD_quintile: 1 (most deprived) → 5 (least deprived).\n\nphysical_activity: \"low\" → \"moderate\" → \"high\".\n\nIf we leave them as ordinary integers/strings, pandas and most modelling functions will treat them either as:\n\npurely categorical (no order at all), or\n\ncontinuous (for example, using 1–5 as if the gaps were equal).\n\nBoth can be misleading:\n\nTreating them as unordered categories throws away the ranking information.\n\nTreating them as continuous quietly assumes that the “distance” from 1→2 is the same as 4→5, which is rarely justified.\n\nA better approach is to mark them explicitly as ordered categorical variables.\nThis preserves the ranking (1 &lt; 2 &lt; … &lt; 5; low &lt; moderate &lt; high) without pretending that the gaps are equal.\nIn the next cell we:\n\ncreate IMD_quintile_ord and physical_activity_ord as ordered Categorical variables;\n\nkeep the original columns so you can see both side by side.\n\nLater, when we model these variables, we can use methods appropriate for ordered data (for example, ordered bar plots, stratified summaries, or proportional-odds models), rather than treating them as plain numbers.\n\n\n\nif \"IMD_quintile\" in df.columns:\n    df[\"IMD_quintile_ord\"] = pd.Categorical(\n        df[\"IMD_quintile\"],\n        categories=[1, 2, 3, 4, 5],\n        ordered=True,\n    )\n\nif \"physical_activity\" in df.columns:\n    df[\"physical_activity_ord\"] = pd.Categorical(\n        df[\"physical_activity\"],\n        categories=[\"low\", \"moderate\", \"high\"],\n        ordered=True,\n    )\n\ncols_to_show = [\n    c for c in [\"IMD_quintile\", \"IMD_quintile_ord\", \"physical_activity\", \"physical_activity_ord\"]\n    if c in df.columns\n]\n\nif cols_to_show:\n    display(df[cols_to_show].head())"
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html#data-collection-pipelines",
    "href": "notebooks/1.04_data_collection_cleaning.html#data-collection-pipelines",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "2 2. Data collection pipelines",
    "text": "2 2. Data collection pipelines\nIn nutritional epidemiology, data usually come from several sources that are linked together:\n\nQuestionnaires and interviews (for example, food frequency questionnaires, 24-hour recalls, lifestyle questionnaires).\nLaboratory measurements (for example, blood biomarkers, urinary biomarkers).\nClinical examinations (for example, blood pressure, anthropometry).\nRegisters and administrative data (for example, hospital episode statistics, cancer registries, mortality data).\nPublic datasets and surveys (for example, NHANES in the United States, NDNS in the United Kingdom).\n\nTypical steps in a data collection pipeline are:\n\nSampling and recruitment of participants.\nBaseline data collection:\n\nQuestionnaires completed on paper, online, or via interview.\nClinical and anthropometric measurements.\nBiospecimen collection for later laboratory analysis.\n\nFollow-up data collection:\n\nRepeat questionnaires or clinic visits.\nLinkage to health registers to obtain information on disease outcomes.\n\nData entry, coding and merging:\n\nScanning or manual entry of questionnaires.\nCoding of free-text responses.\nMerging laboratory, questionnaire and register data using a unique participant identifier.\n\n\nThe synthetic FB2NEP dataset represents a merged cohort where these steps have already taken place. The underlying logic, however, is the same as in real-world studies such as NHANES or NDNS.\n\n2.1 2.1 Units and harmonisation\nWhen combining or comparing datasets, we must pay attention to units and coding schemes:\n\nBlood lipids: mmol/L (UK, Europe) vs mg/dL (US).\nHeight: centimetres vs inches.\nEducation and SES: different qualification systems across countries.\nPhysical activity: self-reported categories vs device-based measures.\n\nBefore merging or comparing datasets we should always check:\n\nUnits and reference ranges (for example, SBP in mmHg vs kPa).\n\nCoding manuals for questionnaires (for example, education categories).\n\nWhether local scores (for example, deprivation indices) are comparable at all.\n\nIn later workbooks, when we compare FB2NEP to NHANES or NDNS, we will see that harmonisation sometimes requires judgement and inevitably involves some information loss."
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html#coding-and-information-loss",
    "href": "notebooks/1.04_data_collection_cleaning.html#coding-and-information-loss",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "3 3. Coding and information loss",
    "text": "3 3. Coding and information loss\nWhen we turn real-world phenomena into variables, we always lose information – long before we fit any models.\nExamples:\n\nTurning height and weight into BMI loses information about body shape.\nTwo people with very different height and weight can have the same BMI.\nTurning continuous BMI into three categories (for example, normal / overweight / obese) throws away all variation within each band.\nTurning detailed income into a few SES categories (ABC1 vs C2DE) hides differences within each broad class.\n\nWhy do we code and collapse variables?\n\nCoding refers to the systematic assignment of numerical or categorical codes to raw data values (e.g., converting BMI into obesity categories such as 0 = underweight, 1 = normal, 2 = overweight, 3 = obese) to facilitate statistical analysis, grouping, or modelling.\n\n\nTo shorten questionnaires.\nTo make models simpler and more stable.\nTo produce tables that fit on one slide or in a paper.\n\nThe trade-off is:\n\nMore coding and categorisation → simpler communication,\nbut less information and sometimes more bias.\n\nIn practice, we should:\n\nkeep the rawest sensible version of each variable;\ndocument every coding step;\nthink about what has been lost at each stage.\n\n\n3.1 3.1 Example: collapsing continuous BMI into categories\nBMI itself is already a derived measure:\n\nIt combines height and weight into a single number (kg/m²);\nIt does not distinguish between muscle and fat mass;\nIt treats all people with the same BMI as equivalent, even if their body composition is very different.\n\nOn top of that, we often categorise BMI:\n\n“normal weight”, “overweight”, “obese”, sometimes with more bands;\nconvenient for guidelines and tables;\nbut it discards differences within each band\n(for example, BMI 30.1 vs 39.9 are both just “obese”).\n\n\nPlease note\nBMI cut-off points associated with metabolic risk differ across ethnic groups.\nSeveral populations develop type 2 diabetes at lower BMI than White populations.\n\n\n\n3.2 Ethnic-specific BMI thresholds for equivalent diabetes risk\nUsing a cohort of 1.47 million adults in England, Caleyachetty et al. (2021) compared the\nBMI level at which different ethnic groups have the same age- and sex-adjusted incidence\nof type 2 diabetes as White adults at BMI 30 kg/m².\n\n\n\n\n\n\n\n\nEthnic group\nBMI associated with equivalent diabetes risk\n95% CI\n\n\n\n\nWhite\n30.0 kg/m² (reference)\n–\n\n\nSouth Asian\n23.9 kg/m²\n23.6–24.0\n\n\nBlack\n28.1 kg/m²\n28.0–28.4\n\n\nChinese\n26.9 kg/m²\n26.7–27.2\n\n\nArab\n26.6 kg/m²\n26.5–27.0\n\n\n\nKey teaching point:\nA BMI of 30 kg/m² does not represent the same metabolic risk across all ethnicities.\nFor some groups (notably South Asian and Chinese populations), clinically relevant risk\nappears at much lower BMI values.\nSource:\nCaleyachetty R. et al. Lancet Diabetes & Endocrinology 2021;9:419–26.\nIn the code below we:\n\nStart from continuous BMI.\n\nCreate three broad categories using pd.cut.\n\nCompare the summary of the original BMI distribution with the counts in each category.\n\nThis is purely illustrative – it shows how easy it is to throw away information. In later weeks, we will discuss when categorisation might be useful and when it is better to keep BMI continuous (or use other measures such as waist circumference).\n\n3.2.1 3.1.1 Visual example: same BMI, different body composition\nBMI cannot distinguish between fat mass and muscle mass.\nIn the illustration below, both individuals have (hypothetically) the same BMI, but very different body composition:\n\none predominantly muscular,\none with higher body fat.\n\n\n\n\nTwo individuals with identical BMI but different body composition\n\n\nThis is why BMI is useful as a crude screening tool, but not a perfect measure of adiposity in individuals.\n\n# 3.1 Example: collapsing continuous BMI into categories\n\nif \"BMI\" in df.columns:\n    bmi = df[\"BMI\"].copy()\n\n    # Very simple categories for illustration only\n    bmi_cat = pd.cut(\n        bmi,\n        bins=[0, 25, 30, np.inf],\n        labels=[\"normal\", \"overweight\", \"obese\"],\n        right=False,\n    )\n\n    print(\"Original BMI (continuous) summary:\")\n    display(bmi.describe())\n\n    print(\"\\nBMI categories (illustrative):\")\n    print(bmi_cat.value_counts(dropna=False))\n\n\n\n3.2.2 3.2 Self-reported height and weight\nMany large studies (including some national surveys) use self-reported height and weight instead of measured values.\nTypical patterns:\n\nPeople tend to over-report height.\nPeople tend to under-report weight.\nThe bias is not random – it often depends on sex, age, and actual body size.\n\nConsequences:\n\nBMI from self-report is usually biased downwards.\nMisclassification across BMI categories (normal / overweight / obese) is common.\nAssociations between BMI and health outcomes can be biased if we ignore this measurement error.\n\nFor teaching purposes, it is helpful to remember that:\n\nBMI combines two measured variables (height, weight)\nand those measurements themselves may be biased."
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html#identifying-implausible-or-inconsistent-values",
    "href": "notebooks/1.04_data_collection_cleaning.html#identifying-implausible-or-inconsistent-values",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "4 4. Identifying implausible or inconsistent values",
    "text": "4 4. Identifying implausible or inconsistent values\nEven carefully collected data may contain values that are implausible or wrong. Examples include:\n\nAnthropometric values outside physiological limits (for example, BMI &lt; 10 kg/m²).\nBlood pressure values that are extremely low or high.\nEnergy intakes that are incompatible with life over the long term.\nMen who are recorded as being “post-menopausal”.\n\nIn practice we define simple rules to flag such values for review, and in some cases to exclude them from analysis.\n\n# 4.1 Example: BMI range check\n#\n# These cut-offs are illustrative. In a real study they should be\n# chosen with clinical input and clear documentation.\n\nif \"BMI\" in df.columns:\n    print(\"Summary of BMI:\")\n    display(df[\"BMI\"].describe())\n\n    implausible_bmi = df[(df[\"BMI\"] &lt; 10) | (df[\"BMI\"] &gt; 70)]\n    print(f\"Number of participants with BMI &lt; 10 or &gt; 70: {len(implausible_bmi)}\")\n    display(implausible_bmi.head())\n\n\n# 4.2 Example: SBP (systolic blood pressure) range check\n\nif \"SBP\" in df.columns:\n    print(\"Summary of SBP:\")\n    display(df[\"SBP\"].describe())\n\n    implausible_sbp = df[(df[\"SBP\"] &lt; 70) | (df[\"SBP\"] &gt; 260)]\n    print(f\"Number of participants with SBP &lt; 70 or &gt; 260 mmHg: {len(implausible_sbp)}\")\n    display(implausible_sbp.head())\n\n\n# 4.3 Distribution of BMI – visual check\n\n%matplotlib inline\n\nif \"BMI\" in df.columns:\n    plt.figure(figsize=(6, 4))\n    df[\"BMI\"].hist(bins=30)\n    plt.xlabel(\"BMI (kg/m²)\")\n    plt.ylabel(\"Number of participants\")\n    plt.title(\"Distribution of BMI – initial check\")\n    plt.tight_layout()\n    plt.show()\n\n\n4.1 4.4 Energy Intake Plausibility and the Goldberg Cut-off\nIn nutritional epidemiology, a recurring question is whether reported energy intake (EI) is compatible with the energy a person is expected to expend.\nThe Goldberg cut-off is a widely used, reproducible approach to detect implausible self-reported energy intakes.\n\n4.1.1 1. Core idea\nThe method compares:\n\\[\n\\frac{\\text{EI}}{\\text{EE}}\n\\]\nwhere EE is the minimum energy expenditure expected for that person.\nIf the ratio is too low, the individual is likely to be an under-reporter.\n\n\n4.1.2 2. Steps in the Goldberg method\n\nEstimate Basal Metabolic Rate (BMR)\nUsually via predictive equations (Schofield, Henry, or similar) using:\n\nage\n\nsex\n\nweight (and occasionally height)\n\nAssume a minimum plausible Physical Activity Level (PAL)\nA typical assumption is PAL = 1.55 for a “sedentary to lightly active” lifestyle.\nThis gives the minimum Total Energy Expenditure (TEE):\n\n\\[\n   \\text{TEE}_{\\min} = \\text{BMR} \\times \\text{PAL}\n\\]\n\nCompute the Goldberg ratio\n\n\\[\n   r = \\frac{\\text{Reported EI}}{\\text{TEE}_{\\min}}\n\\]\n\nCompare the ratio to a cut-off\nA common lower limit is 0.76–0.80, derived by combining:\n\nday-to-day variation in energy intake,\nvariation in BMR prediction,\nsampling duration.\n\nIf \\[ r &lt; \\text{cut-off} \\], the intake is considered implausibly low.\n\n\n\n4.1.3 3. Interpretation\nThe rule does not say the individual is deliberately misreporting.\nIt indicates that the reported intake is statistically incompatible with sustaining basic physiological requirements over time.\nExamples: - A reported EI of 900 kcal/day for a healthy adult is implausible.\n- A physically active person reporting 1,200 kcal/day may also be flagged.\n\n\n4.1.4 4. Why the Goldberg cut-off is widely used\n\nTransparent and fully formula-based.\n\nSimple and implementable in any dataset.\n\nConsistent across studies.\n\nHelps identify outliers before modelling.\n\nHowever, the method is designed for population-level plausibility screening, not as a diagnostic tool for individual behaviour.\n\n\n\n\n4.2 4.5 Limitations of the Goldberg Cut-off\nAlthough influential, the Goldberg approach has important limitations:\n\n4.2.1 (a) Strong assumptions about PAL\nUsing a fixed PAL (for example 1.55) ignores the wide variation in physical activity between individuals.\nTEE may be substantially higher or lower than assumed, causing: - false positives (flagged as under-reporting but plausible),\n- false negatives (severe under-reporting may not be flagged if activity is low).\n\n\n4.2.2 (b) BMR prediction error\nPredictive equations for BMR introduce error: - ±5–10% for many individuals, - larger for those with atypical body composition.\nIf BMR is misestimated, the Goldberg ratio becomes misleading.\n\n\n4.2.3 (c) One-day (or few-day) intake is not habitual intake\nShort-term 24-h recalls show large day-to-day variation, making the ratio unstable.\n\n\n4.2.4 (d) Cannot distinguish intentional dieting\nLow reported intakes may reflect: - short-term restriction, - illness, - poor recall, - dieting behaviour.\nThe cut-off treats all as “implausible”.\n\n\n4.2.5 (e) Systematic bias in self-reporting\nEnergy misreporting is not random: - overweight individuals tend to under-report energy, especially snacks and high-fat foods; - some demographic groups systematically under- or over-report.\nThus the Goldberg decision can itself introduce bias, for example: - removing more participants with high BMI, - altering diet–BMI or diet–disease associations.\nThis concern is highlighted in recent evaluation studies.\n\n\n\n\n4.3 4.6 Recent evidence and critique\nA recent paper in Nature Food Bajunaid et al., 2025 “Energy intake and expenditure in doubly labelled water studies worldwide”\nprovides an updated perspective:\n\nDoubly labelled water (DLW) studies show large inter-individual variation in TEE, wider than commonly assumed in Goldberg calculations.\nHabitual EI is often higher than self-reported, but misreporting varies by age, sex, BMI and cultural context.\nThe study stresses that physiological variation is larger than the Goldberg framework assumes.\nTherefore, using a universal PAL and narrow cut-offs may systematically misclassify valid data.\n\nThis supports a broader consensus that:\n\nThe Goldberg cut-off is useful as a descriptive tool,\nbut insufficient for high-stakes decisions (for example, excluding participants).\n\n\n\n\n4.4 Takeaway\nThe Goldberg cut-off is a useful first-pass plausibility screen, but:\n\nit rests on strong assumptions,\nit is sensitive to prediction error,\nand it can introduce bias if used uncritically.\n\nModern datasets—with diverse populations, high variation in physical activity, and systematic misreporting—require more nuanced approaches, ideally supported by validation studies (for example, DLW data).\nIn FB2NEP we therefore use the Goldberg principle to illustrate:\n\nhow plausibility checks work,\nwhere modelling decisions come from,\nand why epidemiological analyses require transparent assumptions.\n\n\n# 4.5 Simple exploration of energy intake\n\nif \"energy_kcal\" in df.columns:\n    print(\"Summary of reported energy intake (kcal/d):\")\n    display(df[\"energy_kcal\"].describe())\n\n    plt.figure(figsize=(6, 4))\n    df[\"energy_kcal\"].hist(bins=30)\n    plt.xlabel(\"Energy intake (kcal/day)\")\n    plt.ylabel(\"Number of participants\")\n    plt.title(\"Distribution of reported energy intake\")\n    plt.tight_layout()\n    plt.show()"
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html#plausibility-and-scale-hippo-vs-mouse-energy-intake",
    "href": "notebooks/1.04_data_collection_cleaning.html#plausibility-and-scale-hippo-vs-mouse-energy-intake",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "5 4.5 Plausibility and scale: hippo vs mouse energy intake",
    "text": "5 4.5 Plausibility and scale: hippo vs mouse energy intake\nThink of reported energy intake:\n\nA very small human reporting 15 000 kcal/day is hippo-level – probably implausible.\nA large human reporting 600 kcal/day long term is mouse-level – probably implausible too.\n\nThe FB2NEP dataset includes energy_kcal. When we look at its distribution we will:\n\ncheck whether values lie in a broadly plausible human range, and\n\nremember that even plausible values can still be under- or over-reported.\n\nLater, you will encounter the Goldberg cut-off, which formalises this idea by comparing reported intake with estimated energy requirements."
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html#rare-categories-and-prefer-not-to-say",
    "href": "notebooks/1.04_data_collection_cleaning.html#rare-categories-and-prefer-not-to-say",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "6 5. Rare categories and “Prefer not to say”",
    "text": "6 5. Rare categories and “Prefer not to say”\nQuestionnaires often include categories such as “Prefer not to say” or very rare responses. These raise practical questions:\n\nShould they be combined with another category?\nShould they be treated as missing data?\nDo they indicate a problem with the question (for example, perceived sensitivity)?\n\nSome patterns can be red flags:\n\nA question that is consistently skipped (many missing values) compared with others.\n→ Possible misunderstanding, poor wording, or discomfort with the topic.\nA single participant who answers “Prefer not to say” to almost everything.\n→ Possible disengagement; their data may be low quality overall.\nSystematic non-response to certain topics (for example, income, alcohol).\n→ Missingness may be MNAR and needs careful handling.\n\n\n6.1 5.1 Test questions and internal consistency\nSome questionnaires include one or two test questions to check whether participants are paying attention. Typical examples are:\n\n“To show that you are paying attention, please select ‘strongly agree’ for this statement.”\n\n“For this question only, please tick ‘never’.”\n\n“The colour of the sky on a clear day is… (please tick ‘blue’).”\n\nOther examples are factual statements about well-known places such as:\n\n“Wasilla, Alaska is in the continental United States — please select ‘Strongly disagree’.”\n“Barrow, Alaska never receives snow — please select ‘Disagree’.”\n\nThese items are widely used in survey-methods teaching because the correct answer is obvious, yet respondents who are not reading the questions carefully will fail them.\nIf participants fail these items, their responses might be low quality.\n\nThis is common in online surveys and panel studies.\nIn large epidemiological cohorts the practice is less standard, but the idea is similar: we want to detect obviously invalid or careless data.\n\nInstead of explicit test questions, cohort studies often rely on internal consistency checks, for example:\n\nA participant reporting “never drinks alcohol” but also giving detailed answers about\nnumber of glasses of wine per week and maximum units on one occasion.\n\nReporting 0 minutes of physical activity per week, but also stating that they cycle to work every day.\n\nMarking “current smoker” and “has never smoked regularly” on different pages.\n\nA male participant answering “post-menopausal” or reporting age at menopause.\n\nFB2NEP does not include explicit test questions, but you can treat these kinds of internal contradictions as serving a similar purpose: they flag records that may need closer inspection or, in some analyses, exclusion.\n\n# 5.2 Simple frequency tables for key categorical variables\n\nfor col in [\"sex\", \"SES_class\", \"smoking_status\", \"physical_activity\"]:\n    if col in df.columns:\n        print(f\"\\nValue counts for {col}:\")\n        print(df[col].value_counts(dropna=False))"
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html#missing-data-mcar-mar-mnar-overview",
    "href": "notebooks/1.04_data_collection_cleaning.html#missing-data-mcar-mar-mnar-overview",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "7 6. Missing data: MCAR, MAR, MNAR (overview)",
    "text": "7 6. Missing data: MCAR, MAR, MNAR (overview)\nAlmost all real datasets contain missing values. Three concepts are important:\n\nMCAR (Missing Completely At Random):\n\nThe probability of missingness is unrelated to observed or unobserved data.\nExample: a random sample of blood tubes is lost in the post.\n\nMAR (Missing At Random):\n\nThe probability of missingness depends only on observed variables.\nExample: older participants are less likely to provide a urine sample, but age is recorded.\n\nMNAR (Missing Not At Random):\n\nThe probability of missingness depends on unobserved values.\nExample: participants with very high alcohol intake are less likely to report their intake.\n\n\nIn the synthetic dataset we have:\n\nMCAR-type missingness on some biomarker and diet variables.\nMAR-type missingness depending on age and deprivation.\nSmall MNAR components for alcohol and BMI.\n\nWe start by computing the proportion missing in each variable.\n\n# 6.1 Proportion of missing values in each variable\n\nmissing_fraction = df.isna().mean().sort_values(ascending=False)\nmissing_fraction.head(20)\n\n\n# 6.2 Visualising missingness for the top 25 variables\n\nplt.figure(figsize=(10, 4))\nmissing_fraction.head(25).plot(kind=\"bar\")\nplt.ylabel(\"Proportion missing\")\nplt.title(\"Proportion of missing values (top 25 variables)\")\nplt.tight_layout()\nplt.show()\n\n\n# 6.3 Example: does missing BMI depend on age?\n\nif {\"BMI\", \"age\"}.issubset(df.columns):\n    df[\"BMI_missing\"] = df[\"BMI\"].isna()\n    print(\"Age distribution by BMI missingness:\")\n    display(df.groupby(\"BMI_missing\")[\"age\"].describe())\n\n\n# 6.4 Example: does missing fruit_veg_g_d depend on IMD_quintile? (MAR pattern)\n\nif {\"fruit_veg_g_d\", \"IMD_quintile\"}.issubset(df.columns):\n    df[\"FV_missing\"] = df[\"fruit_veg_g_d\"].isna()\n    tab = pd.crosstab(df[\"IMD_quintile\"], df[\"FV_missing\"], normalize=\"index\")\n    print(\"Proportion missing fruit_veg_g_d by IMD_quintile:\")\n    display(tab)"
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html#simple-validation-checks",
    "href": "notebooks/1.04_data_collection_cleaning.html#simple-validation-checks",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "8 7. Simple validation checks",
    "text": "8 7. Simple validation checks\nFinally we perform a few simple validation checks:\n\nAre there any men with a non-“NA” menopausal status?\nDo key variables have reasonable distributions by sex or SES?\nAre any obvious coding errors visible when we group by categories?\n\nThese checks will inform later modelling decisions.\n\n# 7.1 Check that menopausal_status is only set for women\n\nif {\"sex\", \"menopausal_status\"}.issubset(df.columns):\n    inconsistent = df[(df[\"sex\"] == \"M\") & (df[\"menopausal_status\"] != \"NA\")]\n    print(f\"Number of men with non-NA menopausal_status: {len(inconsistent)}\")\n    display(inconsistent.head())\n\n\n# 7.2 Check SBP distribution by sex\n\nif {\"SBP\", \"sex\"}.issubset(df.columns):\n    print(\"SBP by sex:\")\n    display(df.groupby(\"sex\")[\"SBP\"].describe())"
  },
  {
    "objectID": "notebooks/1.04_data_collection_cleaning.html#summary",
    "href": "notebooks/1.04_data_collection_cleaning.html#summary",
    "title": "1.04 – Data Collection and Cleaning",
    "section": "9 8. Summary",
    "text": "9 8. Summary\n\nWe distinguished between continuous, categorical, ordinal, and count variables, and saw how they are stored and summarised in Python.\nWe discussed how coding (for example, collapsing BMI or SES) always involves some loss of information, which must be balanced against simplicity.\nWe outlined typical data collection pipelines in nutritional epidemiology and highlighted the importance of units and harmonisation across data sources.\nWe used simple plausibility checks to flag extreme values for BMI, SBP, and energy intake, and introduced the idea behind the Goldberg cut-off.\nWe explored rare categories, “Prefer not to say”, and the idea of test questions and internal consistency.\nWe introduced the concepts of MCAR, MAR and MNAR missingness and examined missing patterns for selected variables.\nWe performed a few validation checks (for example, menopausal status in men) to illustrate how coding errors can be detected early.\n\nThese steps are part of routine data cleaning in nutritional epidemiology and prepare the ground for later workbooks on sampling, representativeness, and modelling."
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html",
    "href": "notebooks/2.03_health_inequalities.html",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "",
    "text": "Learning Objectives: - Understand the Index of Multiple Deprivation (IMD) and how it is constructed - Distinguish between life expectancy and healthy life expectancy - Understand the distinction between absolute and relative measures of inequality - Calculate the Slope Index of Inequality (SII) and Relative Index of Inequality (RII) - Interpret concentration curves and concentration indices - Apply these measures to dietary intake and nutrition-related health outcomes"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#introduction-why-measure-inequalities",
    "href": "notebooks/2.03_health_inequalities.html#introduction-why-measure-inequalities",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "1 1. Introduction: Why Measure Inequalities?",
    "text": "1 1. Introduction: Why Measure Inequalities?\nAverage population health can improve while inequalities widen. Consider:\n\nLife expectancy in England increased for all groups between 2001-2019\nBut the gap between the most and least deprived areas also increased\n\nIf we only track averages, we miss this divergence. Health inequality metrics help us:\n\nDescribe the current distribution of health across social groups\nMonitor whether policies are reducing or widening gaps\nTarget interventions toward those in greatest need\n\n\n1.1 The Marmot Curve\nHealth follows a gradient — it’s not simply that the poorest are unhealthy. Each step up the socioeconomic ladder is associated with better health outcomes."
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#setup",
    "href": "notebooks/2.03_health_inequalities.html#setup",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "2 2. Setup",
    "text": "2 2. Setup\n\n# ============================================================\n# Bootstrap cell (works both locally and in Colab)\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\n\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\ncwd = pathlib.Path.cwd()\n\nif (cwd / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd\nelif (cwd.parent / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd.parent\nelse:\n    repo_root = cwd / REPO_DIR\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nscripts_dir = repo_root / \"scripts\"\nif str(scripts_dir) not in sys.path:\n    sys.path.insert(0, str(scripts_dir))\n\nprint(f\"Repository root: {repo_root}\")\nprint(\"Bootstrap completed successfully.\")\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nfrom ipywidgets import FloatSlider, VBox, Output\nimport ipywidgets as widgets\nfrom IPython.display import display, HTML\nfrom pathlib import Path\nimport openpyxl\n\nfrom epi_utils import (\n    calculate_sii, calculate_rii, calculate_concentration_index,\n    plot_concentration_curve, INEQUALITY_EXAMPLE_DATA, fit_sii_rii\n)\n\n# ————— Paths —————\nDATA_DIR = Path(\"../data\")\nIMD_CSV = DATA_DIR / \"IoD2019_ranks.csv\"\nLE_CSV  = DATA_DIR / \"le_imd.csv\"\nhse_path = DATA_DIR / \"HSE-2022-Overweight-and-obesity-tables.xlsx\"\nOWID_path = DATA_DIR / \"le_gdp.csv\"\ngini_path = DATA_DIR / \"gini.csv\"\n\n\nplt.style.use('seaborn-v0_8-whitegrid')\nplt.rcParams['figure.figsize'] = [10, 6]\nnp.random.seed(42)\n\nprint(\"Libraries loaded successfully.\")"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#the-index-of-multiple-deprivation-imd",
    "href": "notebooks/2.03_health_inequalities.html#the-index-of-multiple-deprivation-imd",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "3 3. The Index of Multiple Deprivation (IMD)",
    "text": "3 3. The Index of Multiple Deprivation (IMD)\n\n3.1 What is the IMD?\nThe Index of Multiple Deprivation (IMD) is the official measure of relative deprivation for small areas in England. It ranks every Lower Layer Super Output Area (LSOA) — approximately 32,844 areas, each containing around 1,500 people.\n\n\n3.2 The Seven Domains of Deprivation\n\n\n\n\n\n\n\n\nDomain\nWeight\nWhat it measures\n\n\n\n\nIncome\n22.5%\nProportion of population on low income\n\n\nEmployment\n22.5%\nProportion involuntarily excluded from work\n\n\nEducation\n13.5%\nLack of attainment and skills\n\n\nHealth & Disability\n13.5%\nPremature death and impairment\n\n\nCrime\n9.3%\nRisk of victimisation\n\n\nBarriers to Housing\n9.3%\nPhysical and financial accessibility\n\n\nLiving Environment\n9.3%\nIndoor and outdoor environment quality\n\n\n\n\n\n\nIMD Domains\n\n\n(https://assets.publishing.service.gov.uk/media/5d8e26f6ed915d5570c6cc55/IoD2019_Statistical_Release.pdf)\n\n\n3.3 Important Notes\n\nRelative, not absolute: IMD ranks areas relative to each other\nArea-level, not individual: Not everyone in a deprived area is deprived (ecological fallacy)\nEngland only: Scotland, Wales, and Northern Ireland have separate indices\n\n\n# IMD domain weights visualisation\nimd_domains = pd.DataFrame({\n    'Domain': ['Income', 'Employment', 'Education', 'Health & Disability', \n               'Crime', 'Barriers to Housing', 'Living Environment'],\n    'Weight (%)': [22.5, 22.5, 13.5, 13.5, 9.3, 9.3, 9.3]\n})\n\nfig, ax = plt.subplots(figsize=(10, 6))\ncolors = plt.cm.Set3(np.linspace(0, 1, 7))\nax.pie(imd_domains['Weight (%)'], labels=imd_domains['Domain'],\n       autopct='%1.1f%%', colors=colors, startangle=90)\nax.set_title('Index of Multiple Deprivation: Domain Weights', fontsize=14)\nplt.tight_layout()\nplt.show()\n\nprint(\"Note: Income and Employment together account for 45% of the overall IMD.\")\n\n\n\n3.4 Exploring IMD Maps\nKey Resources:\n\nONS Interactive IMD Map — Explore deprivation by LSOA\nCDRC IMD Geodata Pack — Download IMD data\nPHE Fingertips — Health outcomes by deprivation\n\n\n# Display link to interactive IMD map\ndisplay(HTML('''\n&lt;div style=\"text-align: center; padding: 20px; background-color: #f5f5f5; border-radius: 10px;\"&gt;\n    &lt;h3&gt;Index of Multiple Deprivation 2019 - England&lt;/h3&gt;\n    &lt;p&gt;&lt;a href=\"https://dclgapps.communities.gov.uk/imd/iod_index.html\" target=\"_blank\"&gt;\n        Click here to explore the interactive IMD map →\n    &lt;/a&gt;&lt;/p&gt;\n    &lt;p style=\"font-size: 12px; color: #666;\"&gt;Source: MHCLG (2019)&lt;/p&gt;\n&lt;/div&gt;\n'''))\n\n\n\n# ————— Load the full CSV —————\ndf = pd.read_csv(IMD_CSV)\n\n# ————— Extract LA-level summary —————\n# Select only the columns we need\ndf_la = (\n    df[[\n        \"Local Authority District name (2019)\",\n        \"Index of Multiple Deprivation (IMD) Rank (where 1 is most deprived)\",\n        \"Index of Multiple Deprivation (IMD) Score\"\n    ]]\n    .dropna()\n    # Rename for convenience\n    .rename(columns={\n        \"Local Authority District name (2019)\": \"Local Authority\",\n        \"Index of Multiple Deprivation (IMD) Rank (where 1 is most deprived)\": \"IMD Rank\",\n        \"Index of Multiple Deprivation (IMD) Score\": \"IMD Score\"\n    })\n    .drop_duplicates()\n    # Sort so rank=1 is top\n    .sort_values(\"IMD Rank\")\n    .reset_index(drop=True)\n)\n\nprint(\"Loaded IMD 2019 LA ranks:\")\n\n# \n# Most and least deprived local authorities\nla_deprivation = pd.DataFrame({\n    'Local Authority': ['Blackpool', 'Knowsley', 'Hull', 'Middlesbrough', 'Liverpool',\n                        'Hart', 'Wokingham', 'Surrey Heath', 'S. Cambridgeshire', 'Elmbridge'],\n    'Rank (of 317)': [1, 2, 3, 4, 5, 317, 316, 315, 314, 313],\n    'Category': ['Most deprived'] * 5 + ['Least deprived'] * 5\n})\n\nprint(\"Most and Least Deprived Local Authorities (IMD 2019)\")\nprint(\"=\" * 60)\ndisplay(la_deprivation)\n\nMost local authorities cluster across the distribution; only a few lie at the extremes. So where are we?\n\ndf_plot = df_la.sort_values(\"IMD Score\").reset_index(drop=True)\n\n\n\n# Boolean mask for Wokingham\nis_wokingham = df_plot[\"Local Authority\"] == \"Wokingham\"\n\n# Locate Wokingham\nwk_idx = df_plot.index[df_plot[\"Local Authority\"] == \"Wokingham\"][0]\nwk_score = df_plot.loc[wk_idx, \"IMD Score\"]\n\n# X positions (one per authority)\nx = np.arange(len(df_plot))\n\nplt.figure(figsize=(10, 5))\n\n# All authorities (background)\nplt.bar(\n    x,\n    df_plot[\"IMD Score\"],\n    alpha=0.3\n)\n\n# Arrow pointing to Wokingham\nplt.annotate(\n    \"Wokingham\",\n    xy=(wk_idx, wk_score),\n    xytext=(wk_idx, wk_score + 10),  # vertical offset\n    arrowprops=dict(\n        arrowstyle=\"-&gt;\",\n        linewidth=1.5\n    ),\n    ha=\"center\"\n)\n\nplt.annotate(\n    \"\",\n    xy=(0.95, -0.12),\n    xytext=(0.05, -0.12),\n    xycoords=\"axes fraction\",\n    textcoords=\"axes fraction\",\n    arrowprops=dict(arrowstyle=\"-&gt;\", linewidth=1.2),\n)\n\n# End labels\nplt.text(\n    0.02, -0.12, \"least deprived\",\n    transform=plt.gca().transAxes,\n    ha=\"left\", va=\"center\"\n)\n\nplt.text(\n    0.98, -0.12, \"most deprived\",\n    transform=plt.gca().transAxes,\n    ha=\"right\", va=\"center\"\n)\n\n# Axis handling\nplt.xticks([])  # remove unreadable labels\nplt.ylabel(\"IMD Score\")\nplt.title(\"Index of Multiple Deprivation (2019)\\nWokingham highlighted\")\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#life-expectancy-vs-healthy-life-expectancy",
    "href": "notebooks/2.03_health_inequalities.html#life-expectancy-vs-healthy-life-expectancy",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "4 4. Life Expectancy vs Healthy Life Expectancy",
    "text": "4 4. Life Expectancy vs Healthy Life Expectancy\n\n4.1 Definitions\n\nLife expectancy (LE): Average years a newborn would live if current mortality rates persist\nHealthy life expectancy (HLE): Average years expected in “good” or “very good” health\n\n\n\n4.2 The Double Burden of Deprivation\nThe gap between LE and HLE represents years lived in poor health. In deprived areas, people: 1. Die younger 2. Spend a larger proportion of their shorter lives in poor health\n\nle_hle_data = pd.read_csv(LE_CSV)\n\n\n# Sort in the natural order (most deprived -&gt; least deprived)\nle_hle_data = le_hle_data.sort_values(\"IMD_decile\").reset_index(drop=True)\n\nprint(\"Life Expectancy and Healthy Life Expectancy by IMD Decile (England, 2020–2022)\")\ndisplay(le_hle_data[[\"IMD_decile\", \"LE_male\", \"HLE_male\", \"LE_female\", \"HLE_female\"]])\n\nThe State Pension Age is 66 (rising to 67 by 2028). In the most deprived areas, men reach their HLE 14.5 years BEFORE pension age.\n\npension_age = 66\n\n# X positions\nx = np.arange(len(le_hle_data))\nwidth = 0.35\n\nfig, ax = plt.subplots(figsize=(12, 6))\n\n# Colour-blind safe palette (Okabe–Ito)\nmale_colour = \"#0072B2\"    # blue\nfemale_colour = \"#E69F00\"  # orange\n\n# ---- Male bars ----\nax.bar(\n    x - width/2,\n    le_hle_data[\"LE_male\"],\n    width,\n    label=\"Male LE\",\n    color=male_colour,\n    alpha=0.6,\n    hatch=\"\"\n)\nax.bar(\n    x - width/2,\n    le_hle_data[\"HLE_male\"],\n    width,\n    label=\"Male HLE\",\n    color=male_colour,\n    hatch=\"//\"\n)\n\n# ---- Female bars ----\nax.bar(\n    x + width/2,\n    le_hle_data[\"LE_female\"],\n    width,\n    label=\"Female LE\",\n    color=female_colour,\n    alpha=0.6,\n    hatch=\"\"\n)\nax.bar(\n    x + width/2,\n    le_hle_data[\"HLE_female\"],\n    width,\n    label=\"Female HLE\",\n    color=female_colour,\n    hatch=\"//\"\n)\n\n# ---- Axes and labels ----\nax.set_xticks(x)\nax.set_xticklabels([f\"D{d}\" for d in le_hle_data[\"IMD_decile\"]])\nax.set_xlabel(\"IMD decile (D1 = most deprived)\")\nax.set_ylabel(\"Years\")\nax.set_title(\n    \"Life expectancy (LE) and healthy life expectancy (HLE)\\n\"\n    \"by IMD decile and sex, England (2020–2022)\"\n)\nax.set_ylim(0, 90)\n\n# ---- Pension age line ----\nax.axhline(\n    y=pension_age,\n    linestyle=\"--\",\n    linewidth=2,\n    label=f\"State Pension Age ({pension_age})\"\n)\n\n# ---- Legend ----\nax.legend(ncol=2, loc=\"upper left\")\n\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#measuring-inequality-sii-and-rii",
    "href": "notebooks/2.03_health_inequalities.html#measuring-inequality-sii-and-rii",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "5 5. Measuring Inequality: SII and RII",
    "text": "5 5. Measuring Inequality: SII and RII\nWhen comparing D1 vs D10, only two points on the deprivation gradient are used.\nThis is simple, but it discards information and is sensitive to noise at the extremes.\nThe Slope Index of Inequality (SII) and Relative Index of Inequality (RII) were designed to do something more principled:\n\nuse all groups (all deciles, quintiles, etc.)\naccount for group population size (if groups are unequal)\nsummarise the entire social gradient with a single number\nallow comparison across places and time\n\nThey are widely used in health inequalities research for precisely these reasons."
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#slope-index-of-inequality-sii",
    "href": "notebooks/2.03_health_inequalities.html#slope-index-of-inequality-sii",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "6 Slope Index of Inequality (SII)",
    "text": "6 Slope Index of Inequality (SII)\n\n6.1 What SII measures\nSII is an absolute measure of inequality.\nIt estimates the absolute difference in the outcome between the theoretical extremes of the deprivation distribution:\n\npredicted outcome at the least deprived end minus predicted outcome at the most deprived end\n\nIf the outcome is healthy life expectancy (HLE), SII is measured in years.\nExample interpretation:\n\nAn SII of 18 years for HLE means that, across the deprivation gradient, the fitted difference between the least and most deprived ends is about 18 healthy years.\n\n\n\n6.2 Why “theoretical” extremes?\nSII does not simply compare observed values in D1 and D10.\nInstead, it models deprivation as a continuous gradient from 0 to 1 and estimates outcomes at those endpoints.\nThis makes SII more stable than a simple top–bottom gap."
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#from-categories-to-a-gradient-the-ridit-score",
    "href": "notebooks/2.03_health_inequalities.html#from-categories-to-a-gradient-the-ridit-score",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "7 From categories to a gradient: the ridit score",
    "text": "7 From categories to a gradient: the ridit score\nDeprivation is observed in categories (deciles), but inequality is a gradient concept.\n\n7.1 Step 1: cumulative population position\nAssuming equal-sized deciles:\n\nDecile 1 spans ranks 0.0–0.1\n\nDecile 2 spans 0.1–0.2\n\n…\n\nDecile 10 spans 0.9–1.0\n\n\n\n7.2 Step 2: midpoint (ridit)\nEach group is assigned the midpoint of its interval:\n\n\n\nDecile\nRidit\n\n\n\n\nD1\n0.05\n\n\nD2\n0.15\n\n\n…\n…\n\n\nD10\n0.95\n\n\n\nThese ridit values represent relative social position and become the x-values in the regression.\nIf groups differ in population size, midpoints are computed from the cumulative population distribution instead. This is one reason SII is preferred over simple gaps."
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#regression-formulation",
    "href": "notebooks/2.03_health_inequalities.html#regression-formulation",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "8 Regression formulation",
    "text": "8 Regression formulation\nThe outcome y (e.g. HLE) is regressed on the ridit r:\ny = α + βr\n\nr ∈ [0, 1]\nα: predicted outcome at the most deprived end\nα + β: predicted outcome at the least deprived end\n\nTherefore:\nSII = (α + β) − α = β\nIn a linear model, SII is simply the slope, provided the rank variable runs from 0 to 1.\n\n8.1 Interpretation\n\nPositive SII: outcome increases with decreasing deprivation (typical for favourable health outcomes)\nNegative SII: outcome higher in more deprived groups\n\nBecause SII is absolute, it uses the units of the outcome: - years (LE, HLE) - mmHg (blood pressure) - percentage points (prevalence)"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#why-using-all-points-matters",
    "href": "notebooks/2.03_health_inequalities.html#why-using-all-points-matters",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "9 Why using all points matters",
    "text": "9 Why using all points matters\nUsing all groups rather than only extremes provides:\n\n9.1 1. Greater stability\nExtreme groups can be noisy. A fitted gradient uses all observations.\n\n\n9.2 2. Representativeness\nSII summarises the entire gradient, not just its endpoints.\n\n\n9.3 3. Population weighting\nIf group sizes differ, SII can reflect the distribution of people rather than categories."
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#assumptions-behind-sii",
    "href": "notebooks/2.03_health_inequalities.html#assumptions-behind-sii",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "10 Assumptions behind SII",
    "text": "10 Assumptions behind SII\n\nApproximate linearity\nSII is a linear summary. If the gradient is curved, SII still exists but compresses that pattern into a single number.\nMeaningful ordering\nDeprivation categories must be ordinal (IMD satisfies this).\nAppropriate outcome model\n\nContinuous outcomes: linear regression\n\nRates or proportions: suitable GLMs may be preferable\n\n\nSII answers:\n\n“What is the best linear summary of inequality across the social gradient?”\n\n\n# Read the table \"as is\" (HSE tables are not tidy spreadsheets)\nt6 = pd.read_excel(hse_path, sheet_name=\"Table 6\", header=None)\n\n# Quintile labels are on row 3, columns 1..5 in this workbook\nquintile_labels = list(t6.iloc[3, 1:6].values)\n\n# Helper: find a row by exact label in column 0\ndef _row_idx(label: str) -&gt; int:\n    matches = t6.index[t6[0].astype(str).str.strip().eq(label)]\n    if len(matches) == 0:\n        raise ValueError(f\"Could not find row label: {label!r}\")\n    return int(matches[0])\n\n# Row indices for obesity prevalence (% Obese) for each sex\nrow_male_obese = _row_idx(\"% Obese\")          # first occurrence is in the Men section\nrow_women = _row_idx(\"Women\")                 # marker row; women section starts here\nrow_female_obese = t6.index[(t6.index &gt; row_women) & (t6[0].astype(str).str.strip().eq(\"% Obese\"))][0]\n\n# Row indices for weighted bases (Men and Women)\nrow_weighted_bases = _row_idx(\"Weighted bases\")\nrow_male_weight = row_weighted_bases + 1      # Men\nrow_female_weight = row_weighted_bases + 2    # Women\n\n# Extract values (columns 1..5 correspond to quintiles)\nmale_prev_pct = t6.iloc[row_male_obese, 1:6].astype(float).values\nfemale_prev_pct = t6.iloc[row_female_obese, 1:6].astype(float).values\n\nmale_weight = t6.iloc[row_male_weight, 1:6].astype(float).values\nfemale_weight = t6.iloc[row_female_weight, 1:6].astype(float).values\n\n# Build tidy dataframe (convert % to proportions)\ndf = pd.DataFrame({\n    \"quintile_label\": quintile_labels * 2,\n    \"sex\": [\"Men\"] * 5 + [\"Women\"] * 5,\n    \"obesity_prev\": np.concatenate([male_prev_pct, female_prev_pct]) / 100.0,\n    \"weighted_base\": np.concatenate([male_weight, female_weight]),\n})\n\n# Reorder so that ridit = 0 is MOST deprived and ridit = 1 is LEAST deprived.\n# In this table the columns run Least -&gt; Most, so reverse within each sex.\ndef _reorder_most_to_least(group: pd.DataFrame) -&gt; pd.DataFrame:\n    return group.iloc[::-1].reset_index(drop=True)\n\ndf = df.groupby(\"sex\", group_keys=False).apply(_reorder_most_to_least)\n\n# Compute population share within sex (weights sum to 1 within each sex)\ndf[\"population_share\"] = df[\"weighted_base\"] / df.groupby(\"sex\")[\"weighted_base\"].transform(\"sum\")\n\n# Compute ridit using the cumulative population distribution (weighted)\ndf[\"cum_pop\"] = df.groupby(\"sex\")[\"population_share\"].cumsum()\ndf[\"cum_pop_lag\"] = df.groupby(\"sex\")[\"cum_pop\"].shift(1, fill_value=0)\ndf[\"ridit\"] = (df[\"cum_pop\"] + df[\"cum_pop_lag\"]) / 2\n\n# Calculate SII and RII for each sex and store in results dictionary\nresults = {}\nfor sex, group in df.groupby(\"sex\"):\n    # SII: linear regression of prevalence on ridit\n    sii_slope, sii_intercept, r_value, p_value, std_err = stats.linregress(\n        group[\"ridit\"], group[\"obesity_prev\"]\n    )\n    \n    # RII: regression on log(prevalence), then exponentiate to get ratio\n    # RII = predicted prevalence at ridit=1 / predicted prevalence at ridit=0\n    log_prev = np.log(group[\"obesity_prev\"])\n    rii_slope, rii_intercept, _, _, _ = stats.linregress(group[\"ridit\"], log_prev)\n    \n    # RII is the ratio of fitted values at ridit=1 vs ridit=0\n    # exp(intercept + slope*1) / exp(intercept + slope*0) = exp(slope)\n    rii = np.exp(rii_slope)\n    \n    results[sex] = {\n        \"data\": group.copy(),\n        \"sii\": sii_slope,\n        \"sii_intercept\": sii_intercept,\n        \"rii\": rii,\n        \"rii_slope\": rii_slope,\n        \"rii_intercept\": rii_intercept,\n        \"r_squared\": r_value ** 2,\n        \"p_value\": p_value,\n    }\n\ndisplay(df)\nprint(\"\\nSII and RII calculated for each sex (see next cell for visualisation)\")\n\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nx_line = np.linspace(0, 1, 200)\n\nfor sex, res in results.items():\n    g = res[\"data\"]\n\n    ax.scatter(g[\"ridit\"], g[\"obesity_prev\"], s=100, label=f\"{sex} (observed)\")\n    ax.plot(x_line, res[\"sii_intercept\"] + res[\"sii\"] * x_line,\n            linewidth=2, label=f\"{sex} fit (SII = {res['sii']:.3f})\")\n\n    for _, row in g.iterrows():\n        ax.annotate(row[\"quintile_label\"], (row[\"ridit\"], row[\"obesity_prev\"]),\n                    textcoords=\"offset points\", xytext=(0, 10), ha=\"center\")\n\nax.set_xlabel(\"Ridit score (0 = most deprived, 1 = least deprived)\")\nax.set_ylabel(\"Obesity prevalence\")\nax.set_title(\"Slope Index of Inequality (SII): adult obesity by deprivation (HSE 2022)\")\nax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f\"{y:.0%}\"))\nax.legend()\nplt.tight_layout()\nplt.show()\n\nfor sex, res in results.items():\n    print(f\"{sex}: SII = {res['sii']:.3f} (absolute change in prevalence from most to least deprived)\")"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#relative-index-of-inequality-rii",
    "href": "notebooks/2.03_health_inequalities.html#relative-index-of-inequality-rii",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "11 Relative Index of Inequality (RII)",
    "text": "11 Relative Index of Inequality (RII)\nRII is the relative counterpart of SII.\nInstead of an absolute difference, it expresses inequality as a ratio.\nA common formulation:\nlog(y) = α + βr\nThen:\nRII = exp(β)\nInterpretation: - RII = 1.25 → outcome is about 25% higher at the least deprived end than at the most deprived end (in the fitted gradient)\nRII is often preferred for: - prevalence - rates - outcomes naturally interpreted multiplicatively\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nfor sex, res in results.items():\n    g = res[\"data\"]\n\n    ax.scatter(g[\"ridit\"], g[\"obesity_prev\"], s=100, label=f\"{sex} (observed)\")\n    ax.plot(x_line, np.exp(res[\"rii_intercept\"] + res[\"rii_slope\"] * x_line),\n            linewidth=2, label=f\"{sex} fit (RII = {res['rii']:.2f})\")\n\n    for _, row in g.iterrows():\n        ax.annotate(row[\"quintile_label\"], (row[\"ridit\"], row[\"obesity_prev\"]),\n                    textcoords=\"offset points\", xytext=(0, 10), ha=\"center\")\n\nax.set_xlabel(\"Ridit score (0 = most deprived, 1 = least deprived)\")\nax.set_ylabel(\"Obesity prevalence\")\nax.set_title(\"Relative Index of Inequality (RII): adult obesity by deprivation (HSE 2022)\")\nax.yaxis.set_major_formatter(plt.FuncFormatter(lambda y, _: f\"{y:.0%}\"))\nax.legend()\nplt.tight_layout()\nplt.show()\n\nfor sex, res in results.items():\n    print(f\"{sex}: RII = {res['rii']:.2f} (ratio across the gradient, least vs most deprived in fitted model)\")"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#summary",
    "href": "notebooks/2.03_health_inequalities.html#summary",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "12 Summary",
    "text": "12 Summary\n\nWe replace deprivation categories by their position in the cumulative population distribution (ridit). We then regress the outcome on this rank. The slope of that regression gives the SII: the predicted absolute difference between the least and most deprived ends of society, using all groups rather than only the extremes."
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#global-inequalities-within-country-versus-between-country-gradients",
    "href": "notebooks/2.03_health_inequalities.html#global-inequalities-within-country-versus-between-country-gradients",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "13 6. Global inequalities: within-country versus between-country gradients",
    "text": "13 6. Global inequalities: within-country versus between-country gradients\nHealth inequalities exist at multiple levels:\n\nWithin-country inequalities (the Marmot gradient): differences across deprivation groups within one country.\nBetween-country inequalities (global differences): differences in average outcomes between countries.\nCross-country comparisons at the same income level: the same household income can correspond to different health outcomes in different countries.\n\nThe figure below illustrates a striking point: even at comparable (PPP-adjusted) household income, life expectancy differs substantially between England and the USA. This is not primarily an “income problem”; it reflects systems, environments, risks, and social policy.\n\n\n\nLife expectancy by net household income (England vs USA)\n\n\nSource: Financial Times analysis using US Small-area Life Expectancy Estimates Project, American Community Survey, and ONS small area income estimates (as stated in the figure).\n\n13.1 The Preston curve: income and health across countries\nAt the global level, health also follows a gradient — but it is non-linear.\nThe Preston curve plots life expectancy against national income (usually GDP per capita adjusted for purchasing power). It shows three key features:\n\nSteep gains at low income\nAmong poorer countries, small increases in income are associated with large improvements in life expectancy.\nDiminishing returns at higher income\nBeyond a certain point, additional income is associated with much smaller gains in life expectancy.\nMore than income alone\nCountries with similar incomes can have markedly different life expectancies, reflecting differences in:\n\nhealth systems\n\neducation\n\npublic health infrastructure\n\nsocial protection and inequality\n\n\nThe Preston curve is the global analogue of the Marmot gradient: it emphasises that health improves stepwise with socioeconomic advantage, but not in a simple linear fashion.\n\n# ============================================================\n# OWID Preston curve dataset + simple Preston regression\n# (Run this cell once; the next two cells only plot.)\n# ============================================================\n\ndf_owid = pd.read_csv(OWID_path)\n\nYEAR = 2019\ndf_year = df_owid[df_owid[\"Year\"] == YEAR].copy()\n\n# Keep only real countries (drop aggregates such as \"World\")\ndf_year = df_year[df_year[\"Code\"].notna()].copy()\n\n# Identify relevant columns robustly\nlife_col = [c for c in df_year.columns if \"Life expectancy\" in c][0]\ngdp_col  = [c for c in df_year.columns if \"GDP per capita\" in c][0]\n\ndf_year = df_year[[\"Entity\", \"Code\", \"Year\", life_col, gdp_col]].dropna().copy()\n\n# Countries to highlight (ISO3 -&gt; short label)\nhighlight_codes = {\n    \"GBR\": \"UK\",\n    \"USA\": \"US\",\n    \"KWT\": \"Kuwait\",\n    \"UGA\": \"Uganda\",\n    \"TUR\": \"Turkey\",\n    \"JPN\": \"Japan\",\n    \"HKG\": \"Hong Kong\",\n    \"CHN\": \"China\",\n    \"MYS\": \"Malaysia\",\n    \"IND\": \"India\",\n    \"PAK\": \"Pakistan\",\n    \"FRA\": \"France\",\n    \"SVK\": \"Slovakia\",\n}\n\n# Fit Preston regression: LE ~ log(GDP)\ndf_model = df_year.copy()\ndf_model[\"log_gdp\"] = np.log(df_model[gdp_col])\n\nexclude_codes = [\"PSE\", \"AFG\", \"SYR\", \"YEM\", \"SSD\"]  # Unreliable data\n\ndf_model = df_model[~df_model[\"Code\"].isin(exclude_codes)]\n\nslope, intercept, r_value, p_value, std_err = stats.linregress(\n    df_model[\"log_gdp\"],\n    df_model[life_col]\n)\n\ndf_model[\"le_pred\"]  = intercept + slope * df_model[\"log_gdp\"]\ndf_model[\"le_resid\"] = df_model[life_col] - df_model[\"le_pred\"]  # years\n\nprint(f\"Model: LE = {intercept:.2f} + {slope:.2f}·log(GDP)\")\nprint(f\"R² (LE ~ log(GDP)): {r_value**2:.2f}\")\n\n\n# ============================================================\n# Preston curve plot (observed points + fitted line)\n# ============================================================\n\nfig, ax = plt.subplots(figsize=(10, 6))\n\nax.scatter(df_model[gdp_col], df_model[life_col], alpha=0.6)\n\n# Fitted line drawn across GDP range (avoid log(0))\nx_gdp = np.linspace(df_model[gdp_col].min(), df_model[gdp_col].max(), 300)\ny_fit = intercept + slope * np.log(x_gdp)\nax.plot(x_gdp, y_fit, linewidth=2, color=\"black\", alpha=0.8, label=\"Fit: LE ~ log(GDP)\")\n\nax.set_xlabel(\"GDP per capita (PPP, international-$)\")\nax.set_ylabel(\"Life expectancy at birth (years)\")\nax.set_title(f\"Preston curve: life expectancy versus national income ({YEAR})\")\n\n# Licence credit\nfig.text(\n    0.01, 0.01,\n    \"OurWorldInData.org/life-expectancy | CC BY\",\n    ha=\"left\", va=\"bottom\", fontsize=9, color=\"grey\"\n)\n\n# Highlight selected countries\ndf_hl = df_model[df_model[\"Code\"].isin(highlight_codes)].copy()\nax.scatter(\n    df_hl[gdp_col],\n    df_hl[life_col],\n    s=80,\n    color=\"crimson\",\n    edgecolor=\"black\",\n    zorder=3,\n)\n\nfor _, row in df_hl.iterrows():\n    ax.annotate(\n        highlight_codes[row[\"Code\"]],\n        (row[gdp_col], row[life_col]),\n        xytext=(5, 5),\n        textcoords=\"offset points\",\n        fontsize=9,\n        weight=\"bold\",\n        color=\"crimson\",\n    )\n\nax.legend()\n\nplt.tight_layout()\nplt.show()\n\nprint(\n    \"Interpretation: life expectancy rises rapidly at low incomes and then shows diminishing returns.\\n\"\n    \"Countries can sit above or below the fitted curve, suggesting factors beyond income.\"\n)\n\n\n# ============================================================\n# Residual plot: \"same income, different health\"\n# (highlighted countries + global best/worst residuals)\n# ============================================================\n\nn_extremes = 3  # top/bottom countries by residual\n\ndf_extreme_low  = df_model.nsmallest(n_extremes, \"le_resid\")\ndf_extreme_high = df_model.nlargest(n_extremes, \"le_resid\")\n\ndf_plot = pd.concat([\n    df_model[df_model[\"Code\"].isin(highlight_codes)],\n    df_extreme_low,\n    df_extreme_high\n]).drop_duplicates(subset=\"Code\")\n\n# Labels: highlighted get short labels; extremes fall back to country name\ndf_plot[\"label\"] = df_plot[\"Code\"].map(highlight_codes)\ndf_plot[\"label\"] = df_plot[\"label\"].fillna(df_plot[\"Entity\"])\n\ndf_plot = df_plot.sort_values(\"le_resid\")\n\nfig, ax = plt.subplots(figsize=(9, 6))\n\nax.barh(df_plot[\"label\"], df_plot[\"le_resid\"], alpha=0.85)\nax.axvline(0, linewidth=1)\n\nax.set_xlabel(\"Residual life expectancy (years)\\nObserved minus predicted from Preston regression\")\nax.set_ylabel(\"\")\nax.set_title(f\"Same income, different health: deviations from Preston curve ({YEAR})\")\n\n\n\n13.2 Income inequality and health: moving beyond averages\nSo far, we have compared average health outcomes between countries.\nHowever, countries with similar average income can differ substantially in how that income is distributed.\nThe Gini coefficient is a summary measure of income inequality: - 0 indicates perfect equality - 1 indicates maximal inequality (one person has all income)\nA long-standing question in public health is whether more unequal societies have worse health outcomes, even after accounting for average income.\nIn the next example, we examine the relationship between: - national income inequality (Gini coefficient), and - life expectancy at birth\nThis is a cross-country, ecological analysis. It is useful for description and hypothesis generation, but it cannot establish causality at the individual level.\n\nYEAR = 2019\ndf_gini = pd.read_csv(gini_path)\ndf_owid = pd.read_csv(OWID_path)\n\ndf_year = df_owid[df_owid[\"Year\"] == YEAR].copy()\ndf_year = df_year[df_year[\"Code\"].notna()].copy()\n\nlife_col = [c for c in df_year.columns if \"Life expectancy\" in c][0]\n\ndf_health = df_year[[\"Code\", \"Entity\", life_col]].rename(\n    columns={\"Code\": \"iso3\", \"Entity\": \"country\", life_col: \"life_expectancy\"}\n).dropna()\n\n# 3) Merge and plot\ndf_merge = df_gini.merge(df_health, on=\"iso3\", how=\"inner\")\n\nfig, ax = plt.subplots(figsize=(10, 6))\nax.scatter(df_merge[\"gini\"], df_merge[\"life_expectancy\"], alpha=0.7)\n\n# Optional: simple linear fit (purely descriptive)\nm, b = np.polyfit(df_merge[\"gini\"], df_merge[\"life_expectancy\"], 1)\nx_line = np.linspace(df_merge[\"gini\"].min(), df_merge[\"gini\"].max(), 200)\nax.plot(x_line, m * x_line + b, linewidth=2)\n\nax.set_xlabel(\"Gini coefficient (income inequality; higher = more unequal)\")\nax.set_ylabel(\"Life expectancy at birth (years)\")\nax.set_title(f\"Income inequality (Gini) and life expectancy ({YEAR})\")\n\n\n# ---- Add labels with small offsets (Gini plot) ----\nfor _, row in df_merge.iterrows():\n    iso3 = row[\"iso3\"]\n    if iso3 not in highlight_codes:\n        continue\n\n    ax.annotate(\n        highlight_codes[iso3],\n        (row[\"gini\"], row[\"life_expectancy\"]),\n        xytext=(5, 5),\n        textcoords=\"offset points\",\n        fontsize=9,\n        weight=\"bold\",\n        color=\"crimson\",\n    )\n\n\nplt.tight_layout()\nplt.show()\n\nprint(\n    \"Important: this is an ecological, cross-country comparison.\\n\"\n    \"It is useful for description and hypothesis generation, not causal inference.\"\n)\nprint(f\"Countries included: {len(df_merge)}\")"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#discussion-questions",
    "href": "notebooks/2.03_health_inequalities.html#discussion-questions",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "14 7. Discussion Questions",
    "text": "14 7. Discussion Questions\n\nIMD limitations: Why might the IMD be a poor measure of individual deprivation?\nLE vs HLE: Why is the HLE gap larger than the LE gap? What does this imply for pension policy?\nProportionate universalism: How might you measure whether a dietary intervention follows this principle?"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#exercises",
    "href": "notebooks/2.03_health_inequalities.html#exercises",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "15 8. Exercises",
    "text": "15 8. Exercises\n\n15.1 Exercise 1: Explore IMD in Your Area\n\nGo to the ONS IMD Explorer\nFind your home area (or Reading)\nNote the IMD decile and domain scores\n\n\n# Record your findings\nmy_area = {\n    'Area name': '',\n    'IMD Decile': None,\n    'Most deprived domain': ''\n}"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#summary-1",
    "href": "notebooks/2.03_health_inequalities.html#summary-1",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "16 Summary",
    "text": "16 Summary\n\nIMD combines seven domains to rank areas by deprivation\nHealthy life expectancy reveals larger inequality gaps than total LE\nThe “double burden” means deprived populations live shorter lives AND more years in poor health\nSII measures absolute inequality; RII expresses it relatively"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#key-resources",
    "href": "notebooks/2.03_health_inequalities.html#key-resources",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "17 Key Resources",
    "text": "17 Key Resources\n\nONS IMD Interactive Map\nPHE Fingertips\nONS Health State Life Expectancy"
  },
  {
    "objectID": "notebooks/2.03_health_inequalities.html#references",
    "href": "notebooks/2.03_health_inequalities.html#references",
    "title": "2.03 – Health Inequalities: Measuring the Gradient",
    "section": "18 References",
    "text": "18 References\n\nMHCLG (2019). English Indices of Deprivation.\nMarmot M (2010). Fair Society, Healthy Lives.\nMarmot M (2020). Health Equity in England: The Marmot Review 10 Years On."
  },
  {
    "objectID": "notebooks/1.10_missing_data_and_sensitivity.html",
    "href": "notebooks/1.10_missing_data_and_sensitivity.html",
    "title": "1.10 – Missing Data and Sensitivity Analysis",
    "section": "",
    "text": "Version 0.0.1\nIn all real epidemiological datasets, some information is missing. In this workbook we focus more systematically on:\nWe work with a simple regression example using the synthetic FB2NEP cohort.\nRun the first two code cells to set up the repository and load the data.\n# ============================================================\n# FB2NEP bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the fb2nep-epi repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads and runs scripts/bootstrap.py.\n# - Makes the main dataset available as the variable `df`.\n#\n# Important:\n# - You may see messages printed below (for example from pip\n#   or from the bootstrap script). This is expected.\n# - You may also see WARNINGS (often in yellow). In most cases\n#   these are harmless and can be ignored for this module.\n# - The main thing to watch for is a red error traceback\n#   (for example FileNotFoundError, ModuleNotFoundError).\n#   If that happens, please re-run this cell first. If the\n#   error persists, ask for help.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\nimport importlib.util\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\n# REPO_URL: address of the GitHub repository.\n# REPO_DIR: folder name that will be created when cloning.\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the fb2nep-epi repository\n# ------------------------------------------------------------\n# In local Jupyter, you may already be inside the repository,\n# for example in fb2nep-epi/notebooks.\n#\n# In Colab, the default working directory is /content, so\n# we need to clone the repository into /content/fb2nep-epi\n# and then change into that folder.\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/bootstrap.py exists here)\nif (cwd / \"scripts\" / \"bootstrap.py\").is_file():\n    repo_root = cwd\n\n# Case B: we are outside the repository (for example in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if it is not present yet\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change the working directory to the repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\nprint(f\"Repository root set to: {repo_root}\")\n\n# ------------------------------------------------------------\n# 2. Load scripts/bootstrap.py as a module and call init()\n# ------------------------------------------------------------\n# The shared bootstrap script contains all logic to:\n# - Ensure that required Python packages are installed.\n# - Ensure that the synthetic dataset exists (and generate it\n#   if needed).\n# - Load the dataset into a pandas DataFrame.\n#\n# We load the script as a normal Python module (fb2nep_bootstrap)\n# and then call its init() function.\nbootstrap_path = repo_root / \"scripts\" / \"bootstrap.py\"\n\nif not bootstrap_path.is_file():\n    raise FileNotFoundError(\n        f\"Could not find {bootstrap_path}. \"\n        \"Please check that the fb2nep-epi repository structure is intact.\"\n    )\n\n# Create a module specification from the file\nspec = importlib.util.spec_from_file_location(\"fb2nep_bootstrap\", bootstrap_path)\nbootstrap = importlib.util.module_from_spec(spec)\nsys.modules[\"fb2nep_bootstrap\"] = bootstrap\n\n# Execute the bootstrap script in the context of this module\nspec.loader.exec_module(bootstrap)\n\n# The init() function is defined in scripts/bootstrap.py.\n# It returns:\n# - df   : the main synthetic cohort as a pandas DataFrame.\n# - CTX  : a small context object with paths, flags and settings.\ndf, CTX = bootstrap.init()\n\n# Optionally expose a few additional useful variables from the\n# bootstrap module (if they exist). These are not essential for\n# most analyses, but can be helpful for advanced use.\nfor name in [\"CSV_REL\", \"REPO_NAME\", \"REPO_URL\", \"IN_COLAB\"]:\n    if hasattr(bootstrap, name):\n        globals()[name] = getattr(bootstrap, name)\n\nprint(\"Bootstrap completed successfully.\")\nprint(\"The main dataset is available as the variable `df`.\")\nprint(\"The context object is available as `CTX`.\")\n# Imports used throughout this workbook\n#\n# - numpy, pandas: general data handling\n# - matplotlib: simple visualisations\n# - statsmodels: regression and multiple imputation (MICE)\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom statsmodels.imputation.mice import MICEData, MICE\n\nfrom scripts.helpers_tables import ensure_cmdstan\n\ncmdstan_root = ensure_cmdstan()\n\nfrom cmdstanpy import CmdStanModel\nprint(\"Using CmdStan from:\", cmdstan_root)\n\n%matplotlib inline\n# Quick inspection of the main dataset\n# This is just to remind ourselves of the structure of the FB2NEP cohort.\n\ndf.head()"
  },
  {
    "objectID": "notebooks/1.10_missing_data_and_sensitivity.html#background-what-do-we-mean-by-missing-data",
    "href": "notebooks/1.10_missing_data_and_sensitivity.html#background-what-do-we-mean-by-missing-data",
    "title": "1.10 – Missing Data and Sensitivity Analysis",
    "section": "1 Background: what do we mean by “missing data”?",
    "text": "1 Background: what do we mean by “missing data”?\nBefore inspecting missingness in the FB2NEP dataset, we first clarify what missing means in an epidemiological context.\n\n1.1 (a) What can be missing?\nInformation can be missing at several levels:\n\nWhole participants\nThese individuals never appear in the analysis dataset (not recruited, withdrew immediately, or provided no usable data).\nEntire visits or time points\nA participant attends baseline but not follow-up; in longitudinal data this appears as absent rows or missing whole sets of variables.\nIndividual variables (items)\nA single measurement is absent:\n\nSBP not taken or not recorded.\n\nHeight or weight missing.\n\nA questionnaire item skipped.\n\n\nIn this workbook we mainly deal with item-level missingness in outcome, exposure, and covariates within a regression model.\n\n\n1.2 (b) Different types of missing entries\nNot all missing values have the same meaning:\n\nItem non-response\nA measurement should exist, but is not observed.\nUnit non-response\nA whole clinic visit or questionnaire is missing.\nStructurally missing (“not applicable”)\nThese entries are supposed to be missing. Examples:\n\nmenopausal_status in men.\n\nCVD_date for participants without a CVD event.\n\n\nStructural missingness is not imputed because it is determined by the logic of the variable.\n\n\n1.3 (c) What does “missing” mean in the dataset?\nIn the FB2NEP dataset, missing values appear as NaN in pandas.\nThis means:\n\nThe participant exists in the cohort.\n\nBut the dataset contains no recorded value for that variable.\n\nIt does not mean:\n\nzero,\n\n“no disease”,\n\n“never”,\n\nor any other substantive category.\n\nExample:\n\nSBP = 0 mmHg would be physiologically impossible and signals an error.\n\nSBP = NaN means the measurement was not collected or not available.\n\nMissingness therefore concerns the measurement, not the person. A missing BMI does not mean the participant lacks body mass; it means the dataset lacks the recorded value.\nIn the rest of this workbook, we focus on:\n\nHow much item-level missingness there is in our chosen variables.\n\nHow different ways of handling these missing values (complete-case analysis, single imputation, multiple imputation) can influence our regression results."
  },
  {
    "objectID": "notebooks/1.10_missing_data_and_sensitivity.html#missingness-overview",
    "href": "notebooks/1.10_missing_data_and_sensitivity.html#missingness-overview",
    "title": "1.10 – Missing Data and Sensitivity Analysis",
    "section": "2 1. Missingness overview",
    "text": "2 1. Missingness overview\nWe now turn to the concrete pattern of missing data in the FB2NEP cohort.\nIn this workbook we focus on a simple blood pressure model:\n\nOutcome: SBP (systolic blood pressure).\nMain exposure: BMI (body mass index).\nCovariates: age, sex, smoking_status, SES_class.\n\nWe begin by calculating the proportion of missing values in each of these variables.\n\n# Select variables of interest for this workbook.\n#\n# We keep the code robust by checking that each variable actually exists in df.\n\nvars_of_interest = [v for v in [\"SBP\", \"BMI\", \"age\", \"sex\", \"smoking_status\", \"SES_class\"] if v in df.columns]\ndf_an = df[vars_of_interest].copy()\n\n# Ensure that categorical variables are coded as categories.\nif \"sex\" in df_an.columns:\n    df_an[\"sex\"] = df_an[\"sex\"].astype(\"category\")\nif \"smoking_status\" in df_an.columns:\n    df_an[\"smoking_status\"] = df_an[\"smoking_status\"].astype(\"category\")\nif \"SES_class\" in df_an.columns:\n    df_an[\"SES_class\"] = df_an[\"SES_class\"].astype(\"category\")\n\n# Proportion of missing values for each variable.\nmissing_props = df_an.isna().mean()\nmissing_props\n\nThe table above shows the fraction of missing values for each variable.\n\nA value of 0.05 means that 5% of observations for that variable are missing.\nThe amount of missing data can differ markedly between variables.\n\nTo visualise this pattern we can draw a simple bar chart.\n\n# Bar plot of missingness for the selected variables.\n\nplt.figure(figsize=(6, 4))\nmissing_props.sort_values(ascending=False).plot(kind=\"bar\")\nplt.ylabel(\"Proportion missing\")\nplt.ylim(0, 1)\nplt.title(\"Proportion of missing values by variable\")\nplt.tight_layout()\nplt.show()\n\n\n2.1 1.1 Conceptual mechanisms of missingness\nThe statistical properties of any missing-data method depend on how data are missing. Three standard mechanisms are:\n\nMCAR – Missing Completely At Random\nThe probability that a value is missing does not depend on any observed or unobserved variable. For example, a blood sample is lost due to a laboratory freezer failure that affects samples randomly.\nMAR – Missing At Random\nThe probability that a value is missing may depend on observed variables, but not on the value of the missing variable itself, after conditioning on the observed data. For example, BMI may be more likely to be missing among older participants, but conditional on age and sex, missingness is unrelated to the true BMI value.\nMNAR – Missing Not At Random\nThe probability that a value is missing still depends on the unobserved value, even after conditioning on observed covariates. For example, participants with very high BMI may be particularly reluctant to be weighed, even after adjusting for age, sex and smoking.\n\nIn practice we rarely know the true mechanism. A key message is: therefore:\n\nWe can rarely “fix” missing data, but we can make our assumptions explicit and explore sensitivity of results to these assumptions."
  },
  {
    "objectID": "notebooks/1.10_missing_data_and_sensitivity.html#complete-case-versus-single-imputation",
    "href": "notebooks/1.10_missing_data_and_sensitivity.html#complete-case-versus-single-imputation",
    "title": "1.10 – Missing Data and Sensitivity Analysis",
    "section": "3 2. Complete-case versus single imputation",
    "text": "3 2. Complete-case versus single imputation\nWe now fit a simple linear regression model with systolic blood pressure (SBP) as the outcome and BMI as the main exposure, adjusted for age and available covariates.\nWe compare two strategies:\n\nComplete-case analysis: use only participants with no missing values in any of the model variables. This is easy but can lead to loss of power and biased estimates, unless data are MCAR (or satisfy a slightly weaker condition).\nSingle imputation: fill in missing values with a single “best guess” (mean or mode) and analyse the resulting dataset as if it were complete. This preserves sample size but underestimates uncertainty.\n\nWe start with the complete-case analysis.\n\n# 2.1 Complete-case analysis\n# --------------------------\n# We drop any row that has at least one missing value in the variables\n# we intend to use in the model.\n\ndf_cc = df_an.dropna()\nprint(f\"Number of complete cases: {len(df_cc)} out of {len(df_an)} participants\")\n\n# Construct the regression formula step by step.\nformula = \"SBP ~ BMI + age\"\nif \"sex\" in df_cc.columns:\n    formula += \" + C(sex)\"\nif \"smoking_status\" in df_cc.columns:\n    formula += \" + C(smoking_status)\"\nif \"SES_class\" in df_cc.columns:\n    formula += \" + C(SES_class)\"\n\nprint(\"Model formula:\", formula)\n\n# Fit the ordinary least squares (OLS) model using statsmodels.\nmodel_cc = smf.ols(formula, data=df_cc).fit()\n\n# Create a compact summary table with estimates and 95% confidence intervals.\ncc_summary = model_cc.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"[0.025\", \"0.975]\"]]\ncc_summary\n\n\n# 2.2 Single imputation (mean/mode)\n# ---------------------------------\n# For illustration, we perform a very simple single imputation:\n# - For numeric variables, replace missing values with the mean.\n# - For categorical variables, replace missing values with the most frequent category.\n#\n# This method is *not* recommended for serious analyses, but it is useful to\n# demonstrate how different handling of missing data can influence estimates.\n\ndf_si = df_an.copy()\n\nfor col in df_si.columns:\n    if df_si[col].dtype.kind in \"biufc\":  # numeric types\n        df_si[col] = df_si[col].fillna(df_si[col].mean())\n    else:  # categorical or object types\n        df_si[col] = df_si[col].fillna(df_si[col].mode().iloc[0])\n\n# Fit the same model to the single-imputed dataset.\nmodel_si = smf.ols(formula, data=df_si).fit()\n\nsi_summary = model_si.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"[0.025\", \"0.975]\"]]\n\n# Compare coefficients from complete-case and single-imputed analyses.\ncomparison_2 = pd.DataFrame({\n    \"cc_coef\": cc_summary[\"Coef.\"],\n    \"si_coef\": si_summary[\"Coef.\"],\n    \"cc_SE\": cc_summary[\"Std.Err.\"],\n    \"si_SE\": si_summary[\"Std.Err.\"]\n})\ncomparison_2\n\nIn the table above, focus on the BMI coefficient and its standard error in the two approaches.\n\nDo the point estimates differ?\n\nAre the confidence intervals similar or noticeably different?\n\nHow many observations were used in the complete-case analysis compared with the imputed analysis?\n\nSingle imputation keeps the original sample size but fails to reflect the extra uncertainty caused by missing data, so standard errors are often too small."
  },
  {
    "objectID": "notebooks/1.10_missing_data_and_sensitivity.html#multiple-imputation-with-mice-simplified",
    "href": "notebooks/1.10_missing_data_and_sensitivity.html#multiple-imputation-with-mice-simplified",
    "title": "1.10 – Missing Data and Sensitivity Analysis",
    "section": "4 3. Multiple imputation with MICE (simplified)",
    "text": "4 3. Multiple imputation with MICE (simplified)\nMultiple imputation aims to improve on single imputation by:\n\nDrawing several plausible values for each missing observation, generating multiple imputed datasets.\nFitting the analysis model in each dataset.\nCombining estimates and standard errors using Rubin’s rules.\n\nConceptually, this is closer to the idea of uncertainty used elsewhere in statistics: we acknowledge that the missing values could have been different, and we propagate this uncertainty into the final estimates.\nHere we use a simple implementation of MICE (Multivariate Imputation by Chained Equations) from statsmodels. The details of the imputation models are beyond the scope of FB2NEP; we treat this as a black box and focus on the idea and the comparison of results.\n\n# 3.1 Prepare data for MICE\n# -------------------------\n# MICE in statsmodels expects a numeric design matrix. We therefore use\n# one-hot encoding (dummy variables) for categories.\n\ndf_mice = pd.get_dummies(df_an, drop_first=True)\n\n# Create a MICEData object that stores the data and handles the chained equations.\nmice_data = MICEData(df_mice)\n\n# Outcome and predictors: here we use all available predictors for imputation\n# and analysis (this is not always ideal, but sufficient for illustration).\nendog = \"SBP\"\npredictors = [c for c in df_mice.columns if c != endog]\nformula_mice = endog + \" ~ \" + \" + \".join(predictors)\nprint(\"MICE model formula:\")\nprint(formula_mice)\n\n# 3.2 Fit the MICE model with m=5 imputations.\n# --------------------------------------------\n# The MICE object performs imputations internally and then fits the regression\n# model repeatedly, combining estimates automatically.\n\n# Note: first argument = formula, second = *class* with .from_formula (sm.OLS)\nmice = MICE(formula_mice, sm.OLS, mice_data)\nresult_mice = mice.fit(5)\n\n# The summary is long, but it shows pooled estimates and standard errors.\nresult_mice.summary()\n\n\n# 3.3 Extract and compare key coefficients across methods\n# -------------------------------------------------------\n# We focus on the BMI effect. For the MICE model, BMI is numeric and should\n# appear among the exogenous (predictor) names.\n\n# 1. Get BMI coefficient and SE from the complete-case and single-impute models\nbmi_cc_coef = model_cc.params.get(\"BMI\", np.nan)\nbmi_cc_se   = model_cc.bse.get(\"BMI\", np.nan)\n\nbmi_si_coef = model_si.params.get(\"BMI\", np.nan)\nbmi_si_se   = model_si.bse.get(\"BMI\", np.nan)\n\nprint(\"Raw BMI coefficients from the two frequentist models:\")\nprint(f\"  Complete-case BMI coef: {bmi_cc_coef:.6f}\")\nprint(f\"  Single-impute BMI coef: {bmi_si_coef:.6f}\")\n\n# 2. Wrap the pooled MICE parameters and standard errors in a DataFrame\npooled = pd.DataFrame(\n    {\n        \"coef\": result_mice.params,\n        \"se\": result_mice.bse,\n    },\n    index=result_mice.model.exog_names,\n)\n\nif \"BMI\" in pooled.index:\n    bmi_mi_coef = pooled.loc[\"BMI\", \"coef\"]\n    bmi_mi_se   = pooled.loc[\"BMI\", \"se\"]\nelse:\n    bmi_mi_coef = np.nan\n    bmi_mi_se   = np.nan\n\nprint(f\"  Multiple-impute BMI coef: {bmi_mi_coef:.6f}\")\n\n# 3. Assemble comparison table\nsummary_methods = pd.DataFrame({\n    \"method\":   [\"complete_case\", \"single_impute\", \"multiple_impute\"],\n    \"BMI_coef\": [bmi_cc_coef,     bmi_si_coef,     bmi_mi_coef],\n    \"BMI_SE\":   [bmi_cc_se,       bmi_si_se,       bmi_mi_se],\n    \"n_used\":   [len(df_cc),      len(df_si),      len(df_an)],\n})\n\nsummary_methods\n\nThis table summarises the estimated BMI effect and its standard error under three strategies.\n\nMultiple imputation typically has smaller standard errors than complete-case analysis (because it uses more data) but larger standard errors than naive single imputation (because it acknowledges imputation uncertainty).\nIn well-behaved situations, point estimates are often similar across methods, but they can differ, especially if missingness is related to key variables.\n\nEven a moderately sceptical hippo would insist that the assumptions behind each method are made clear in any report or dissertation."
  },
  {
    "objectID": "notebooks/1.10_missing_data_and_sensitivity.html#simple-sensitivity-analyses",
    "href": "notebooks/1.10_missing_data_and_sensitivity.html#simple-sensitivity-analyses",
    "title": "1.10 – Missing Data and Sensitivity Analysis",
    "section": "5 4. Simple sensitivity analyses",
    "text": "5 4. Simple sensitivity analyses\nNo missing-data method is perfect, and the mechanism of missingness is usually not known with certainty. Sensitivity analyses explore how robust our conclusions are to alternative assumptions or modelling choices.\nHere we illustrate two simple strategies:\n\nRestricting the analysis to a more “ordinary” BMI range.\nApplying a small delta adjustment to imputed values to mimic an MNAR scenario.\n\n\n5.1 4.1 Restricting the BMI range\nExtreme values can sometimes drive results and may also be more prone to measurement error or missingness. As a basic sensitivity analysis, we refit the complete-case model excluding participants with BMI ≥ 40 kg/m² and compare results.\n\n# 4.1 Restrict analysis to BMI &lt; 40 kg/m^2\n\nif \"BMI\" in df_cc.columns:\n    df_cc_restricted = df_cc[df_cc[\"BMI\"] &lt; 40]\n    print(f\"Complete cases in original model:   {len(df_cc)}\")\n    print(f\"Complete cases with BMI &lt; 40:      {len(df_cc_restricted)}\")\n\n    model_cc_rest = smf.ols(formula, data=df_cc_restricted).fit()\n\n    cc_rest_summary = model_cc_rest.summary2().tables[1][[\"Coef.\", \"Std.Err.\", \"[0.025\", \"0.975]\"]]\n\n    comp_rest = pd.DataFrame({\n        \"original_coef\": cc_summary[\"Coef.\"],\n        \"restricted_coef\": cc_rest_summary[\"Coef.\"],\n        \"original_SE\": cc_summary[\"Std.Err.\"],\n        \"restricted_SE\": cc_rest_summary[\"Std.Err.\"],\n    })\n    comp_rest\n\n\n\n5.2 4.3 Sensitivity to adjustment set (model specification)\nSensitivity analyses are not limited to missing data. In epidemiology, it is good practice to ask how sensitive our main association is to modelling choices, in particular:\n\nWhich covariates we adjust for (potential confounders).\nHow we code exposures and covariates (continuous vs categories).\nFunctional form (for example, linear vs quadratic).\n\nHere we consider the association between BMI and SBP and compare two regression models:\n\nA minimally adjusted model:\n\\[\n\\text{SBP} = \\beta_0 + \\beta_1 \\cdot \\text{BMI} + \\beta_2 \\cdot \\text{age} + \\beta_3 \\cdot \\text{sex} + \\varepsilon.\n\\]\nA more fully adjusted model that also includes potential confounders:\n\\[\n\\text{SBP} = \\beta_0 + \\beta_1 \\cdot \\text{BMI} + \\beta_2 \\cdot \\text{age}\n+ \\beta_3 \\cdot \\text{sex} + \\beta_4 \\cdot \\text{smoking status}\n+ \\beta_5 \\cdot \\text{SES} + \\beta_6 \\cdot \\text{physical activity} + \\varepsilon.\n\\]\n\nThe aim is to see whether the estimated BMI effect (\\(\\beta_1\\)) is stable or changes substantially when we change the adjustment set. Large changes might indicate strong confounding or model misspecification.\n\n# 4.3 Sensitivity to adjustment set (model specification)\n# ------------------------------------------------------\n# We use the complete-case dataset df_cc and compare:\n# - A minimally adjusted model: SBP ~ BMI + age + sex\n# - A more fully adjusted model: add smoking_status, SES_class, physical_activity (if present)\n\n# Ensure we are working with complete cases for all relevant variables.\ncovars_min = [\"age\", \"sex\"]\ncovars_full = [\"age\", \"sex\", \"smoking_status\", \"SES_class\", \"physical_activity\"]\n\n# Keep only variables that actually exist in the data.\ncovars_min = [v for v in covars_min if v in df_cc.columns]\ncovars_full = [v for v in covars_full if v in df_cc.columns]\n\nvars_min = [\"SBP\", \"BMI\"] + covars_min\nvars_full = [\"SBP\", \"BMI\"] + covars_full\n\ndf_cc_min = df_cc[vars_min].dropna()\ndf_cc_full = df_cc[vars_full].dropna()\n\nprint(f\"Minimally adjusted model:   {len(df_cc_min)} complete cases\")\nprint(f\"Fully adjusted model:       {len(df_cc_full)} complete cases\")\n\n# Construct formulas\nformula_min = \"SBP ~ BMI\"\nfor cov in covars_min:\n    if str(df_cc_min[cov].dtype) == \"category\":\n        formula_min += f\" + C({cov})\"\n    else:\n        formula_min += f\" + {cov}\"\n\nformula_full = \"SBP ~ BMI\"\nfor cov in covars_full:\n    if str(df_cc_full[cov].dtype) == \"category\":\n        formula_full += f\" + C({cov})\"\n    else:\n        formula_full += f\" + {cov}\"\n\nprint(\"Minimal model formula:   \", formula_min)\nprint(\"Full model formula:      \", formula_full)\n\n# Fit both models\nmodel_min = smf.ols(formula_min, data=df_cc_min).fit()\nmodel_full = smf.ols(formula_full, data=df_cc_full).fit()\n\n# Extract BMI coefficients and standard errors\nbmi_min_coef = model_min.params.get(\"BMI\", np.nan)\nbmi_min_se   = model_min.bse.get(\"BMI\", np.nan)\n\nbmi_full_coef = model_full.params.get(\"BMI\", np.nan)\nbmi_full_se   = model_full.bse.get(\"BMI\", np.nan)\n\nsens_adjust = pd.DataFrame({\n    \"model\": [\"minimal\", \"full\"],\n    \"BMI_coef\": [bmi_min_coef, bmi_full_coef],\n    \"BMI_SE\": [bmi_min_se, bmi_full_se],\n    \"n_used\": [len(df_cc_min), len(df_cc_full)],\n})\n\nsens_adjust"
  },
  {
    "objectID": "notebooks/1.10_missing_data_and_sensitivity.html#bayesian-approaches",
    "href": "notebooks/1.10_missing_data_and_sensitivity.html#bayesian-approaches",
    "title": "1.10 – Missing Data and Sensitivity Analysis",
    "section": "6 5. Bayesian approaches",
    "text": "6 5. Bayesian approaches\nBayesian methods treat missing values as unknown parameters and estimate them jointly with all other parameters in the model.\nIn a Bayesian missing-data model:\n\nWe specify a likelihood for the observed data given parameters and missing values.\nWe specify prior distributions for both the model parameters and the missing values.\nWe use Markov Chain Monte Carlo (MCMC) or related algorithms to sample from the joint posterior distribution.\n\nThe resulting posterior samples naturally incorporate uncertainty about missing values into uncertainty about regression coefficients. This is conceptually similar to multiple imputation, but the Bayesian approach integrates all uncertainty in a single framework and can make it easier to specify complex MNAR models.\nImplementing Bayesian missing-data models is beyond the scope of FB2NEP, but it is useful to be aware of them, particularly for more advanced research projects.\n\n# 5. Bayesian regression with Stan (CmdStanPy)\n# --------------------------------------------\n# Model: SBP ~ BMI + age + sex\n#\n# We use weakly informative priors and sample from the posterior using Stan,\n# accessed via CmdStanPy. This is an illustrative example that mirrors the\n# frequentist linear regression used earlier.\n\n\n# If you have put these helpers into a module, import them here, e.g.:\n# from helpers_bayes import ensure_cmdstan, stan_summary_table\n# For a standalone notebook, you can paste the helper definitions above this cell.\n\ncmdstan_root = ensure_cmdstan()\n\nfrom cmdstanpy import CmdStanModel\nprint(\"Using CmdStan from:\", cmdstan_root)\n\n# Prepare a clean analysis dataset (complete cases for the relevant variables)\n\ndf_bayes = df_an.dropna(subset=[\"SBP\", \"BMI\", \"age\", \"sex\"]).copy()\ndf_bayes[\"sex_M\"] = (df_bayes[\"sex\"] == \"M\").astype(float)  # float now\n\ndata_stan = {\n    \"N\": len(df_bayes),\n    \"SBP\": df_bayes[\"SBP\"].to_numpy(),\n    \"BMI\": df_bayes[\"BMI\"].to_numpy(),\n    \"age\": df_bayes[\"age\"].to_numpy(),\n    \"sex_M\": df_bayes[\"sex_M\"].to_numpy(),\n}\n\n\nprint(f\"N for Bayesian model: {data_stan['N']}\")\n\n\nstan_code = \"\"\"\ndata {\n  int&lt;lower=1&gt; N;\n  vector[N] SBP;\n  vector[N] BMI;\n  vector[N] age;\n  vector[N] sex_M;        // now a vector, not a scalar\n}\n\nparameters {\n  real intercept;\n  real beta_BMI;\n  real beta_age;\n  real beta_sex;\n  real&lt;lower=0&gt; sigma;\n}\n\nmodel {\n  // Weakly informative priors\n  intercept ~ normal(120, 20);\n  beta_BMI  ~ normal(0.5, 0.5);\n  beta_age  ~ normal(0.6, 0.3);\n  beta_sex  ~ normal(0, 2);\n  sigma     ~ normal(15, 5);\n\n  // Linear predictor\n  SBP ~ normal(\n    intercept\n    + beta_BMI * BMI\n    + beta_age * age\n    + beta_sex * sex_M,\n    sigma\n  );\n}\n\"\"\"\nwith open(\"sbp_regression.stan\", \"w\") as f:\n    f.write(stan_code)\n\nsbp_model = CmdStanModel(stan_file=\"sbp_regression.stan\", force_compile=True)\n\n\nfit = sbp_model.sample(\n    data=data_stan,\n    seed=11088,\n    chains=4,\n    parallel_chains=4,\n    iter_warmup=1000,\n    iter_sampling=2000,\n    show_console=True,   # keep on while debugging\n)\n\n\nfrom scripts.helpers_tables import stan_summary_table\n\nparam_order = [\"intercept\", \"beta_BMI\", \"beta_age\", \"beta_sex\", \"sigma\"]\nbayes_tbl = stan_summary_table(fit, param_order=param_order)\nbayes_tbl\n\n\n# 6. Comparison of Bayesian (Stan) and Frequentist Estimates\n# ----------------------------------------------------------\n\n\n\n# Extract BMI, age, sex coefficients from frequentist models\nbmi_cc  = model_cc.params.get(\"BMI\", np.nan)\nage_cc  = model_cc.params.get(\"age\", np.nan)\nsex_cc  = model_cc.params.get(\"C(sex)[T.M]\", np.nan)\n\n# From MI results (already computed earlier)\nbmi_mi  = bmi_mi_coef\nage_mi  = np.nan   # replace if you computed MI for age\nsex_mi  = np.nan   # replace if you computed MI for sex\n\n# From Stan posterior means\nbmi_stan = bayes_tbl.loc[\"beta_BMI\", \"mean\"]\nage_stan = bayes_tbl.loc[\"beta_age\", \"mean\"]\nsex_stan = bayes_tbl.loc[\"beta_sex\", \"mean\"]\n\ncomparison = pd.DataFrame({\n    \"method\": [\"OLS (complete-case)\", \"Multiple Imputation\", \"Bayesian (Stan)\"],\n    \"BMI_coef\": [bmi_cc, bmi_mi, bmi_stan],\n})\n\ncomparison"
  },
  {
    "objectID": "notebooks/1.10_missing_data_and_sensitivity.html#practical-mini-assignment",
    "href": "notebooks/1.10_missing_data_and_sensitivity.html#practical-mini-assignment",
    "title": "1.10 – Missing Data and Sensitivity Analysis",
    "section": "7 6. Practical mini-assignment",
    "text": "7 6. Practical mini-assignment\nIn this final section you will carry out a small, assignment-style analysis using the FB2NEP cohort.\n\n7.1 Task\n\nChoose an outcome (for example, SBP or a biomarker).\nChoose a main exposure (for example, BMI, salt_g_d, or physical_activity).\nChoose at least two covariates (for example, age, sex, SES_class).\nInspect and describe the pattern of missingness in your chosen variables.\nFit three models:\n\nComplete-case analysis.\nSingle imputation (mean/mode or another reasonable rule).\nMultiple imputation using MICE.\n\nPerform one sensitivity analysis, for example:\n\nRestrict the analysis to a more typical range of the exposure.\nApply a small delta-based MNAR adjustment similar to Section 4.2.\n\nWrite a short interpretation (approximately 150–200 words) answering:\n\nHow do the point estimates differ between methods?\n\nHow do the standard errors differ?\n\nHow sensitive are your conclusions to the chosen assumptions?"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html",
    "href": "notebooks/2.06_policy_simulation.html",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "",
    "text": "Learning Objectives: - Apply cost-effectiveness analysis to real-world resource allocation decisions - Understand diminishing returns and the cost-effectiveness frontier - Explore the tension between efficiency and equity in health policy - Critically evaluate the limitations of purely economic approaches to priority-setting"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#the-challenge",
    "href": "notebooks/2.06_policy_simulation.html#the-challenge",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "1 The Challenge",
    "text": "1 The Challenge\nYou are advising a regional health authority with a £50 million budget for preventive health programmes over the next 5 years. Your task is to allocate this budget across available interventions to maximise population health — measured in DALYs averted.\nBut maximising DALYs isn’t the only consideration: - Equity: Do benefits reach those in greatest need? - Uncertainty: How confident are we in these estimates? - Time horizon: Should we value immediate gains over long-term prevention?"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#setup",
    "href": "notebooks/2.06_policy_simulation.html#setup",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "2 1. Setup",
    "text": "2 1. Setup\n\n# ============================================================\n# Bootstrap cell (works both locally and in Colab)\n#\n# What this cell does:\n# - Ensures that we are inside the course repository.\n# - In Colab: clones the repository from GitHub if necessary.\n# - Loads the course utility module (epi_utils.py).\n#\n# Important:\n# - You may see messages printed below (e.g. from pip or git).\n# - Warnings (often in yellow) are usually harmless.\n# - If you see a red error traceback, re-run this cell first.\n# ============================================================\n\nimport os\nimport sys\nimport pathlib\nimport subprocess\n\n# ------------------------------------------------------------\n# Configuration: repository location and URL\n# ------------------------------------------------------------\nREPO_URL = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\nREPO_DIR = \"fb2nep-epi\"\n\n# ------------------------------------------------------------\n# 1. Ensure we are inside the repository\n# ------------------------------------------------------------\ncwd = pathlib.Path.cwd()\n\n# Case A: we are already in the repository (scripts/epi_utils.py exists)\nif (cwd / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd\n# Case B: we are in a subdirectory of the repository\nelif (cwd.parent / \"scripts\" / \"epi_utils.py\").is_file():\n    repo_root = cwd.parent\n# Case C: we are outside the repository (e.g. in Colab)\nelse:\n    repo_root = cwd / REPO_DIR\n\n    # Clone the repository if not present\n    if not repo_root.is_dir():\n        print(f\"Cloning repository from {REPO_URL} into {repo_root} ...\")\n        subprocess.run([\"git\", \"clone\", REPO_URL, str(repo_root)], check=True)\n    else:\n        print(f\"Using existing repository at {repo_root}\")\n\n    # Change working directory to repository root\n    os.chdir(repo_root)\n    repo_root = pathlib.Path.cwd()\n\n# Add scripts directory to Python path\nscripts_dir = repo_root / \"scripts\"\nif str(scripts_dir) not in sys.path:\n    sys.path.insert(0, str(scripts_dir))\n\nprint(f\"Repository root: {repo_root}\")\nprint(\"Bootstrap completed successfully.\")\n\n\n# ------------------------------------------------------------\n# Import libraries and course utilities\n# ------------------------------------------------------------\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport ipywidgets as widgets\nfrom ipywidgets import IntSlider, FloatSlider, VBox, HBox, Output, HTML\nfrom IPython.display import display, clear_output\n\n# Import course utilities from the repository\nfrom epi_utils import (\n    INTERVENTIONS, STRATEGIES,\n    calculate_dalys_averted, calculate_marginal_cost_per_daly,\n    calculate_equity_dalys, evaluate_strategy,\n    evaluate_strategy_with_uncertainty, evaluate_with_equity_weights,\n    get_intervention_summary, print_strategy_comparison\n)\n\nnp.random.seed(42)\n\nprint(\"Libraries loaded successfully.\")"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#the-interventions",
    "href": "notebooks/2.06_policy_simulation.html#the-interventions",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "3 2. The Interventions",
    "text": "3 2. The Interventions\nBelow are 8 interventions you can fund. Cost-effectiveness estimates are derived from published literature, NICE guidance, and PHE analyses.\n\n# View intervention summary\nprint(\"Available Interventions\")\nprint(\"=\" * 100)\ndisplay(get_intervention_summary())\n\n\n# Display detailed descriptions\nprint(\"\\nIntervention Details\")\nprint(\"=\" * 100)\nfor key, v in INTERVENTIONS.items():\n    print(f\"\\n{v['name'].upper()}\")\n    print(f\"  {v['description']}\")\n    print(f\"  → Equity profile: Q1 (most deprived) receives {v['equity_distribution'][0]*100:.0f}% of benefits\")"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#diminishing-returns",
    "href": "notebooks/2.06_policy_simulation.html#diminishing-returns",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "4 3. Diminishing Returns",
    "text": "4 3. Diminishing Returns\nReal interventions don’t scale linearly. The first £1M reaches the most accessible population. Subsequent spending encounters harder-to-reach populations and saturation effects.\n\n# Visualise diminishing returns\nfig, axes = plt.subplots(2, 4, figsize=(16, 8))\naxes = axes.flatten()\n\nfor idx, (key, intervention) in enumerate(INTERVENTIONS.items()):\n    ax = axes[idx]\n    spends = np.linspace(0.1, intervention['max_capacity_millions'], 50)\n    marginal_costs = [calculate_marginal_cost_per_daly(s, intervention) for s in spends]\n    \n    ax.plot(spends, marginal_costs, 'b-', linewidth=2)\n    ax.axhline(y=20000, color='r', linestyle='--', alpha=0.7, label='NICE threshold')\n    ax.set_xlabel('Spend (£M)')\n    ax.set_ylabel('Marginal £/DALY')\n    ax.set_title(intervention['name'], fontsize=10)\n    ax.set_ylim(0, min(50000, max(marginal_costs) * 1.1))\n\nplt.suptitle('Marginal Cost-Effectiveness by Spending Level', fontsize=12, fontweight='bold')\nplt.tight_layout()\nplt.show()"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#the-budget-allocation-game",
    "href": "notebooks/2.06_policy_simulation.html#the-budget-allocation-game",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "5 4. The Budget Allocation Game",
    "text": "5 4. The Budget Allocation Game\nYou have £50 million to allocate. Use the sliders to set your budget for each intervention.\n\nTOTAL_BUDGET = 50\n\n# Create sliders\nsliders = {}\nslider_widgets = []\n\nfor key, intervention in INTERVENTIONS.items():\n    slider = IntSlider(\n        value=0, min=0, max=intervention['max_capacity_millions'], step=1,\n        description='', continuous_update=True,\n        layout=widgets.Layout(width='250px')\n    )\n    sliders[key] = slider\n    label = HTML(\n        value=f\"&lt;b&gt;{intervention['name']}&lt;/b&gt;&lt;br&gt;&lt;small&gt;Max: £{intervention['max_capacity_millions']}M&lt;/small&gt;\",\n        layout=widgets.Layout(width='220px')\n    )\n    slider_widgets.append(HBox([label, slider, HTML(value=\"£M\")]))\n\nbudget_html = HTML(value=\"\")\nresults_output = Output()\ncharts_output = Output()\n\ndef update_dashboard(change=None):\n    allocations = {key: slider.value for key, slider in sliders.items()}\n    total_spend = sum(allocations.values())\n    remaining = TOTAL_BUDGET - total_spend\n    \n    # Budget display\n    if remaining &lt; 0:\n        budget_html.value = f\"&lt;div style='padding:10px;background:#ffcccc;border-radius:5px;'&gt;&lt;h3 style='color:red;margin:0;'&gt;⚠️ OVER BUDGET by £{-remaining}M&lt;/h3&gt;&lt;/div&gt;\"\n    elif remaining == 0:\n        budget_html.value = f\"&lt;div style='padding:10px;background:#ccffcc;border-radius:5px;'&gt;&lt;h3 style='color:green;margin:0;'&gt;✓ Budget Fully Allocated&lt;/h3&gt;&lt;/div&gt;\"\n    else:\n        budget_html.value = f\"&lt;div style='padding:10px;background:#ffffcc;border-radius:5px;'&gt;&lt;h3 style='margin:0;'&gt;Budget Remaining: £{remaining}M&lt;/h3&gt;&lt;/div&gt;\"\n    \n    # Calculate results\n    results = []\n    equity_totals = {'Q1': 0, 'Q2': 0, 'Q3': 0, 'Q4': 0, 'Q5': 0}\n    \n    for key, spend in allocations.items():\n        intervention = INTERVENTIONS[key]\n        dalys = calculate_dalys_averted(spend, intervention)\n        equity_dalys = calculate_equity_dalys(dalys, intervention['equity_distribution'])\n        for q, val in equity_dalys.items():\n            equity_totals[q] += val\n        \n        if spend &gt; 0:\n            avg_cost = (spend * 1_000_000) / dalys if dalys &gt; 0 else 0\n            marginal_cost = calculate_marginal_cost_per_daly(spend, intervention)\n        else:\n            avg_cost, marginal_cost = 0, intervention['base_cost_per_daly']\n        \n        results.append({\n            'intervention': intervention['name'], 'spend': spend, 'dalys': dalys,\n            'avg_cost': avg_cost, 'marginal_cost': marginal_cost,\n            'time_to_impact': intervention['time_to_impact_years']\n        })\n    \n    total_dalys = sum(r['dalys'] for r in results)\n    \n    # Results display\n    with results_output:\n        clear_output(wait=True)\n        print(\"=\" * 80)\n        print(f\"TOTAL DALYs AVERTED: {total_dalys:,.0f}\")\n        if total_spend &gt; 0:\n            print(f\"OVERALL COST-EFFECTIVENESS: £{(total_spend * 1_000_000) / total_dalys:,.0f} per DALY\")\n        print(\"=\" * 80)\n        \n        results_df = pd.DataFrame(results)\n        results_df = results_df[results_df['spend'] &gt; 0].sort_values('dalys', ascending=False)\n        if len(results_df) &gt; 0:\n            display_df = results_df[['intervention', 'spend', 'dalys', 'avg_cost', 'marginal_cost']].copy()\n            display_df.columns = ['Intervention', 'Spend (£M)', 'DALYs', 'Avg £/DALY', 'Marginal £/DALY']\n            display(display_df.style.format({\n                'Spend (£M)': '£{:.0f}M', 'DALYs': '{:,.0f}',\n                'Avg £/DALY': '£{:,.0f}', 'Marginal £/DALY': '£{:,.0f}'\n            }).hide(axis='index'))\n    \n    # Charts\n    with charts_output:\n        clear_output(wait=True)\n        if total_dalys &gt; 0:\n            fig = make_subplots(rows=1, cols=2,\n                subplot_titles=('DALYs by Intervention', 'Equity: DALYs by Deprivation Quintile'))\n            \n            funded = sorted([r for r in results if r['spend'] &gt; 0], key=lambda x: x['dalys'], reverse=True)\n            if funded:\n                fig.add_trace(go.Bar(\n                    x=[r['intervention'] for r in funded],\n                    y=[r['dalys'] for r in funded],\n                    marker_color='steelblue'\n                ), row=1, col=1)\n            \n            quintile_labels = ['Q1 (Most Deprived)', 'Q2', 'Q3', 'Q4', 'Q5 (Least Deprived)']\n            quintile_values = [equity_totals[f'Q{i+1}'] for i in range(5)]\n            colors = ['#d73027', '#fc8d59', '#fee090', '#91bfdb', '#4575b4']\n            fig.add_trace(go.Bar(x=quintile_labels, y=quintile_values, marker_color=colors), row=1, col=2)\n            \n            fig.update_layout(height=400, showlegend=False)\n            fig.update_xaxes(tickangle=45, row=1, col=1)\n            fig.show()\n            \n            q1_share = equity_totals['Q1'] / total_dalys * 100\n            print(f\"\\nEquity: {q1_share:.1f}% of benefits go to most deprived quintile (Q1)\")\n            if q1_share &gt; 22:\n                print(\"→ Your allocation is PRO-EQUITY\")\n            elif q1_share &lt; 18:\n                print(\"→ Your allocation may WIDEN INEQUALITIES\")\n\nfor slider in sliders.values():\n    slider.observe(update_dashboard, names='value')\n\nprint(\"BUDGET ALLOCATION GAME - Allocate your £50M:\")\ndisplay(VBox([budget_html, VBox(slider_widgets), results_output, charts_output]))\nupdate_dashboard()"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#strategy-comparison",
    "href": "notebooks/2.06_policy_simulation.html#strategy-comparison",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "6 5. Strategy Comparison",
    "text": "6 5. Strategy Comparison\nLet’s compare three pre-defined strategies.\n\nprint_strategy_comparison()\n\n\n# Visualise comparison\ncomparison_results = []\nfor name, strategy in STRATEGIES.items():\n    metrics = evaluate_strategy(strategy['allocations'])\n    comparison_results.append({\n        'Strategy': name,\n        'Total DALYs': metrics['total_dalys'],\n        'Q1 Share (%)': metrics['q1_share']\n    })\n\ncomparison_df = pd.DataFrame(comparison_results)\n\nfig = make_subplots(rows=1, cols=2,\n    subplot_titles=('Total DALYs Averted', 'Share to Most Deprived (Q1)'))\n\ncolors = ['#2ecc71', '#3498db', '#9b59b6']\nfig.add_trace(go.Bar(x=comparison_df['Strategy'], y=comparison_df['Total DALYs'], marker_color=colors), row=1, col=1)\nfig.add_trace(go.Bar(x=comparison_df['Strategy'], y=comparison_df['Q1 Share (%)'], marker_color=colors), row=1, col=2)\nfig.add_hline(y=20, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Equal (20%)\", row=1, col=2)\nfig.update_layout(height=400, showlegend=False, title_text=\"Strategy Comparison\")\nfig.show()"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#the-efficiency-equity-frontier",
    "href": "notebooks/2.06_policy_simulation.html#the-efficiency-equity-frontier",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "7 6. The Efficiency-Equity Frontier",
    "text": "7 6. The Efficiency-Equity Frontier\n\n# Generate random allocations to map the frontier\nn_simulations = 500\nsimulation_results = []\n\nfor _ in range(n_simulations):\n    raw_weights = np.random.dirichlet(np.ones(8)) * 50\n    allocations = {}\n    for i, key in enumerate(INTERVENTIONS.keys()):\n        allocations[key] = min(raw_weights[i], INTERVENTIONS[key]['max_capacity_millions'])\n    \n    total = sum(allocations.values())\n    if total &gt; 0:\n        scale = 50 / total\n        allocations = {k: min(v * scale, INTERVENTIONS[k]['max_capacity_millions']) for k, v in allocations.items()}\n    \n    metrics = evaluate_strategy(allocations)\n    simulation_results.append({'dalys': metrics['total_dalys'], 'q1_share': metrics['q1_share']})\n\n# Add named strategies\nfor name, strategy in STRATEGIES.items():\n    metrics = evaluate_strategy(strategy['allocations'])\n    simulation_results.append({'dalys': metrics['total_dalys'], 'q1_share': metrics['q1_share'], 'name': name})\n\nsim_df = pd.DataFrame(simulation_results)\n\n# Plot\nfig = go.Figure()\nunnamed = sim_df[sim_df['name'].isna()] if 'name' in sim_df.columns else sim_df\nfig.add_trace(go.Scatter(x=unnamed['dalys'], y=unnamed['q1_share'], mode='markers',\n    marker=dict(size=5, color='lightgray', opacity=0.5), name='Random allocations'))\n\nfor name, color in zip(['Maximum Efficiency', 'Equity Focus', 'Balanced Portfolio'], ['#2ecc71', '#3498db', '#9b59b6']):\n    point = sim_df[sim_df.get('name') == name]\n    if len(point) &gt; 0:\n        fig.add_trace(go.Scatter(x=point['dalys'], y=point['q1_share'], mode='markers+text',\n            marker=dict(size=15, color=color), text=[name], textposition='top center', name=name))\n\nfig.add_hline(y=20, line_dash=\"dash\", line_color=\"red\", annotation_text=\"Equal distribution\")\nfig.update_layout(title='The Efficiency-Equity Trade-off',\n    xaxis_title='Total DALYs (Efficiency →)', yaxis_title='Q1 Share % (Equity →)', height=500)\nfig.show()"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#incorporating-uncertainty",
    "href": "notebooks/2.06_policy_simulation.html#incorporating-uncertainty",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "8 7. Incorporating Uncertainty",
    "text": "8 7. Incorporating Uncertainty\n\n# Evaluate strategies with uncertainty\nuncertainty_results = {}\nfor name, strategy in STRATEGIES.items():\n    samples = evaluate_strategy_with_uncertainty(strategy['allocations'], n_samples=1000, seed=42)\n    uncertainty_results[name] = {\n        'samples': samples, 'mean': np.mean(samples),\n        'p5': np.percentile(samples, 5), 'p95': np.percentile(samples, 95)\n    }\n\n# Visualise\nfig = go.Figure()\ncolors = ['#2ecc71', '#3498db', '#9b59b6']\nfor i, (name, results) in enumerate(uncertainty_results.items()):\n    fig.add_trace(go.Violin(y=results['samples'], name=name, box_visible=True,\n        meanline_visible=True, fillcolor=colors[i], opacity=0.7))\n\nfig.update_layout(title='DALYs Averted with Uncertainty (Monte Carlo)', yaxis_title='DALYs', height=500, showlegend=False)\nfig.show()\n\nprint(\"\\n90% Confidence Intervals:\")\nfor name, r in uncertainty_results.items():\n    print(f\"{name}: {r['mean']:,.0f} [{r['p5']:,.0f} - {r['p95']:,.0f}]\")"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#equity-weighting",
    "href": "notebooks/2.06_policy_simulation.html#equity-weighting",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "9 8. Equity Weighting",
    "text": "9 8. Equity Weighting\nShould a DALY averted in a deprived area count for more?\n\n# Interactive equity weight explorer\nequity_slider = FloatSlider(value=1.0, min=1.0, max=3.0, step=0.1,\n    description='Q1 Weight:', continuous_update=True, style={'description_width': 'initial'})\nequity_output = Output()\n\ndef update_equity_analysis(change):\n    q1_weight = equity_slider.value\n    weights = {\n        'Q1': q1_weight, 'Q2': 1 + (q1_weight - 1) * 0.6, 'Q3': 1.0,\n        'Q4': 1 - (q1_weight - 1) * 0.3, 'Q5': 1 - (q1_weight - 1) * 0.5\n    }\n    \n    with equity_output:\n        clear_output(wait=True)\n        print(f\"Weights: Q1={weights['Q1']:.1f}, Q2={weights['Q2']:.1f}, Q3={weights['Q3']:.1f}, Q4={weights['Q4']:.1f}, Q5={weights['Q5']:.1f}\")\n        \n        results = []\n        for name, strategy in STRATEGIES.items():\n            weighted = evaluate_with_equity_weights(strategy['allocations'], weights)\n            unweighted = evaluate_strategy(strategy['allocations'])['total_dalys']\n            results.append({'Strategy': name, 'Unweighted': unweighted, 'Weighted': weighted, 'Change': (weighted/unweighted-1)*100})\n        \n        df = pd.DataFrame(results).sort_values('Weighted', ascending=False)\n        display(df.style.format({'Unweighted': '{:,.0f}', 'Weighted': '{:,.0f}', 'Change': '{:+.1f}%'}).hide(axis='index'))\n\nequity_slider.observe(update_equity_analysis, names='value')\nprint(\"Explore how equity weighting changes rankings:\")\ndisplay(equity_slider, equity_output)\nupdate_equity_analysis(None)"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#build-your-own-strategy",
    "href": "notebooks/2.06_policy_simulation.html#build-your-own-strategy",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "10 9. Build Your Own Strategy",
    "text": "10 9. Build Your Own Strategy\n\n# Define your strategy\nmy_strategy = {\n    'salt_reduction': 0,\n    'folic_acid': 0,\n    'weight_management': 0,\n    'diabetes_prevention': 0,\n    'school_meals': 0,\n    'smoking_cessation': 0,\n    'hypertension_screening': 0,\n    'sdil_extension': 0\n}\n\ntotal = sum(my_strategy.values())\nprint(f\"Allocated: £{total}M / £50M\")\n\nif total &gt; 0:\n    metrics = evaluate_strategy(my_strategy)\n    print(f\"DALYs averted: {metrics['total_dalys']:,.0f}\")\n    print(f\"Cost per DALY: £{metrics['cost_per_daly']:,.0f}\")\n    print(f\"Q1 share: {metrics['q1_share']:.1f}%\")"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#discussion-questions",
    "href": "notebooks/2.06_policy_simulation.html#discussion-questions",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "11 10. Discussion Questions",
    "text": "11 10. Discussion Questions\n\nWhat drove your choices? Cost-effectiveness, equity, evidence quality?\nDid you fund school meals? High cost per DALY but strong equity impact. How do you value children’s future health?\nShould we use equity weights? Who decides how much more a DALY in Middlesbrough is worth?\nWhat’s missing? Political feasibility, implementation capacity, public preferences…"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#key-takeaways",
    "href": "notebooks/2.06_policy_simulation.html#key-takeaways",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "12 Key Takeaways",
    "text": "12 Key Takeaways\n\nCost-effectiveness varies by scale (diminishing returns)\nEfficiency and equity often trade off\nUncertainty should influence decisions\nTime horizon shapes priorities\nValues are embedded in methods"
  },
  {
    "objectID": "notebooks/2.06_policy_simulation.html#references",
    "href": "notebooks/2.06_policy_simulation.html#references",
    "title": "2.06: Policy Simulation — Resource Allocation for Population Health",
    "section": "13 References",
    "text": "13 References\n\nNICE (2013). Guide to the methods of technology appraisal.\nWHO (2017). Tackling NCDs: Best buys.\nCookson R et al. (2017). Cost-effectiveness and health equity. Value in Health."
  },
  {
    "objectID": "assessment/index.html",
    "href": "assessment/index.html",
    "title": "FB2NEP — Nutritional Epidemiology and Public Health",
    "section": "",
    "text": "This site contains the core learning materials for FB2NEP: Nutritional Epidemiology and Public Health.\nYou will mainly work through the notebooks in this repository. Use Blackboard for timetables, announcements, and assessment submission links."
  },
  {
    "objectID": "assessment/index.html#welcome",
    "href": "assessment/index.html#welcome",
    "title": "FB2NEP — Nutritional Epidemiology and Public Health",
    "section": "",
    "text": "This site contains the core learning materials for FB2NEP: Nutritional Epidemiology and Public Health.\nYou will mainly work through the notebooks in this repository. Use Blackboard for timetables, announcements, and assessment submission links."
  },
  {
    "objectID": "assessment/index.html#assessments",
    "href": "assessment/index.html#assessments",
    "title": "FB2NEP — Nutritional Epidemiology and Public Health",
    "section": "2 Assessments",
    "text": "2 Assessments\nAssessment is by coursework only and consists of two assessments:\n\nAssessment 1: Data analysis and written interpretation (using a personal synthetic dataset).\nAssessment 2: A 7-minute recorded presentation pitching a public health intervention."
  },
  {
    "objectID": "assessment/index.html#use-of-ai-tools",
    "href": "assessment/index.html#use-of-ai-tools",
    "title": "FB2NEP — Nutritional Epidemiology and Public Health",
    "section": "3 Use of AI tools",
    "text": "3 Use of AI tools\nUse of generative AI is not permitted for any part of the module assessments. This includes (but is not limited to):\n\ngenerating, rewriting, paraphrasing, translating, summarising, or structuring text;\ngenerating code, analyses, tables, figures, or slide content;\n“checking” or “improving” your work, even if you believe the output is only editorial.\n\nYou may use standard software for writing and analysis (for example LaTeX, pandoc, Word, PowerPoint, Excel, R, Python/Colab), but the intellectual work, analysis decisions, interpretation, and wording must be entirely your own."
  },
  {
    "objectID": "assessment/assessment_1.html",
    "href": "assessment/assessment_1.html",
    "title": "",
    "section": "",
    "text": "Code"
  },
  {
    "objectID": "assessment/assessment_1.html#objective",
    "href": "assessment/assessment_1.html#objective",
    "title": "",
    "section": "1.1 Objective",
    "text": "1.1 Objective\n\nThis assessment evaluates your understanding of core concepts in nutritional epidemiology and your ability to apply epidemiological reasoning to data analysis. Using a synthetic dataset designed to investigate associations between sugar-sweetened beverage (SSB) intake, obesity, and cardiovascular disease (CVD) risk, you will demonstrate skills in exposure assessment, causal reasoning, statistical interpretation, and clear scientific communication.\n\n\n1.1.1 Background\nYou are working as a junior researcher analysing observational data from a population-based study investigating associations between sugar-sweetened beverage (SSB) intake, obesity, and cardiovascular disease (CVD) risk.\nBefore any statistical analysis is conducted, the research team must make a number of substantive epidemiological decisions: how SSB intake should be estimated, what types of bias and censoring might arise, which variables plausibly confound the relationship of interest, and how causal relationships should be conceptualised.\nSection A of this assessment reflects this early analytical phase. You should answer the questions as if you were planning or reviewing a real-world epidemiological analysis, drawing on standard principles from nutritional epidemiology rather than on the specifics of the teaching dataset.\nSection B then represents the analytical phase, in which these ideas are explored using a realistic synthetic dataset provided for teaching purposes. The dataset is designed to mimic the structure and challenges of real observational nutrition data, while allowing each student to work with an individual dataset.\nAcross both sections, the emphasis is on epidemiological reasoning and interpretation, not on finding a single “correct” numerical answer."
  },
  {
    "objectID": "assessment/assessment_1.html#use-of-artificial-intelligence",
    "href": "assessment/assessment_1.html#use-of-artificial-intelligence",
    "title": "",
    "section": "1.2 Use of artificial intelligence",
    "text": "1.2 Use of artificial intelligence\nThe use of artificial intelligence tools (including generative AI for text, code, figures, or interpretation) is strictly prohibited in this assessment."
  },
  {
    "objectID": "assessment/assessment_1.html#what-you-must-submit",
    "href": "assessment/assessment_1.html#what-you-must-submit",
    "title": "",
    "section": "1.3 What you must submit",
    "text": "1.3 What you must submit\n\nA single document containing:\n\nSection A short-answer responses and DAG\nSection B written answers and interpretations\nTables and figures copied from the notebook where appropriate\n\n\nThe Jupyter notebook itself must not be submitted."
  },
  {
    "objectID": "assessment/assessment_1.html#section-a-conceptual-epidemiology-25",
    "href": "assessment/assessment_1.html#section-a-conceptual-epidemiology-25",
    "title": "",
    "section": "1.4 Section A – Conceptual epidemiology (25%)",
    "text": "1.4 Section A – Conceptual epidemiology (25%)\nAnswer each question in no more than 100 words unless stated otherwise.\n\nWhat is the most appropriate method to estimate SSB intake in a population study, and why?\nExplain one type of censoring that can occur in epidemiological studies.\nDraw a directed acyclic graph (DAG) illustrating how SSB intake could affect CVD risk.\nIdentify one confounder in this relationship and explain your choice using causal reasoning."
  },
  {
    "objectID": "assessment/assessment_1.html#section-b-data-analysis-and-interpretation-75",
    "href": "assessment/assessment_1.html#section-b-data-analysis-and-interpretation-75",
    "title": "",
    "section": "1.5 Section B – Data analysis and interpretation (75%)",
    "text": "1.5 Section B – Data analysis and interpretation (75%)\n\n1.5.1 Background\nYou will use the provided assignment notebook to generate and analyse a personal synthetic dataset investigating SSB intake, obesity, and CVD risk.\nImportant:\n\nThe dataset is generated dynamically using your student ID.\nDataset generation is not reproducible across separate sessions unless completed in one continuous run.\nYou must therefore generate and analyse the dataset in a single session.\n\n\n\n1.5.2 Assignment notebook\nLink to assessment notebook\n     \n\n\n1.5.3 Questions\nThe questions below correspond exactly to those embedded in the notebook.\n\nB1: Create a Table 1 comparing obese and non-obese participants and provide a short epidemiological commentary.\nB2: Describe the distributions of key variables and justify any transformations or categorisations.\nB3: Fit and interpret a regression model relating SSB intake, obesity, and CVD risk.\nB4: Use your DAG to justify the adjustment strategy and identify any variables that should not be adjusted for.\n\nOptional bonus marks are available as described in the notebook."
  },
  {
    "objectID": "assessment/assessment_1.html#marking-rubric-and-assessment-criteria",
    "href": "assessment/assessment_1.html#marking-rubric-and-assessment-criteria",
    "title": "",
    "section": "1.6 Marking rubric and assessment criteria",
    "text": "1.6 Marking rubric and assessment criteria\n\n1.6.1 Section A – Conceptual epidemiology\nMarks are awarded for correctness, clarity, and epidemiological reasoning.\n\nAppropriate and well-justified exposure assessment method\nCorrect explanation of censoring\nPlausible and logically structured DAG\nCorrect confounder selection and justification\nClear and concise written communication\n\n\n\n1.6.2 Section B – Data analysis and interpretation\nMarks are awarded for interpretation and reasoning, not for programming skill.\n\nTable 1: Correct construction and meaningful comparison of groups\nDistributions: Accurate description and justified transformation decisions\nRegression: Correct interpretation of effect estimates, uncertainty, and confounding\nCausal reasoning: Appropriate use of DAGs to guide adjustment"
  }
]