{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 · Advanced topics — confounding, colliders, mediation, imputation\n",
    "\n",
    "> **Purpose**: sharpen causal thinking (confounders vs colliders vs mediators), show how (mis)adjustment shifts estimates, and run simple missing-data sensitivity checks.\n",
    "\n",
    "> **Learning objectives**\n",
    "- Identify **confounders**, **colliders**, and **mediators** in nutrition questions.\n",
    "- Observe how estimates change when you (in)correctly adjust variables.\n",
    "- Compare **complete-case** vs **simple imputation** (teaching) and reflect on MNAR.\n",
    "- (Optional) Visualise DAGs to justify adjustment sets.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1abf971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the repo root (which has scripts/bootstrap.py) is on sys.path.\n",
    "import sys, os, pathlib, subprocess\n",
    "\n",
    "REPO_NAME = \"fb2nep-epi\"\n",
    "REPO_URL  = \"https://github.com/ggkuhnle/fb2nep-epi.git\"\n",
    "IN_COLAB  = \"google.colab\" in sys.modules\n",
    "\n",
    "def ensure_repo_on_path():\n",
    "    here = pathlib.Path.cwd()\n",
    "    # Walk up a few levels to find scripts/bootstrap.py\n",
    "    for p in [here, *here.parents]:\n",
    "        if (p / \"scripts\" / \"bootstrap.py\").exists():\n",
    "            os.chdir(p)                 # normalise CWD to repo root\n",
    "            sys.path.append(str(p))     # ensure imports like \"from scripts...\" work\n",
    "            return p\n",
    "    # Not found locally: if on Colab, clone then chdir\n",
    "    if IN_COLAB:\n",
    "        # clone only if missing\n",
    "        if not (pathlib.Path(\"/content\") / REPO_NAME).exists():\n",
    "            subprocess.run([\"git\", \"clone\", REPO_URL], check=False)\n",
    "        os.chdir(f\"/content/{REPO_NAME}\")\n",
    "        sys.path.append(os.getcwd())\n",
    "        return pathlib.Path.cwd()\n",
    "    # Otherwise, we can’t proceed\n",
    "    raise FileNotFoundError(\"Could not find repo root containing scripts/bootstrap.py\")\n",
    "\n",
    "repo_root = ensure_repo_on_path()\n",
    "print(\"Repo root:\", repo_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap: ensure repo root on path, then import init\n",
    "import sys, pathlib\n",
    "sys.path.append(str(pathlib.Path.cwd().parent))\n",
    "from scripts.bootstrap import init\n",
    "df, ctx = init()\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Definitions (succinct)\n",
    "- **Confounder**: causes both exposure and outcome; opens a backdoor path. *Adjust for it.*\n",
    "- **Collider**: is caused by two (or more) variables; conditioning opens a spurious path. *Do not adjust.*\n",
    "- **Mediator**: lies on the causal path from exposure to outcome. Adjusting estimates the **direct** effect (not total). *Adjust only if your target is the direct effect.*\n",
    "\n",
    "**Example questions**\n",
    "- *Red meat → Cancer*: likely confounded by **SES**, **smoking**, **age**; uncertain role for **BMI** (pathway vs confounding).\n",
    "- *Salt → CVD*: **age**/**SES** confound; **SBP** is a plausible **mediator**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Adjustment demo — red meat → Cancer\n",
    "We’ll fit three logistic models and compare ORs for `red_meat_g_d`:\n",
    "1) **Unadjusted**  \n",
    "2) **+ Confounders**: age, sex, SES, IMD, smoking, BMI  \n",
    "3) **+ Mediator candidate**: add `SBP` (if you argue salt → SBP → CVD is analogous; for cancer, this is a *didactic* overadjustment example)\n",
    "\n",
    "_Interpretation focus_: how the OR shifts and why that could happen under the DAG."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "\n",
    "OUTCOME = 'Cancer_incident'\n",
    "EXPOSURE = 'red_meat_g_d'\n",
    "conf = ['age','BMI','sex','SES_class','IMD_quintile','smoking_status']\n",
    "\n",
    "m = df[[OUTCOME, EXPOSURE] + conf + ['SBP']].dropna().copy()\n",
    "print('n (complete-cases for this block):', len(m))\n",
    "\n",
    "def cat(v):\n",
    "    return f\"C({v})\" if (m[v].dtype=='object' or str(m[v].dtype).startswith('category')) else v\n",
    "\n",
    "def tidy_or(fit):\n",
    "    OR = np.exp(fit.params).rename('OR')\n",
    "    CI = np.exp(fit.conf_int()).rename(columns={0:'2.5%',1:'97.5%'})\n",
    "    return pd.concat([OR,CI], axis=1).round(3)\n",
    "\n",
    "# 1) Unadjusted\n",
    "y1, X1 = dmatrices(f'{OUTCOME} ~ {EXPOSURE}', data=m, return_type='dataframe')\n",
    "fit1 = sm.Logit(y1, X1).fit(disp=False)\n",
    "\n",
    "# 2) + Confounders\n",
    "rhs2 = ' + '.join([EXPOSURE] + [cat(v) for v in conf])\n",
    "y2, X2 = dmatrices(f'{OUTCOME} ~ ' + rhs2, data=m, return_type='dataframe')\n",
    "fit2 = sm.Logit(y2, X2).fit(disp=False)\n",
    "\n",
    "# 3) + Mediator candidate (overadjustment example)\n",
    "rhs3 = rhs2 + ' + SBP'\n",
    "y3, X3 = dmatrices(f'{OUTCOME} ~ ' + rhs3, data=m, return_type='dataframe')\n",
    "fit3 = sm.Logit(y3, X3).fit(disp=False)\n",
    "\n",
    "t1, t2, t3 = tidy_or(fit1), tidy_or(fit2), tidy_or(fit3)\n",
    "t1.loc[[EXPOSURE]], t2.filter(like=EXPOSURE, axis=0), t3.filter(like=EXPOSURE, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reading the shift**\n",
    "- If the OR moves **towards 1** after adjusting for confounders → confounding was inflating the crude association.\n",
    "- If adding a **mediator** (like SBP in salt→CVD; here didactic) pulls the OR towards 1 → you’re estimating a more **direct** effect (part of the total effect is soaked up)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Collider caution (mini simulation within our cohort)\n",
    "Create an **artificial collider** `CL` influenced by both exposure and outcome risk, then show that conditioning on it induces association even if we randomise exposure within levels of confounders. This is an illustration — don’t add such variables to your real models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(11088)\n",
    "d = df[[EXPOSURE, OUTCOME] + conf].dropna().copy()\n",
    "\n",
    "# Build a collider CL that is more likely when exposure high AND outcome=1\n",
    "x = (d[EXPOSURE] - d[EXPOSURE].mean())/d[EXPOSURE].std()\n",
    "p = 1/(1+np.exp(-(0.6*x + 1.0*d[OUTCOME] - 0.1)))\n",
    "d['CL'] = (rng.uniform(size=len(d)) < p).astype(int)\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "\n",
    "# Model without conditioning on CL\n",
    "y_nc, X_nc = dmatrices(f'{OUTCOME} ~ {EXPOSURE} + ' + ' + '.join([f'C({v})' if d[v].dtype=='object' or str(d[v].dtype).startswith('category') else v for v in conf]), data=d, return_type='dataframe')\n",
    "fit_nc = sm.Logit(y_nc, X_nc).fit(disp=False)\n",
    "# Model conditioning on the collider (WRONG)\n",
    "y_c, X_c = dmatrices(f'{OUTCOME} ~ {EXPOSURE} + CL + ' + ' + '.join([f'C({v})' if d[v].dtype=='object' or str(d[v].dtype).startswith('category') else v for v in conf]), data=d, return_type='dataframe')\n",
    "fit_c = sm.Logit(y_c, X_c).fit(disp=False)\n",
    "\n",
    "def or_of(term, fit):\n",
    "    import numpy as np, pandas as pd\n",
    "    OR = np.exp(fit.params[term])\n",
    "    lo, hi = np.exp(fit.conf_int().loc[term].values)\n",
    "    return pd.Series({'OR': round(float(OR),3), '2.5%': round(float(lo),3), '97.5%': round(float(hi),3)})\n",
    "\n",
    "or_nc = or_of(EXPOSURE, fit_nc)\n",
    "or_c  = or_of(EXPOSURE, fit_c)\n",
    "print('Without collider conditioning (correct):\\n', or_nc.to_dict())\n",
    "print('With collider conditioning (WRONG):\\n', or_c.to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Expectation_: adding `CL` typically **distorts** the exposure OR compared with the non-collider model. In real work, colliders are often inadvertently introduced via restricting the sample or adjusting for variables affected by both exposure and outcome (e.g., conditioning on a selection mechanism)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Missing data — complete-case vs simple imputation (teaching)\n",
    "We’ll compare **complete-case (CC)** analysis to a crude **median/mode imputation**. This is for **teaching only**; in practice prefer **multiple imputation** (not covered here to avoid new dependencies)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_model = [OUTCOME, EXPOSURE] + conf\n",
    "cc = df[vars_model].dropna().copy()\n",
    "print('CC n =', len(cc))\n",
    "\n",
    "imp = df[vars_model].copy()\n",
    "for c in imp.select_dtypes(include=['float64','int64']).columns:\n",
    "    imp[c] = imp[c].fillna(imp[c].median())\n",
    "for c in imp.select_dtypes(include=['object','category']).columns:\n",
    "    md = imp[c].mode(dropna=True)\n",
    "    if len(md): imp[c] = imp[c].fillna(md.iloc[0])\n",
    "print('Imputed n =', len(imp) - imp.isna().sum(axis=1).gt(0).sum())\n",
    "\n",
    "def fit_logit(dat):\n",
    "    def cat(v):\n",
    "        return f\"C({v})\" if (dat[v].dtype=='object' or str(dat[v].dtype).startswith('category')) else v\n",
    "    rhs = ' + '.join([EXPOSURE] + [cat(v) for v in conf])\n",
    "    y, X = dmatrices(f'{OUTCOME} ~ ' + rhs, data=dat, return_type='dataframe')\n",
    "    return sm.Logit(y, X).fit(disp=False)\n",
    "\n",
    "fit_cc  = fit_logit(cc)\n",
    "fit_imp = fit_logit(imp)\n",
    "\n",
    "def tidy_or_table(term, *fits):\n",
    "    rows = []\n",
    "    for tag, ft in fits:\n",
    "        OR = np.exp(ft.params[term])\n",
    "        lo, hi = np.exp(ft.conf_int().loc[term].values)\n",
    "        rows.append({'Model': tag, 'OR': round(float(OR),3), '2.5%': round(float(lo),3), '97.5%': round(float(hi),3)})\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "tidy_or_table(EXPOSURE, ('Complete-case', fit_cc), ('Median/mode impute', fit_imp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**\n",
    "- Differences between CC and imputed estimates indicate **missingness sensitivity**. If MAR is plausible and the imputation model is poor, bias can remain.\n",
    "- **MNAR** (e.g., sicker participants underreport diet) cannot be diagnosed from observed data alone — acknowledge and explore bounds if critical to inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) (Optional) DAG visual to justify adjustment set\n",
    "Use a small DAG to state your assumptions for **red meat → Cancer** (or your chosen pair), then argue your adjustment set from the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import networkx as nx, matplotlib.pyplot as plt\n",
    "    G = nx.DiGraph()\n",
    "    G.add_edges_from([\n",
    "        ('SES','red_meat_g_d'),('SES','Cancer_incident'),\n",
    "        ('smoking_status','red_meat_g_d'),('smoking_status','Cancer_incident'),\n",
    "        ('age','red_meat_g_d'),('age','Cancer_incident'),\n",
    "        ('red_meat_g_d','Cancer_incident')\n",
    "    ])\n",
    "    pos = {'SES':(-1,1),'smoking_status':(1,1),'age':(0,1.4),'red_meat_g_d':(0,0),'Cancer_incident':(0,-1)}\n",
    "    plt.figure(figsize=(5.5,4.2))\n",
    "    nx.draw(G, pos, with_labels=True, node_size=1600, node_color='#e6f2ff', arrows=True)\n",
    "    plt.title('DAG: confounding in red meat → cancer'); plt.axis('off'); plt.tight_layout(); plt.show()\n",
    "except Exception as e:\n",
    "    print('DAG skipped (networkx optional):', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) # TODO — your practice\n",
    "1. Choose a primary question (e.g., `salt_g_d → CVD_incident`). Specify a **minimal sufficient adjustment set** in Markdown and justify with a DAG sketch (optional cell below).\n",
    "2. Fit **unadjusted**, **confounder-adjusted**, and **mediator-adjusted** (if applicable) models; compare ORs.\n",
    "3. Repeat with **complete-case** vs **simple imputation**; summarise how sensitive your estimate is to missingness handling.\n",
    "4. In 3–5 sentences, explain a plausible **MNAR** mechanism for your exposure and what direction of bias it would induce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ## Key takeaways\n",
    ">\n",
    "> - Causal clarity first: **confounders** in, **colliders** out; **mediators** only if your estimand is the *direct* effect.\n",
    "> - (Mis)adjustment visibly moves estimates — always link choices back to a DAG.\n",
    "> - Missing data strategy matters; complete-case vs crude imputation can differ. Real work uses **multiple imputation** with a rich imputation model.\n",
    "> - Be explicit about **assumptions** and **sensitivity** — that’s the craft of epidemiology."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
